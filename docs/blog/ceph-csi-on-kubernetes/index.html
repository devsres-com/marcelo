<!doctype html>
<html lang="pt">
  <head>
  <meta charset="utf-8">
<title>Ceph CSI no Kubernetes - </title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="generator" content="Hugo 0.79.0" /><meta itemprop="name" content="Ceph CSI no Kubernetes">
<meta itemprop="description" content="Usando CSI para montar volumes RBD e sistemas de arquivo cephfs">
<meta itemprop="datePublished" content="2020-12-12T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-12-12T00:00:00+00:00" />
<meta itemprop="wordCount" content="4572">
<meta itemprop="image" content="">



<meta itemprop="keywords" content="kubernetes,csi,ceph," />
<meta property="og:title" content="Ceph CSI no Kubernetes" />
<meta property="og:description" content="Usando CSI para montar volumes RBD e sistemas de arquivo cephfs" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://devsres.com/marcelo/blog/ceph-csi-on-kubernetes/" />
<meta property="og:image" content="" />
<meta property="article:published_time" content="2020-12-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-12-12T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>

<meta name="twitter:title" content="Ceph CSI no Kubernetes"/>
<meta name="twitter:description" content="Usando CSI para montar volumes RBD e sistemas de arquivo cephfs"/>
<meta name="twitter:site" content="@marcelo_devsres"/>
<link rel="stylesheet" href="/marcelo/css/bundle.min.d9e04ae08c9b3049b766dbd4aeab7d862c5ea1d13679b621490e0f5df5507497.css" integrity="sha256-2eBK4IybMEm3ZtvUrqt9hixeodE2ebYhSQ4PXfVQdJc="><link rel="stylesheet" href="/marcelo/css/add-on.css">
</head>

  <body>
    

<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/marcelo/" class="nav">
        
          Blog
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          <a href="/marcelo/" class="nav link"><i class='fas fa-home'></i> Início</a>
        
      
        
          
          <a href="/marcelo/about/" class="nav link"><i class='far fa-id-card'></i> Sobre</a>
        
      
        
          
          <a href="/marcelo/blog/" class="nav link"><i class='far fa-newspaper'></i> Blog</a>
        
      
        
          
          <a href="/marcelo/categories/" class="nav link"><i class='fas fa-sitemap'></i> Categorias</a>
        
      
        
          
          <a href="/marcelo/contact/" class="nav link"><i class='far fa-envelope'></i> Contato</a>
        
      
      <a href="#share-menu" class="nav link share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      <a href="#search-input" class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    <a href="#share-menu" class="nav share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="nav lang-toggle" lang="pt">pt</a>
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu"></div></menu>
  <menu id="lang-menu" class="flyout-menu menu">
  <a href="#" lang="pt" class="nav link active">Português (pt)</a>
  
    
      
        <a href="/marcelo/en" lang="en" class="nav no-lang link">English (en)</a>
      
    
      
        <a href="/marcelo/fr" lang="fr" class="nav no-lang link">Français (fr)</a>
      
    
      
        <a href="/marcelo/pl" lang="pl" class="nav no-lang link">Polski (pl)</a>
      
    
      
    
      
        <a href="/marcelo/de" lang="de" class="nav no-lang link">Deutsche (de)</a>
      
    
      
        <a href="/marcelo/es" lang="es" class="nav no-lang link">Española (es)</a>
      
    
      
        <a href="/marcelo/zh-cn" lang="zh-cn" class="nav no-lang link">中文 (zh-cn)</a>
      
    
      
        <a href="/marcelo/zh-tw" lang="zh-tw" class="nav no-lang link">中文 (zh-tw)</a>
      
    
      
        <a href="/marcelo/ja" lang="ja" class="nav no-lang link">日本語 (ja)</a>
      
    
      
        <a href="/marcelo/nl" lang="nl" class="nav no-lang link">Nederlands (nl)</a>
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=Ceph%20CSI%20no%20Kubernetes&amp;url=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  

  
    <a href="//www.reddit.com/submit?url=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f&amp;title=Ceph%20CSI%20no%20Kubernetes" target="_blank" rel="noopener" class="nav share-btn reddit">
          <p>Reddit</p>
        </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f&amp;title=Ceph%20CSI%20no%20Kubernetes" target="_blank" rel="noopener" class="nav share-btn linkedin">
            <p>LinkedIn</p>
          </a>
  

  
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f&amp;description=Ceph%20CSI%20no%20Kubernetes" target="_blank" rel="noopener" class="nav share-btn pinterest">
          <p>Pinterest</p>
        </a>
  

  
        <a href="mailto:?subject=Confira%20esta%20postagem%20por Marcelo%20Andrade&amp;body=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f" target="_blank" class="nav share-btn email" data-proofer-ignore>
          <p>Email</p>
        </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  <a href="/marcelo/"><img src="https://devsres.com/marcelo/img/main/marcelo.jpg" class="circle" width="100" alt="/dev/sre" /></a>
  <header>
    <h1>Marcelo Andrade</h1>
  </header>
  <main>
    <p><b>Sysop 2 SRE</b></p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/marcelo-devsres" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/mrrandrade" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/devsresnetwork" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>


<li><a href="//youtube.com/channel/UCgbbwB1Wxndh8urBPpyx9XA/" target="_blank" rel="noopener" title="YouTube" class="fab fa-youtube"></a></li>





<li><a href="//instagram.com/marcelo_devsres" target="_blank" rel="noopener" title="Instagram" class="fab fa-instagram"></a></li>

<li><a href="//twitter.com/marcelo_devsres" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>




<li><a href="//telegram.me//dev/sres" target="_blank" rel="noopener" title="telegram" class="fab fa-telegram"></a></li>








      </ul>
    </footer>
  
</section>

      <main id="site-main">
        
  <article class="post">
    <header>
  <div class="title">
    
      <h2><a href="/marcelo/blog/ceph-csi-on-kubernetes/">Ceph CSI no Kubernetes</a></h2>
    
    
      <p>Usando CSI para montar volumes RBD e sistemas de arquivo cephfs</p>
    
  </div>
  <div class="meta">
    <time datetime="2020-12-12 00:00:00 &#43;0000 UTC">12 de December, 2020</time>
    <p>Marcelo Andrade</p>
    <p>22 Minutos De Leitura</p>
  </div>
</header>

    <div id="socnet-share">
      




  
    
    <a href="//twitter.com/share?text=Ceph%20CSI%20no%20Kubernetes&amp;url=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  

  
    <a href="//www.reddit.com/submit?url=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f&amp;title=Ceph%20CSI%20no%20Kubernetes" target="_blank" rel="noopener" class="nav share-btn reddit">
          <p>Reddit</p>
        </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f&amp;title=Ceph%20CSI%20no%20Kubernetes" target="_blank" rel="noopener" class="nav share-btn linkedin">
            <p>LinkedIn</p>
          </a>
  

  
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f&amp;description=Ceph%20CSI%20no%20Kubernetes" target="_blank" rel="noopener" class="nav share-btn pinterest">
          <p>Pinterest</p>
        </a>
  

  
        <a href="mailto:?subject=Confira%20esta%20postagem%20por Marcelo%20Andrade&amp;body=https%3a%2f%2fdevsres.com%2fmarcelo%2fblog%2fceph-csi-on-kubernetes%2f" target="_blank" class="nav share-btn email" data-proofer-ignore>
          <p>Email</p>
        </a>
  


    </div>
    <div class="content">
      <a href="/marcelo/blog/ceph-csi-on-kubernetes/" class="image" style="--bg-image: url('https://devsres.com/marcelo/img/reusable/radioactive-waste.jpg');">
    <img src="https://devsres.com/marcelo/img/reusable/radioactive-waste.jpg" alt="radiactive storage">
  </a>
      <p>Aqui vai o relato fresquinho da experiência de um dos meus trabalhos mais recentes: o deploy de plugins CSI para o provisionamento automático de filesystems CephFS e volumes RBD.</p>
<p>Uma nota infeliz: o cluster Ceph foi instalado e é mantido por outras pessoas, e eu não tenho qualquer tipo de acesso. Então o relato vai ser incompleto, faltando os procedimentos do lado &ldquo;de lá&rdquo;. Se você está esperando aqui ideias para apoiar a instalação e configuração de clusters Ceph ou uso de soluções como o <a href="https://rook.io/">Rook</a>, este não é o texto para você. O foco aqui está no deploy do CSI.</p>
<p>Vou ver se consigo um esboço de documentação combinada com eles e atualizar o texto no futuro!</p>
<h1 id="armazenamento-no-kubernetes-para-quê">Armazenamento no Kubernetes: para quê?</h1>
<p>O mundo Kubernetes adora coisas <em>stateless</em> e microsserviços; tudo seria muito mais fácil para todos se não precisássemos nos preocupar com ter que guardar dados. Olha só que ideal: o <em>Pod</em> sobe, um sistema de arquivos é instanciado do zero com a aplicação, ele morre, tudo é descartado e todo mundo fica feliz.</p>
<p>Mas nem tudo (na verdade, praticamente nada) é da forma que gostaríamos. Precisamos resolver o problema das coisas <em>stateful</em> em um mundo pensado <em>stateless</em>.</p>
<p>Felizmente, temos aqui ao lado um cluster <a href="https://ceph.io/">Ceph</a> cheio de Terabytes livres pronto para resolver o problema de armazenamento. Quais são as opçõpes disponíveis? Bem, temos:</p>
<ul>
<li>Objetos (S3);</li>
<li>RBD;</li>
<li>CephFS;</li>
</ul>
<h2 id="armazenamento-de-objetos-ie-s3">Armazenamento de objetos (i.e., &ldquo;S3&rdquo;)</h2>
<p>A primeira sugestão para este problema é: usa armazenamento de objetos estilo <em>S3</em>, ora!</p>
<p>É verdade, podemos usar esse recurso. Inclusive podemos, com algum grau de trabalho, <a href="https://docs.ceph.com/en/latest/radosgw/s3/">usar o Ceph para criar uma interface S3-like</a>. E funciona bem! A melhor parte: nenhuma modificação é feita nos sistemas de arquivo do container, sendo a opção ideal que demanda o mínimo do cluster Kubernetes em si. Todas as operações serão conduzidas pela aplicação diretamente contra a API pública do Ceph responsável pelo armazenamento de objetos.</p>
<p>Mas obviamente: se o trabalho não está do lado das operações, está do lado dos <strong>desenvolvedores</strong>. A aplicação precisa ser codificada para usar objetos.</p>
<p>Mas como você pode imaginar, armazenamento de objetos é um conceito <strong>novíssimo</strong> na computação, tipo, tem apenas umas poucas décadas. S3 surgiu apenas em 2006, tipo, praticamente ontem, a tecnologia sequer chegou à maioridade civil. Ainda não deu tempo de adaptar todo tipo de aplicação para este tipo de armazenamento. Talvez com a computação quântica chegar a gente consiga migrar tudo para S3 e ao mesmo tempo se ver livre do Cobol (/sarcasm)</p>
<p>Enfim, precisamos de alternativas ao armazenamento de objetos.</p>
<blockquote>
<p>(<strong>Cicatrizes de batalha</strong>: Armazenamento de objetos no Ceph funciona, mas não sem alguma dor de cabeça: algumas coisas precisam de adaptação; por exemplo, até outro dia, o Spark Operator tinha dificuldades em ler objetos de S3 sem gerar um trabalhão gigantesco para customizar a imagem com as bibliotecas do Hadoop mais novo, mas isso não é tão relevante assim para o momento, apenas cuidado com os detalhes e a versão das libs que você vai usar!).</p>
</blockquote>
<h2 id="armazenamento-de-bloco-block-storage">Armazenamento de bloco (block storage)</h2>
<p>Precisamos de uma estratégia para instanciar dispositivos de bloco para servir como discos dos nossos Pods. O caso de uso mais simples é criar um disco dedicado para servir de volume de dados dedicado para um único Pod (embora você possa ser mais arrojado aqui).</p>
<p>Para isso, temos o <a href="https://docs.ceph.com/en/latest/rbd/"><strong>Ceph Block Device</strong></a>, conhecido como <strong>RBD</strong>, que vai atuar em uma correspondência direta com serviços de nuvem como AWS EBS ou os Azure VHD page blobs (ainda não superei esse nome).</p>
<h2 id="sistema-de-arquivos-de-rede">Sistema de arquivos de rede</h2>
<p>Além dos blocos, uma outra solicitação comum por sistemas de arquivo de rede distribuídos ao estilo NFS (que inclusive <a href="https://kubernetes.io/docs/concepts/storage/volumes/#nfs">é uma opção no Kubernetes</a> a um custo de complexidade relativamente baixo).</p>
<p>Podemos usar o <a href="https://docs.ceph.com/en/latest/cephfs/"><strong>Ceph Filesystem</strong></a> ou <strong>CephFS</strong> para esta finalidade.</p>
<h1 id="qual-o-jeito-certo-de-usar-ceph-no-kubernetes">Qual o jeito &ldquo;certo&rdquo; de usar Ceph no Kubernetes?</h1>
<p>Das três opções que temos, o armazenamento de objetos depende única e exclusivamente da aplicação, e por isso é uma favorita de 10 em cada 10 administradores de infraestrutura.</p>
<p>As outras duas demandam algum conhecimento e configuração tanto do lado Kubernetes quanto do lado Ceph.</p>
<p>Levando em consideração que começamos nossa história com Kubernetes na versão 1.4, a integração com Cephfs e RBD originalmente implantada faz uso do código nativo do Kubernetes (também chamado de &ldquo;in-tree&rdquo;) para a montagem dos volumes, e segue assim até hoje. Entretanto, para um novo projeto, decidi fazer uso da <strong>Container Storage Interface</strong> (CSI) para fazer as novas integrações.</p>
<p>A CSI <a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">graduou para <em>stable</em></a> com o Kubernetes 1.13, e a ideia é a mesma dos demais CxI: desacoplar o código do Kubernetes dos diferentes <em>providers</em> de coisas. Não há razão para não usá-la exceto:</p>
<ul>
<li>preguiça;</li>
<li>incompatibilidade;</li>
<li>instabilidade;</li>
</ul>
<p>Obviamente somos profissionais da #ResistênciaOPS, então preguiça não é problema; vamos avaliar o resto!</p>
<h1 id="ceph-csi">Ceph CSI</h1>
<p>Algumas informações relevantes estão disponíveis no <a href="https://github.com/ceph/ceph-csi">site do projeto Ceph CSI</a>. A primeira delas é que saiu uma nova release no dia em que eu comecei a escrever essa documentação, então minha implantação já está defasada (nota: já atualizei)!</p>
<p>A segunda é a &lsquo;<em>support matrix</em>&rsquo; que apresenta as funcionalidades e o estado delas, bem como as versões compatíveis.</p>
<p>A primeira informação relevante a ser extraída é que você só pode usar o Ceph CSI se seu cluster Ceph for ao menos 14.0.0 (Nautilus); isso foi uma barreira para minhas iniciativas até pouco tempo atras.</p>
<p>Reproduzindo a tabela encontrada em 09/12/2020:</p>
<table>
<thead>
<tr>
<th>Plugin</th>
<th>Features</th>
<th>Feature Status</th>
<th>CSI Driver Version</th>
<th>CSI Spec Version</th>
<th>Ceph Cluster Version</th>
<th>Kubernetes Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>RBD</td>
<td>Dynamically provision, de-provision Block mode RWO volume</td>
<td>GA</td>
<td>&gt;= v1.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.14.0</td>
</tr>
<tr>
<td></td>
<td>Dynamically provision, de-provision Block mode RWX volume</td>
<td>GA</td>
<td>&gt;= v1.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.14.0</td>
</tr>
<tr>
<td></td>
<td>Dynamically provision, de-provision File mode RWO volume</td>
<td>GA</td>
<td>&gt;= v1.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.14.0</td>
</tr>
<tr>
<td></td>
<td>Provision File Mode ROX volume from snapshot</td>
<td>Alpha</td>
<td>&gt;= v3.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=v14.2.2)</td>
<td>&gt;= v1.17.0</td>
</tr>
<tr>
<td></td>
<td>Provision File Mode ROX volume from another volume</td>
<td>Alpha</td>
<td>&gt;= v3.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=v14.2.2)</td>
<td>&gt;= v1.16.0</td>
</tr>
<tr>
<td></td>
<td>Provision Block Mode ROX volume from snapshot</td>
<td>Alpha</td>
<td>&gt;= v3.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=v14.2.2)</td>
<td>&gt;= v1.17.0</td>
</tr>
<tr>
<td></td>
<td>Provision Block Mode ROX volume from another volume</td>
<td>Alpha</td>
<td>&gt;= v3.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=v14.2.2)</td>
<td>&gt;= v1.16.0</td>
</tr>
<tr>
<td></td>
<td>Creating and deleting snapshot</td>
<td>Alpha</td>
<td>&gt;= v1.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.17.0</td>
</tr>
<tr>
<td></td>
<td>Provision volume from snapshot</td>
<td>Alpha</td>
<td>&gt;= v1.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.17.0</td>
</tr>
<tr>
<td></td>
<td>Provision volume from another volume</td>
<td>Alpha</td>
<td>&gt;= v1.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.16.0</td>
</tr>
<tr>
<td></td>
<td>Expand volume</td>
<td>Beta</td>
<td>&gt;= v2.0.0</td>
<td>&gt;= v1.1.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.15.0</td>
</tr>
<tr>
<td></td>
<td>Metrics Support</td>
<td>Beta</td>
<td>&gt;= v1.2.0</td>
<td>&gt;= v1.1.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.15.0</td>
</tr>
<tr>
<td></td>
<td>Topology Aware Provisioning Support</td>
<td>Alpha</td>
<td>&gt;= v2.1.0</td>
<td>&gt;= v1.1.0</td>
<td>Nautilus (&gt;=14.0.0)</td>
<td>&gt;= v1.14.0</td>
</tr>
<tr>
<td>CephFS</td>
<td>Dynamically provision, de-provision File mode RWO volume</td>
<td>Beta</td>
<td>&gt;= v1.1.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=14.2.2)</td>
<td>&gt;= v1.14.0</td>
</tr>
<tr>
<td></td>
<td>Dynamically provision, de-provision File mode RWX volume</td>
<td>Beta</td>
<td>&gt;= v1.1.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=v14.2.2)</td>
<td>&gt;= v1.14.0</td>
</tr>
<tr>
<td></td>
<td>Dynamically provision, de-provision File mode ROX volume</td>
<td>Alpha</td>
<td>&gt;= v3.0.0</td>
<td>&gt;= v1.0.0</td>
<td>Nautilus (&gt;=v14.2.2)</td>
<td>&gt;= v1.14.0</td>
</tr>
<tr>
<td></td>
<td>Creating and deleting snapshot</td>
<td>Alpha</td>
<td>&gt;= v3.1.0</td>
<td>&gt;= v1.0.0</td>
<td>Octopus (&gt;=v15.2.3)</td>
<td>&gt;= v1.17.0</td>
</tr>
<tr>
<td></td>
<td>Provision volume from snapshot</td>
<td>Alpha</td>
<td>&gt;= v3.1.0</td>
<td>&gt;= v1.0.0</td>
<td>Octopus (&gt;=v15.2.3)</td>
<td>&gt;= v1.17.0</td>
</tr>
<tr>
<td></td>
<td>Provision volume from another volume</td>
<td>Alpha</td>
<td>&gt;= v3.1.0</td>
<td>&gt;= v1.0.0</td>
<td>Octopus (&gt;=v15.2.3)</td>
<td>&gt;= v1.16.0</td>
</tr>
<tr>
<td></td>
<td>Expand volume</td>
<td>Beta</td>
<td>&gt;= v2.0.0</td>
<td>&gt;= v1.1.0</td>
<td>Nautilus (&gt;=v14.2.2)</td>
<td>&gt;= v1.15.0</td>
</tr>
<tr>
<td></td>
<td>Metrics</td>
<td>Beta</td>
<td>&gt;= v1.2.0</td>
<td>&gt;= v1.1.0</td>
<td>Nautilus (&gt;=v14.2.2)</td>
<td>&gt;= v1.15.0</td>
</tr>
</tbody>
</table>
<p>Tirando o fato de que o provisionamento dinâmico de CephFS se encontra em estágio <em>Beta</em>, não há nenhum grande empecilho à implementação.</p>
<h2 id="helm-charts">Helm charts</h2>
<p>Nenhum procedimento de instalação é listado de imediato, mas tanto o plugin CSI do rbd quanto do cephfs contam com Helm charts disponíveis para instalação!</p>
<ul>
<li><a href="https://artifacthub.io/packages/helm/ceph-csi/ceph-csi-rbd"><strong>ceph-csi-rbd</strong></a></li>
<li><a href="https://artifacthub.io/packages/helm/ceph-csi/ceph-csi-cephfs"><strong>ceph-csi-cephfs</strong></a></li>
</ul>
<p>Qual a primeira coisa que devemos fazer com Helm charts alheios? Baixar e fuçar:</p>
<pre><code># # Rede isolada :(
# source proxy-me-up.sh

# helm repo add ceph-csi https://ceph.github.io/csi-charts

# helm repo update
...
...Successfully got an update from the &quot;ceph-csi&quot; chart repository
...

# helm pull ceph-csi/ceph-csi-rbd

# helm pull ceph-csi/ceph-csi-cephfs

# # Nunca esquecer de desconfigurar as variáveis de proxy para não entrar em pânico quando der kubectl e receber um timeout.
# unset ${!HTTP*} ${!http*}
</code></pre><p>Mas voltando ao chart: uma coisa que já me deixa triste éo fato de que não há uma maneira de você especificar um Docker Registry que faça &lsquo;override&rsquo; global para todas as imagens - aparentemente estou acostumado demais com os fantásticos Helm charts da <a href="https://bitnami.com/">Bitnami</a>.</p>
<p>E aqui vai uma lembrança nada a ver com o assunto Ceph CSI: em 20/11/2020, o Docker Hub ativou <a href="https://www.docker.com/increase-rate-limits">rate limit</a> para usuários anônimos; portanto deixar o seu cluster baixar imagens diretamente de registries públicos, mesmo para teste, não é mais uma opção.</p>
<p>A primeira coisa a fazer é levantar quais as imagens usadas pelos Charts e quais os parâmetros que vou precisar customizar para usar meu Registry privado.</p>
<pre><code># # Eu prefiro isso a dar helm show values
# for i in ceph*.tgz ; do tar xvf $i ; done

# fgrep -A1 repository ceph-csi-cephfs/values.yaml  ceph-csi-rbd/values.yaml
ceph-csi-cephfs/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-node-driver-registrar
ceph-csi-cephfs/values.yaml-      tag: v2.0.1
--
ceph-csi-cephfs/values.yaml:      repository: quay.io/cephcsi/cephcsi
ceph-csi-cephfs/values.yaml-      tag: v3.2.0
--
ceph-csi-cephfs/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-provisioner
ceph-csi-cephfs/values.yaml-      tag: v2.0.4
--
ceph-csi-cephfs/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-attacher
ceph-csi-cephfs/values.yaml-      tag: v3.0.2
--
ceph-csi-cephfs/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-resizer
ceph-csi-cephfs/values.yaml-      tag: v1.0.1
--
ceph-csi-cephfs/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-snapshotter
ceph-csi-cephfs/values.yaml-      tag: v3.0.2
--
ceph-csi-rbd/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-node-driver-registrar
ceph-csi-rbd/values.yaml-      tag: v2.0.1
--
ceph-csi-rbd/values.yaml:      repository: quay.io/cephcsi/cephcsi
ceph-csi-rbd/values.yaml-      tag: v3.2.0
--
ceph-csi-rbd/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-provisioner
ceph-csi-rbd/values.yaml-      tag: v2.0.4
--
ceph-csi-rbd/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-attacher
ceph-csi-rbd/values.yaml-      tag: v3.0.2
--
ceph-csi-rbd/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-resizer
ceph-csi-rbd/values.yaml-      tag: v1.0.1
--
ceph-csi-rbd/values.yaml:      repository: k8s.gcr.io/sig-storage/csi-snapshotter
ceph-csi-rbd/values.yaml-      tag: v3.0.2
</code></pre><p>Daqui podemos observar que:</p>
<ul>
<li>A imagem do plugin CSI é a mesma para ambos;</li>
</ul>
<p>Será que dá para consolidar ambos em um único pod de instalação? Isso pode ser útil para evitar ter que consumir metade do Pod Limit dos nodes com pods de &lsquo;overhead&rsquo;. Mas este não é meu problema no momento, vou optar pela comodidade do Helm chart pronto.</p>
<ul>
<li>Os plugins precisam de diversos outros componentes CSI para funcionarem.</li>
</ul>
<p>E com um bônus: agora eles aparentemente alinharam as versões dos 5 componentes extras entre os 2 charts.</p>
<blockquote>
<p><strong>Cicatrizes de batalha</strong>: se você, como eu, gosta de acompanhar os projetos individualmente e usar sempre a versão mais recente, é bastante provavel que você <strong>não possa fazer isso aqui</strong>. A evolução dos componentes CSI caminha por uma trilha diferente dos drivers CSI dos fornecedores. Você <strong>necessariamente</strong> precisa usar as versões &lsquo;homologadas&rsquo; por estes ou vai se dar mal.</p>
</blockquote>
<p>Apenas para referência, a versão na qual fiz todos os testes (3.1.2, de apenas 20 dias atrás) usavam as seguintes versões, bem mais antigas:</p>
<pre><code># # jsonpath 
k get pods -o jsonpath='{range .items[*].spec.containers[*]}{.image}{&quot;\n&quot;}{end}' | sort | uniq  | cut -f3 -d'/' | sort | uniq
cephcsi:v3.1.2
csi-attacher:v2.1.1
csi-node-driver-registrar:v1.3.0
csi-provisioner:v1.6.0
csi-resizer:v0.5.0
csi-snapshotter:v2.1.0
csi-snapshotter:v2.1.1
</code></pre><p>Onde tem a informação do que é compatível com o que?</p>
<p>Bem, além da tabela que copiei acima indicando as versões do CSI demandadas, você pode olhar&hellip; <em>no Helm chart</em>. Ou nos <em>yamls de exemplo</em>. Não parece haver uma tabela de referência em outro lugar no site. Vou entrar no <a href="https://cephcsi.slack.com/">Slack do projeto</a> e averiguar - não havendo, quem sabe contribuir?</p>
<h2 id="análise-dos-componentes-do-csi">Análise dos componentes do CSI</h2>
<p>Eu gosto de entender os templates dos Helm charts para implantações; se o festival de go-templates te deixar pouco confortável, possivelmente eles replicam o que está nestes exemplos:</p>
<ul>
<li><a href="https://github.com/ceph/ceph-csi/tree/master/deploy/rbd/kubernetes">Deploy RBD</a></li>
<li><a href="https://github.com/ceph/ceph-csi/tree/master/deploy/cephfs/kubernetes">Deploy Cephfs</a></li>
</ul>
<p>A partir da análise dos <em>manifests</em>, dá para entender a dinâmica dos componentes. Ambos usam uma organização de componentes semelhante:</p>
<ul>
<li><strong>Deployment de Provisioner</strong>;</li>
<li><strong>Daemonset de nodePlugin</strong>.</li>
</ul>
<p>Eu poderia copiar as ASCII Arts do <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">projeto CSI</a> que são bem legais, mas acho que a Red Hat fez um desenho que ilustra bem a ideia dos componentes descritos nas próximas sessões:</p>
<p><img src="/marcelo/static/reusable/openshift-csi.png" alt="Comunicação CSI"></p>
<h2 id="provisioners">Provisioners</h2>
<p>Os <strong>provisioners</strong> são os &lsquo;controller&rsquo; propriamente ditos, que irão monitorar a API do Kubernetes e executar as principais ações relacionadas aos volumes do ponto de vista do cluster;</p>
<p>São executados como <em>Deployments</em> a título de redundância, em que apenas um de seus <em>Pods</em> está ativo em um dado momento. Contém diversos containers; os comuns a ambos os drivers são:</p>
<p>Os provisioners são <em>Pods</em> com múltiplos containers, e precisam &ldquo;carregar&rdquo; os seguintes módulos para implementar a tradução de chamadas Kubernetes via CSI:</p>
<ul>
<li><strong>csi-provisioner</strong>: o <a href="https://github.com/kubernetes-csi/external-provisioner">external provisioner</a> do Kubernetes CSI; o controller responsável pela monitoração dos <em>PersistentVolumeClaims</em> criados e dispara as funções de criar e remover os volumes dos plugins CSI do Ceph.</li>
<li><strong>csi-attacher</strong>: o <a href="https://github.com/kubernetes-csi/external-attacher">external-attacher</a> fica com as chamadas de API CSI responsáveis pelo uso dos volumes por nodes;</li>
<li><strong>csi-resizer</strong>: o <a href="https://github.com/kubernetes-csi/external-resizer">external-resizer</a>, que intuitivamente implementa as chamadas para redimensionamento de PVCs;</li>
<li><strong>csi-snapshotter</strong>: o <a href="https://github.com/kubernetes-csi/external-snapshotter">external-snapshotter</a>, também intuitivamente responsável pela monitoração de CRDs de Snapshot e gerenciar seu ciclo de vida. Este plugin virou GA apenas no Kubernetes 1.20, mas o Ceph CSI indica o uso deste recurso como <em>Alpha</em>, então vale a precaução.</li>
</ul>
<p>Além destes componentes externos ao projeto, os CSI também usam:</p>
<ul>
<li><strong>liveness-prometheus</strong>: o próprio plugin cephcsi executando em modo &lsquo;&ndash;type=liveness&rsquo;, dedicado a garantir o estado do <em>Pod</em> e a exportar métricas;</li>
</ul>
<p>Por fim, uma nova versão do plugin cephcsi é executada das seguintes formas, dependendo do tipo de componente usado.</p>
<p>Para o CephFS:</p>
<ul>
<li><strong>csi-cephfsplugin</strong>: um único container, executado em modo &lsquo;&ndash;type=cephfs&rsquo;, responsável pela comunicação com os containers que executam os drivers nos nós;</li>
</ul>
<p>Para o RBD:</p>
<ul>
<li><strong>csi-rbdplugin</strong>: um container em modo &lsquo;&ndash;type=rbd&rsquo;.</li>
<li><strong>csi-rbdplugin-controller</strong>: um container em modo &lsquo;&ndash;type=controller&rsquo;.</li>
</ul>
<h2 id="nodeplugins">NodePlugins</h2>
<p>Os nodePlugins são executados em todas as máquinas de workload, que são aquelas que irão efetivamente mapear os volumes para pods.</p>
<p>São executados como <em>Daemonsets</em> para subir em todas as máquinas do cluster. Recebem diretamente a tradução das chamadas CSI realizadas ao driver &lsquo;provisioner&rsquo; e implementam as operações de montagem dos sistemas de arquivos e dispositivos de blocos.</p>
<p>Os <em>Pods</em> de ambos executam três containers:</p>
<ul>
<li><strong>driver-registrar</strong>: o <a href="https://github.com/kubernetes-csi/node-driver-registrar/releases">node-driver-registrar</a> registra o driver CSI no Kubelet daquele host;</li>
<li><strong>liveness-prometheus</strong>: o próprio plugin cephcsi executando em modo &lsquo;&ndash;type=liveness&rsquo;, de maneira semelhante ao executado nos provisioners;</li>
</ul>
<p>O terceiro container é o driver propriamente dito:</p>
<ul>
<li><strong>csi-rbdplugin</strong> ou <strong>csi-cephfsplugin</strong>: o driver cephcsi, executando no modo apropriado, e responsável pelas operações de mapeamento dos dispositivos e montagem dos sistemas de arquivo cephfs.</li>
</ul>
<p><strong>Considerações de segurança adicionais</strong>: este <em>Pod</em>, pela sua natureza, demanda privilégios especiais; executa com as diretivas &lsquo;hostPID=true&rsquo;, &lsquo;hostNetwork=true&rsquo; além de mapear diversos volumes.</p>
<h2 id="observações-relevantes">Observações relevantes</h2>
<p>Entender devidamente a função de cada componente demanda leitura do <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md">documento de design dos CSI</a>.</p>
<p><strong>Importante</strong>: como o driver CSI do Ceph e os demais componentes têm seu desenvolvimento dissociado um do outro, eles evoluem em ritmos diferentes. Consequentemente, versões mais novas de algun dos 4 componentes &lsquo;CSI&rsquo; do lado do Kubernetes não necessariamente são compatívels com as implementações dos plugins.</p>
<p>Onde tem a informação do que é compatível com o que? <strong>Em lugar nenhum</strong>. É necessário olhar os helm charts dos releases ou os manifests de exemplo disponibilizados pelo projeto.</p>
<h1 id="instalação">Instalação</h1>
<h2 id="pré-requisitos">Pré-requisitos</h2>
<p>Como falei lá no começo, não tenho acesso ao Ceph (apenas temporariamente mwahaha), então precisei interfacear com os administradores do cluster Ceph para que me repassassem as informações necessárias.</p>
<p>Para implantar os CSI, precisamos de dois tipos de informações:</p>
<ol>
<li>
<p><strong>Parâmetros de configuração</strong>:</p>
<ul>
<li><strong>Cluster_ID</strong>: esta informação é crítica e corresponde ao identificador único do Ceph para o cluster Kubernetes. Aviso de antemão que depurar configurações divergentes de cluster_id entre cluster ceph e kubernetes por incrível que pareça não é tão trivial.</li>
<li><strong>Endereçamento IP do cluster</strong>: tanto dos monitores quando dos demais componentes. A rede é importante para possível liberação de regra em firewall, mas para o processo de instalação mesmo, a única coisa que precisamos configurar é os endereços IP de todos os <a href="https://docs.ceph.com/en/latest/start/intro/">Monitors</a> do cluster.</li>
<li><strong>Portas</strong>: Idealmente, na configuração, devemos evitar especificar as portas e deixar o cliente usar da maneira que der. Por que isso? Os Ceph Monitors usam duas portas por padrão: 3300 e 6789. Cada porta atende a uma versão diferente do protocolo msgr, sendo a porta 3300 a mais nova. Dependendo do tipo de uso, uma versão será usada, ou a outra.</li>
<li><strong>subvolumeGroup</strong>: parâmetro relevante para o CephFS. O valor default é &ldquo;<em>csi</em>&quot;; algo diferente demandará configuração.</li>
</ul>
</li>
<li>
<p><strong>Credenciais com as permissões corretas</strong>.</p>
</li>
</ol>
<p>Os parâmetros (1.) você irá passar para o Helm no ato da instalação. Você <strong>não precisa</strong> das credenciais para a instalação dos plugins, então é possível dissociar estas duas etapas.</p>
<p>As credenciais (2.) devem ser configuradas no processo de criação das <em>StorageClasses</em>. Elas <strong>não</strong> são configuradas pelos Helm charts - eles instalam os plugins CSI apenas.</p>
<p>Como infelizmente meu cluster ainda não conta com acesso ao Git interno (um mês não é tempo suficiente para criação de regras de firewall) mas os meus prazos precisam ser cumpridos, eu <strong>não poderei usar GitOps</strong> para fazer o deploy dos CSI.</p>
<p>(Quando eu finalmente conseguir implantar o Fluxv2, eu prometo que vou descrever aqui).</p>
<h2 id="instalação-do-ceph-csi-rbd">Instalação do Ceph CSI RBD</h2>
<p>É possível executar o helm install passando um arquivo de <em>values</em>, que é mais estético e prático. Eu prefiro chamar na linha de comando dezenas de &ndash;set como a tripa gigante abaixo. De brinde, segue a maneira não descrita pela documentação oficial para aplicar &ndash;set em <em>Lists</em> na linha de comando em Helm charts.</p>
<p>Para este comando, precisamos dos seguintes pré-requisitos:</p>
<ul>
<li>ID do cluster: torça para que não passem errado para você;</li>
<li>IPs dos monitores.</li>
</ul>
<p>Estas informações irão virar um campo &lsquo;config.json&rsquo; em um <em>Configmap</em> chamado <strong>ceph-csi-config-rbd</strong>.</p>
<pre><code># export PRIVATE_REPO=hub.intra/resistenciaOPS

# helm install \
  --namespace &quot;csi&quot; \
  --create-namespace=true \
  ceph-csi-rbd \
  /srv/k8s-bootstrap/charts/ceph-csi-rbd \
  --set configMapName=ceph-csi-config-rbd \
    --set kmsConfigMapName=ceph-csi-rbd-encryption-kms-config \
  --set csiConfig[0].clusterID=6969197-ddf7-418f-8f07-3e6744c6af80 \
  --set csiConfig[0].monitors[0]=192.168.69.1 \
  --set nodeplugin.registrar.image.repository=$PRIVATE_REPO/csi-node-driver-registrar \
  --set nodeplugin.plugin.image.repository=$PRIVATE_REPO/cephcsi \
  --set nodeplugin.nodeSelector.'node-role\.kubernetes\.io/worker'='' \
  --set provisioner.provisioner.image.repository=$PRIVATE_REPO/csi-provisioner \
  --set provisioner.attacher.image.repository=$PRIVATE_REPO/csi-attacher \
  --set provisioner.resizer.image.repository=$PRIVATE_REPO/csi-resizer \
  --set provisioner.snapshotter.image.repository=$PRIVATE_REPO/csi-snapshotter \
  --set provisioner.nodeSelector.'node-role\.kubernetes\.io/master'='' \
  --set provisioner.tolerations[0].key=node-role.kubernetes.io/master \
  --set provisioner.tolerations[0].effect=NoSchedule \
  --set provisioner.replicaCount=2
</code></pre><p>Os únicos comentários relevantres sobre o comando acima são:</p>
<ul>
<li>set configMapName=ceph-csi-config-rbd: as duas versões agora usam o mesmo nome de configmap. Tentar instalar as duas com o helm irá gerar conflito. As alternativas são: modificar o nome do configmap e torná-los independente de novo, ou configurar a diretiva &lsquo;externallyManagedConfigmap&rsquo;, que provavelmente é a melhor. Sou preguiçoso e fui com a primeira.</li>
<li>set nodeplugin.nodeSelector: não tenho interesse de executar o plugin em todas as máquinas do Cluster, apenas naquelas que eu considero &lsquo;workers&rsquo;. Desta forma, não irão subir, por exemplo, 2 Pods para isso no Master.</li>
<li>set provisioner.nodeSelector/provisioner.tolerations: já o provisioner é o contrário: eu <strong>quero</strong> que ele rode apenas nos nós de Control Plane.</li>
<li>set provisioner.replicaCount=2 porque só tenho dois masters.</li>
</ul>
<p>Por que eu quero que ele rode nos nós de Control plane?</p>
<pre><code># k exec -it -c csi-rbdplugin ceph-csi-rbd-provisioner-6dff8cf8f4-7jxft -- whoami
root
</code></pre><p>Eu poderia rodar um comando &lsquo;helm status&rsquo; para mostrar para vocês o resultado da execução, mas o Helm 3 <a href="https://github.com/helm/helm/issues/5952"><strong>quebrou o helm status</strong></a>, e a issue em aberto há um ano e meio não me deixa muito otimista quanto à resolução.</p>
<p>Segue um resultado com o que ele <em>pode vir a instalar</em>, dependendo dos parâmetros:</p>
<pre><code># helm get manifest ceph-csi-rbd | yq -r '&quot;\(.kind):\(.metadata.name)&quot;'
ServiceAccount:ceph-csi-rbd-nodeplugin
ServiceAccount:ceph-csi-rbd-provisioner
ConfigMap:ceph-csi-config-rbd
ConfigMap:ceph-csi-encryption-kms-config
ClusterRole:ceph-csi-rbd-provisioner
ClusterRole:ceph-csi-rbd-provisioner-rules
ClusterRoleBinding:ceph-csi-rbd-provisioner
Role:ceph-csi-rbd-provisioner
RoleBinding:ceph-csi-rbd-provisioner
Service:ceph-csi-rbd-nodeplugin-http-metrics
Service:ceph-csi-rbd-provisioner-http-metrics
DaemonSet:ceph-csi-rbd-nodeplugin
Deployment:ceph-csi-rbd-provisioner
</code></pre><p>Se quiser mais detalhes, pode pedir assim:</p>
<pre><code># for i in $( helm get manifest ceph-csi-rbd | yq -r '&quot;\(.kind):\(.metadata.name)&quot;' ) ; do kubectl get ${i/:/ } -o yaml ; read ;  done
</code></pre><h2 id="instalação-do-ceph-csi-cephfs">Instalação do Ceph CSI Cephfs</h2>
<p>Muito semelhante ao anterior:</p>
<pre><code># export PRIVATE_REPO=hub.intra/resistenciaOPS

# helm install \
  --namespace &quot;csi&quot; \
  --create-namespace=true \
  ceph-csi-cephfs \
  /srv/k8s-bootstrap/charts/ceph-csi-cephfs \
  --set configMapName=ceph-csi-config-cephfs \
  --set csiConfig[0].clusterID=6969d032-c77e-4c55-a560-d559c2f41058 \
  --set csiConfig[0].monitors[0]=192.168.69.1 \
  --set nodeplugin.registrar.image.repository=$PRIVATE_REPO/csi-node-driver-registrar \
  --set nodeplugin.plugin.image.repository=$PRIVATE_REPO/cephcsi \
  --set nodeplugin.nodeSelector.'node-role\.kubernetes\.io/worker'='' \
  --set provisioner.provisioner.image.repository=$PRIVATE_REPO/csi-provisioner \
  --set provisioner.attacher.image.repository=$PRIVATE_REPO/csi-attacher \
  --set provisioner.resizer.image.repository=$PRIVATE_REPO/csi-resizer \
  --set provisioner.snapshotter.image.repository=$PRIVATE_REPO/csi-snapshotter \
  --set provisioner.nodeSelector.'node-role\.kubernetes\.io/master'='' \
  --set provisioner.tolerations[0].key=node-role.kubernetes.io/master \
  --set provisioner.tolerations[0].effect=NoSchedule \
  --set provisioner.replicaCount=2
</code></pre><p>Basicamente as mesmas considerações da instalação anterior valem para cá.</p>
<p>Ele cria basicamente as mesmas coisas:</p>
<pre><code># helm get manifest ceph-csi-cephfs | yq -r '&quot;\(.kind):\(.metadata.name)&quot;'
ServiceAccount:ceph-csi-cephfs-nodeplugin
ServiceAccount:ceph-csi-cephfs-provisioner
ConfigMap:ceph-csi-config-cephfs
ClusterRole:ceph-csi-cephfs-provisioner
ClusterRole:ceph-csi-cephfs-provisioner-rules
ClusterRoleBinding:ceph-csi-cephfs-provisioner
Role:ceph-csi-cephfs-provisioner
RoleBinding:ceph-csi-cephfs-provisioner
Service:ceph-csi-cephfs-nodeplugin-http-metrics
Service:ceph-csi-cephfs-provisioner-http-metrics
DaemonSet:ceph-csi-cephfs-nodeplugin
Deployment:ceph-csi-cephfs-provisioner
</code></pre><h2 id="criação-das-storageclasses">Criação das StorageClasses</h2>
<p>Vou usar as pouco inspiradas <em>StorageClasses</em> abaixo:</p>
<ul>
<li>rbd: aqui há um campo importante que não há em outro lugar: o <strong>pool</strong> RBD. A única <a href="https://github.com/ceph/ceph-csi/blob/master/examples/rbd/storageclass.yaml#L33">imageFeature suportada</a> é &lsquo;layering&rsquo;.</li>
</ul>
<pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
allowVolumeExpansion: true  
mountOptions:
- discard
parameters:
  clusterID: 6975d032-c77e-4c55-a560-d559c2f41058
  csi.storage.k8s.io/controller-expand-secret-name: ceph-rbd
  csi.storage.k8s.io/controller-expand-secret-namespace: csi
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/node-stage-secret-name: ceph-rbd
  csi.storage.k8s.io/node-stage-secret-namespace: csi
  csi.storage.k8s.io/provisioner-secret-name: ceph-rbd
  csi.storage.k8s.io/provisioner-secret-namespace: csi
  imageFeatures: layering
  pool: kubernetes
provisioner: rbd.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
</code></pre><ul>
<li>cephfs:</li>
</ul>
<pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-cephfs
allowVolumeExpansion: true
mountOptions:
- debug
parameters:
  clusterID: 6969d032-c77e-4c55-a560-d559c2f41058
  csi.storage.k8s.io/controller-expand-secret-name: ceph-cephfs
  csi.storage.k8s.io/controller-expand-secret-namespace: csi
  csi.storage.k8s.io/node-stage-secret-name: ceph-cephfs
  csi.storage.k8s.io/node-stage-secret-namespace: csi
  csi.storage.k8s.io/provisioner-secret-name: ceph-cephfs
  csi.storage.k8s.io/provisioner-secret-namespace: csi
  fsName: cephfs
  mounter: kernel
provisioner: cephfs.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate

</code></pre><p>Aqui indicamos as secrets que serão usadas para provisionamento, no caso <strong>ceph-cephfs</strong> e <strong>ceph-rbd</strong>.</p>
<h2 id="criação-da-secrets">Criação da secrets</h2>
<p>Para RBD, é necessário especificar uma userID e uma userKey:</p>
<pre><code># k create secret generic ceph-rbd \
  --type='kubernetes.io/rbd' \
  --from-literal=userID='usuario' \
  --from-literal=userKey='chave' 
</code></pre><p>Já o CephFS precisa de dois conjuntos de credenciais distintas: além de uma userID com sua chave, é necessário usar uma chave de admin:</p>
<pre><code># k create secret generic ceph-cephfs \
  --type='kubernetes.io/cephfs' \
  --from-literal=userID='usuario' \
  --from-literal=userKey='chave' \
  --from-literal=adminID='usuario' \
  --from-literal=adminKey='chave'
</code></pre><p>Por que isso? A diferença é descrita por <a href="https://github.com/ceph/ceph-csi/blob/master/examples/cephfs/secret.yaml">estes dois crípticos comentários</a>:</p>
<pre><code># Required for statically provisioned volumes
  userID: &lt;plaintext ID&gt;
  userKey: &lt;Ceph auth key corresponding to ID above&gt;

  # Required for dynamically provisioned volumes
  adminID: &lt;plaintext ID&gt;
  adminKey: &lt;Ceph auth key corresponding to ID above&gt;
</code></pre><p>A melhor explicação para a diferença está <a href="https://github.com/ceph/ceph-csi/blob/master/docs/static-pvc.md">aqui</a>, o que traz certo alívio em ver que não é nada fantástico: &lsquo;statically provisioned&rsquo; são volumes que você criou na mão, e não o provisioner.</p>
<h1 id="testes">Testes</h1>
<p>Neste primeiro momento, não queremos testar funcionalidades avançadas. Os objetivos de testes são simples:</p>
<ul>
<li>Se o componente <em>provisioner</em> de cada plugin CSI está conseguindo provisionar adequadamente o PV para o PVC solicitado;</li>
<li>Se o componente que executa em cada nó está conseguindo montar os volumes ou sistemas de arquivo.</li>
</ul>
<p>Para cada teste, vamos subir um tipo diferente de objeto Kubernetes:</p>
<ul>
<li>Para testar o <strong>cephfs</strong>, vamos subir um deployment com pod anti affinity para que cada um suba em um nó diferente;</li>
<li>Para testar o <strong>rbd</strong>, vamos subir um <strong>statefulset</strong> (com o mesmo anti affinity) que provisionará volumes por meio de diretivas <em>volumeClaimTemplate</em>.</li>
</ul>
<p>Como temos 4 nós, o número de réplicas em cada um deles será 4.</p>
<h2 id="deployment-de-teste-de-cephfs">Deployment de teste de CephFS</h2>
<p>Aqui, nada de muito especial exceto a diretiva de <em>anti-affinity</em> usando a <em>topologyKey</em> que determina que um Pod sob este <em>Deployment</em> não subirá em um nó que já tenha um <em>Pod</em> deste deployment.</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
      app: cephfs-testfs
  name: cephfs-test
  namespace: csi
spec:
  replicas: 4
  selector:
    matchLabels:
      app: cephfs-testfs
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: cephfs-testfs
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cephfs-testfs
            topologyKey: kubernetes.io/hostname
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: nginx
        volumeMounts:
        - mountPath: /var/lib/www
          name: mypvc
      volumes:
      - name: mypvc
        persistentVolumeClaim:
          claimName: pvc-cephfs
</code></pre><p>Antes de executar o <em>Deployment</em> acima, será necessário criar o PVC:</p>
<p>Aqui, o PVC não será criado automaticamente, você deve fazer isso.</p>
<pre><code># cat pvc-cephfs.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-cephfs
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph-cephfs
</code></pre><p>O que acontece ao criar o PVC?</p>
<pre><code># k get pvc,pv
NAME                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/pvc-cephfs            Bound    pvc-f30393a6-42ee-4ffd-b3e5-784f1e940adc   1Gi        RWX            ceph-cephfs    2d11h

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
persistentvolume/pvc-f30393a6-42ee-4ffd-b3e5-784f1e940adc   1Gi        RWX            Delete           Bound    csi/pvc-cephfs            ceph-cephfs             2d11h
</code></pre><p>Agora pode-se criar o deployment, que distribuirá os <em>Pods</em> adequadamente entre os nós, cada um montando o mesmo PVC oferecendo leitura e escrita concorrente:</p>
<pre><code># k get pods -l app=cephfs-test -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP             NODE                                   NOMINATED NODE   READINESS GATES
cephfs-test-1006230814-6winp   1/1     Running   0          24h   192.168.22.164  worker1.intra   &lt;none&gt;           &lt;none&gt;
cephfs-test-1006230814-fmgu3   1/1     Running   0          24h   192.168.19.42   worker2.intra   &lt;none&gt;           &lt;none&gt;
cephfs-test-1006230814-6ekbw   1/1     Running   0          24h   192.168.8.146   worker3.intra   &lt;none&gt;           &lt;none&gt;
cephfs-test-1006230814-fz9sd   1/1     Running   0          24h   192.168.20.24   worker3.intra   &lt;none&gt;           &lt;none&gt;

</code></pre><h2 id="statefulset-de-teste-para-rbd">Statefulset de teste para RBD</h2>
<p>Para quem não conhece, um <em>statefulset</em> é um Deployment diferente, que cria pods com nomes pré-definidos em uma ordem pré definida.</p>
<p>A ideia é garantir uma identidade própria e uma &ldquo;unicabilidade&rdquo; aos Pods (tentei transcrever a definição da melhor forma possível, foi isso que consegui, desculpem!).</p>
<p>A relevância de ser ou não <em>Statefulset</em> para este teste é menor; o que é importante mesmo é o fato de que existe um campo <em>volumeClaimTemplate</em> para pod que garante que o Kubernetes auto-instancie os PVCs necessários para o teste, tornando esta abordagem perfeita para o sysadmin preguiçoso!</p>
<pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ceph-rbd-test
spec:
  selector:
    matchLabels:
      app: ceph-rbd-test
  serviceName: &quot;nginx&quot;
  replicas: 4
  template:
    metadata:
      labels:
        app: ceph-rbd-test
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ceph-rbd-test
            topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      storageClassName: &quot;ceph-rbd&quot;
      resources:
        requests:
          storage: 1Gi
</code></pre><p>O que acontece ao criar o statefulset?</p>
<pre><code># k get pods -l app=ceph-rbd-test -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP             NODE                                   NOMINATED NODE   READINESS GATES
ceph-rbd-test-0   1/1     Running   0          24h   192.168.22.167  worker1.intra   &lt;none&gt;           &lt;none&gt;
ceph-rbd-test-1   1/1     Running   0          24h   192.168.19.41   worker2.intra   &lt;none&gt;           &lt;none&gt;
ceph-rbd-test-2   1/1     Running   0          24h   192.168.8.148   worker3.intra   &lt;none&gt;           &lt;none&gt;
ceph-rbd-test-3   1/1     Running   0          24h   192.168.20.23   worker3.intra   &lt;none&gt;           &lt;none&gt;

</code></pre><p>Os PVCs são criados automaticamente, assim como os PVs:</p>
<pre><code># k get pvc,pv
NAME                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/www-ceph-rbd-test-0   Bound    pvc-4f1ca9bf-2c56-4ce4-8a6c-5bca290fd058   1Gi        RWO            ceph-rbd       24h
persistentvolumeclaim/www-ceph-rbd-test-1   Bound    pvc-0bd15bb4-e782-46ec-90ce-10a779c95451   1Gi        RWO            ceph-rbd       24h
persistentvolumeclaim/www-ceph-rbd-test-2   Bound    pvc-9f92d4e4-25e8-4f7c-9eab-2fc16c38b759   1Gi        RWO            ceph-rbd       24h
persistentvolumeclaim/www-ceph-rbd-test-3   Bound    pvc-26548c61-f6fd-424f-a6e2-272ff559bf41   1Gi        RWO            ceph-rbd       24h

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
persistentvolume/pvc-0bd15bb4-e782-46ec-90ce-10a779c95451   1Gi        RWO            Delete           Bound    csi/www-ceph-rbd-test-1   ceph-rbd                24h
persistentvolume/pvc-26548c61-f6fd-424f-a6e2-272ff559bf41   1Gi        RWO            Delete           Bound    csi/www-ceph-rbd-test-3   ceph-rbd                24h
persistentvolume/pvc-4f1ca9bf-2c56-4ce4-8a6c-5bca290fd058   1Gi        RWO            Delete           Bound    csi/www-ceph-rbd-test-0   ceph-rbd                24h
persistentvolume/pvc-9f92d4e4-25e8-4f7c-9eab-2fc16c38b759   1Gi        RWO            Delete           Bound    csi/www-ceph-rbd-test-2   ceph-rbd                24h
persistentvolume/pvc-f30393a6-42ee-4ffd-b3e5-784f1e940adc   1Gi        
</code></pre><p>Com a criação dinâmica dos PVCs, nem é necessário olhar os status dos Pods, pois todos os volumes foram criados.</p>
<p><strong>Observação importante</strong>: PVCs instanciados por statefulsets <strong>não são deletados automaticamente</strong> se o statefulset for deletado. Lembrar disso é importante para evitar acúmulo de lixo no cluster Ceph por testes esquecidos.</p>
<h1 id="erros-comuns">Erros comuns</h1>
<p>Praticamente <strong>todos</strong> os erros vêm de duas coisas:</p>
<ul>
<li><strong>Falha na conectividade com o cluster Ceph</strong>: que, ironicamente, não são relatadas exatamente de maneira trivial, mas por mensagens genéricas como &ldquo;rpc error: code = Unknown desc = operation timeout: context deadline exceeded&rdquo;;</li>
<li><strong>Falha na configuração dos plugins CSI</strong>: de longe a mais comum.</li>
</ul>
<p>A experiência que tive na última semana trabalhando para viabilizar esta implantação é que depurar a integração com o Ceph <strong>é um pesadelo</strong>. Não sei apontar de quem exatamente é a culpa, se das mensagens pouquíssimo claras do plugin ou se da nossa deficiência interna em obter logs relevantes dos componentes do Ceph.</p>
<p>Batemos em todo tipo de dificuldade:</p>
<ul>
<li>Erro de credencial;</li>
<li>Cluster_id incorreto;</li>
<li>Pool incorreto;</li>
<li>Erro de permissão;</li>
<li>Erro de protocolo (especificando a porta 3300, que era incompatível);</li>
<li>Usuários com mesmo conjunto de permissão recebendo respostas diferentes para suas requisições (um falhava, o outro funcionava);</li>
<li>Possível bug obscuro em que uma das máquinas simplesmente não montava o volume;</li>
</ul>
<p>É importante sempre &lsquo;limpar&rsquo; todo o ambiente entre cada testes (possivelmente a sujeira foi a causa do bug obscuro relatado acima).</p>
<p>Mas o processo de depuração foi basicamente auto tune, por meio de &lsquo;guessing games&rsquo;, dos parâmetros configurados.</p>
<p>Uma coisa importante é: não confie no provisionamento automático dos PVs para garantir que a implementação está operacional. O problema pode vir na forma da <strong>montagem dos volumes nas máquinas</strong> - lembrar que o CSI tem dois componentes, o &lsquo;<em>Provisioner</em>&rsquo; que traduz as requisições CSI e o &lsquo;<em>NodePlugin</em>&rsquo; que executa em cada nó.</p>
<p>Não foi a melhor das experiências, e pretendo voltar (quando tiver acesso ao Ceph) para averiguar uma maneira mais inteligente para fazer essa depuração.</p>

    </div>
    <footer>
      <div class="stats">
  
    <ul class="categories">
      
        
          <li><a class="article-terms-link" href="/marcelo/categories/kubernetes/">kubernetes</a></li>
        
          <li><a class="article-terms-link" href="/marcelo/categories/csi/">csi</a></li>
        
          <li><a class="article-terms-link" href="/marcelo/categories/ceph/">ceph</a></li>
        
      
    </ul>
  
  
    <ul class="tags">
      
        
          <li><a class="article-terms-link" href="/marcelo/tags/kubernetes/">kubernetes</a></li>
        
          <li><a class="article-terms-link" href="/marcelo/tags/csi/">csi</a></li>
        
          <li><a class="article-terms-link" href="/marcelo/tags/ceph/">ceph</a></li>
        
      
    </ul>
  
</div>

    </footer>
  </article>
  
    
  <article class="post">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "devsres-marcelo" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </article>




  
  <div class="pagination">
    
    
      <a href="/marcelo/blog/kubernetes-on-prem-1/" class="button right"><span>Kubernetes on-premises - parte 1</span></a>
    
  </div>

      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Posts Recentes</h1>
      </header>
      
      <article class="mini-post">
          <a href="/marcelo/blog/ceph-csi-on-kubernetes/" class="image" style="--bg-image: url('https://devsres.com/marcelo/img/reusable/radioactive-waste.jpg');">
    <img src="https://devsres.com/marcelo/img/reusable/radioactive-waste.jpg" alt="radiactive storage">
  </a>
        <header>
          <h2><a href="/marcelo/blog/ceph-csi-on-kubernetes/">Ceph CSI no Kubernetes</a></h2>
          <time class="published" datetime="2020-12-12 00:00:00 &#43;0000 UTC">12 de December, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/marcelo/blog/kubernetes-on-prem-1/" class="image" style="--bg-image: url('https://devsres.com/marcelo/img/reusable/jenga.png');">
    <img src="https://devsres.com/marcelo/img/reusable/jenga.png" alt="jenga">
  </a>
        <header>
          <h2><a href="/marcelo/blog/kubernetes-on-prem-1/">Kubernetes on-premises - parte 1</a></h2>
          <time class="published" datetime="2020-12-09 00:00:00 &#43;0000 UTC">9 de December, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/marcelo/blog/calico-and-route-tables/" class="image" style="--bg-image: url('https://devsres.com/marcelo/img/reusable/escher-stairs.png');">
    <img src="https://devsres.com/marcelo/img/reusable/escher-stairs.png" alt="Escher stairs">
  </a>
        <header>
          <h2><a href="/marcelo/blog/calico-and-route-tables/">Calico, Linux e tabelas de rota</a></h2>
          <time class="published" datetime="2020-12-01 00:00:00 &#43;0000 UTC">1 de December, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/marcelo/blog/prometheus-node-exporter-tls/" class="image" style="--bg-image: url('https://devsres.com/marcelo/img/reusable/prometheus.png');">
    <img src="https://devsres.com/marcelo/img/reusable/prometheus.png" alt="Prometheus having its liver devoured by the eagle">
  </a>
        <header>
          <h2><a href="/marcelo/blog/prometheus-node-exporter-tls/">Prometheus node exporter com TLS</a></h2>
          <time class="published" datetime="2020-10-30 00:00:00 &#43;0000 UTC">30 de October, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/marcelo/blog/suddenly-cka/" class="image" style="--bg-image: url('https://devsres.com/marcelo/img/main/cka.png');">
    <img src="https://devsres.com/marcelo/img/main/cka.png" alt="Calico cat clawing the door">
  </a>
        <header>
          <h2><a href="/marcelo/blog/suddenly-cka/">De repente, CKA</a></h2>
          <time class="published" datetime="2020-10-28 00:00:00 &#43;0000 UTC">28 de October, 2020</time>
        </header>
      </article>
      
      
        <footer>
          <a href="/marcelo/blog/" class="button">Veja Mais</a>
        </footer>
      
    </section>
  

  
    

      <section id="categories">
        <header>
          <h1><a href="/marcelo/categories">Categorias</a></h1>
        </header>
        <ul>
          
          
          <li>
              <a href="/marcelo/categories/kubernetes/">kubernetes<span class="count">6</span></a>
          
          <li>
              <a href="/marcelo/categories/calico/">calico<span class="count">2</span></a>
          
          <li>
              <a href="/marcelo/categories/docker/">docker<span class="count">2</span></a>
          
          <li>
              <a href="/marcelo/categories/iniciando/">iniciando<span class="count">2</span></a>
          
          <li>
              <a href="/marcelo/categories/intermedi%C3%A1rio/">intermediário<span class="count">2</span></a>
          
          <li>
              <a href="/marcelo/categories/avan%C3%A7ado/">avançado<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/ceph/">ceph<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/containers/">containers<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/csi/">csi<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/linux/">linux<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/node-exporter/">node-exporter<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/on-prem/">on-prem<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/passado/">passado<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/prometheus/">prometheus<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/terraform/">terraform<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/tls/">tls<span class="count">1</span></a>
          
          <li>
              <a href="/marcelo/categories/vmware/">vmware<span class="count">1</span></a>
          
          </li>
        </ul>
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>Sobre</h1>
      </header>
      <p>SRE e Cloud Native solutions engineer.<br>Respirando software livre desde 1997</p>
      <footer>
        <a href="/marcelo/about" class="button">Saber Mais</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/marcelo-devsres" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/mrrandrade" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/devsresnetwork" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>


<li><a href="//youtube.com/channel/UCgbbwB1Wxndh8urBPpyx9XA/" target="_blank" rel="noopener" title="YouTube" class="fab fa-youtube"></a></li>





<li><a href="//instagram.com/marcelo_devsres" target="_blank" rel="noopener" title="Instagram" class="fab fa-instagram"></a></li>

<li><a href="//twitter.com/marcelo_devsres" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>




<li><a href="//telegram.me//dev/sres" target="_blank" rel="noopener" title="telegram" class="fab fa-telegram"></a></li>








      </ul>
  
  <p class="copyright">
    © 2020 
      <br>
    Tema: <a href='https://themes.gohugo.io/hugo-future-imperfect-slim/' target='_blank' rel='noopener'>Hugo Future Imperfect</a><br>Adaptado de <a href='https://html5up.net/future-imperfect' target='_blank' rel='noopener'>HTML5 UP</a> | Disponibilizado pelo <a href='https://gohugo.io/' title='0.79.0' target='_blank' rel='noopener'>Hugo</a>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

      <script src="/marcelo/js/highlight.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script><script src="/marcelo/js/bundle.min.5955090a3253deadcd66071270aa2274dabe15ffc97094cec252d87b6f3f00bf.js" integrity="sha256-WVUJCjJT3q3NZgcScKoidNq&#43;Ff/JcJTOwlLYe28/AL8="></script>
    <script src="/marcelo/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-177944209', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
</html>
