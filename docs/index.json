[{
    "title": "Sobrevivendo com Terraform na AWS - parte 1",
    "date": "",
    "description": "Como começar do zero com Terraform na AWS",
    "body": "Sobrevivendo com Terraform na AWS Então, um pássaro preto gigante te jogou em um terreno desconhecido e inóspito conhecido como AWS e a única coisa que você tem é um terminal Linux e o binário terraform da Hashicorp. O que você faz?\n(Ou talvez não tenha sido um pássaro preto; talvez tenha sido o seu gerente de TI que te incumbiu de construir tudo que será necessário para o início das atividades na AWS. Mas a ideia do pássaro preto gigante parece mais legal).\nInterlude IAM e um usuário administrativo não-\u0026ldquo;root\u0026rdquo; Na \u0026ldquo;língua nativa AWS\u0026rdquo;, o usuário root é aquele com o e-mail que criou a conta. Em ambientes empresariais complexos, teríamos uma estrutura de Organizations com várias contas encadeadas e possivelmente integração com alguma base de usuários locais usando esta ou aquela estratégia de autenticação. Vamos abstrair isso; o usuário root é o gerente de TI que criou a conta e cadastrou o cartão de crédito corporativo sem limites (ou não!) da empresa e te entregou uma única coisa:\n Um usuário do IAM com seu nome.  É com ele (e com uma senha temporária) que você irá logar a primeira (e última!) vez na Console Web AWS. Afinal, nós vivemos para a tela preta do console.\n Autentique-se na Console usando a página customizada de login, o alias da conta ou simplesmente o ID se nada disso tiver sido feito. Busque por IAM, e clique no seu usuário. Clique em Credenciais de Segurança  Aqui temos duas atividades para fazer:\n  Crie uma chave de acesso. É um par ID e chave de acesso que você usará para \u0026ldquo;acessar programaticamente\u0026rdquo; a AWS usando o cliente de linha de comando (e, consequentemente, o terraform). Você pode baixar o .CSV se não estiver pronto para usá-la ainda.\n  Faça o upload de sua chave pública SSH para poder usá-la com o AWS Code Commit. Escreva em algum lugar o \u0026ldquo;*ID da chave do SSH\u0026rdquo;, que é um conjunto de letras em sentido - vamos precisar dela mais na frente.\n  (Nota: você obviamente não precisa usar o CodeCommit se preferir usar o Github, mas né, a título de laboratório, vamos nos ater às coisas AWS).\nCom isso, você está pronto para abandonar o excessivamente colorido universo WWW e partir para o que realmente importa.\nBaixando e configurando os programas essenciais Vamos precisar de dois programas para nos divertir com este trabalho:\n Cliente AWS Terraform  Os dois links acima constam com procedimentos para instalação da última versão, que na data deste post, são aws-cli/2.1.30 e terraform /0.14.7.\nConfigurando o aws cli Para configurar o AWS cli, basta executar:\n$ aws configure AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [None]: sa-east-1 Default output format [None]: json (Não, eu não esqueci minhas access keys aqui, esse é o exemplo da página oficial)\nSerá que está tudo ok? Teste vendo a si mesmo:\naws iam list-users --query \u0026quot;Users[?UserName=='the-ops-hero']\u0026quot; [ { \u0026quot;Path\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;UserName\u0026quot;: \u0026quot;the-ops-hero\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AIDQWERJLATCU123CR7M3UJ\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:iam::446926690069:user/the-ops-hero\u0026quot;, \u0026quot;CreateDate\u0026quot;: \u0026quot;2021-03-11T03:12:47+00:00\u0026quot;, \u0026quot;PasswordLastUsed\u0026quot;: \u0026quot;2021-03-11T03:13:42+00:00\u0026quot; } ] (Essa query syntax chama-se JMESPATH e, se quiser usar adequadamente o AWS cli, é bom ir se acostumando com ela!)\nConfigurando o Terraform É criar o diretório e configurar o provider AWS:\n$ mkdir terraform-boostrap $ cd terraform-bootstrap $ cat \u0026lt;\u0026lt; 'EOF' \u0026gt; provider.aws.tf provider \u0026quot;aws\u0026quot; { region = \u0026quot;sa-east-1\u0026quot; } EOF Traçando uma estratégia inicial Aqui, definimos os pré-requisitos para trabalhar com Terraform com o mínimo de profissionalismo. Quais são?\n Precisamos versionar as coisas que criamos; Precisamos de um lugar para guardar o nosso \u0026lsquo;statefile\u0026rsquo; que não seja na nossa máquina local; Precisamos de uma estratégia para lidar com concorrência de execuções.  Vamos atacar cada um desses pontos progressivamente.\nRequisito 1: Versionando o código Terraform usando CodeCommit Temos à nossa disposição o serviço AWS Code Commit, que é um servidor de repositórios Git mantido pela AWS.\n(Precisamos usar ele? Não, você pode usar Github ou qualquer outra coisa se preferir, mas para manter tudo homogêneo, vamos usá-lo!)\nEntão a primeira coisa a fazer é criar o tal repositório\u0026hellip; Usando Terraform!\nVamos começar a confeccionar o nosso arquivo terraform-bootstrap.tf assim:\n# ./terraform-bootstrap.tf # Qualquer nome que termine com .tf será processado. A maioria das pessoas usa main.tf, mas eu gosto de ser diferente. # Repositório GIT para servir de base para a Infraestrutura de Terraform: resource \u0026quot;aws_codecommit_repository\u0026quot; \u0026quot;terraform-bootstrap\u0026quot; { repository_name = \u0026quot;terraform-git\u0026quot; description = \u0026quot;Comecei na AWS agora, este repositorio tem tudo que preciso para comecar a rodar terraform\u0026quot; tags = { Name = \u0026quot;terraform-dev-sres\u0026quot; CreatedBy = \u0026quot;terraform\u0026quot; Environment = \u0026quot;production\u0026quot; } } Não esqueça de customizar as tags de acordo com seu gosto. Na AWS, tags são tudo.\nVamos criar nosso primeiro recurso?\n$ terraform plan -out terraform-bootstrap.plan An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_codecommit_repository.terraform-bootstrap will be created + resource \u0026quot;aws_codecommit_repository\u0026quot; \u0026quot;terraform-bootstrap\u0026quot; { + arn = (known after apply) + clone_url_http = (known after apply) + clone_url_ssh = (known after apply) + description = \u0026quot;Comecei na AWS agora, este repositorio tem tudo que preciso para comecar a rodar terraform\u0026quot; + id = (known after apply) + repository_id = (known after apply) + repository_name = \u0026quot;terraform-git\u0026quot; + tags = { + \u0026quot;CreatedBy\u0026quot; = \u0026quot;terraform\u0026quot; + \u0026quot;Environment\u0026quot; = \u0026quot;production\u0026quot; + \u0026quot;Name\u0026quot; = \u0026quot;terraform-dev-sres\u0026quot; } } Plan: 1 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ This plan was saved to: terraform-bootstrap.plan To perform exactly these actions, run the following command to apply: terraform apply \u0026quot;terraform-bootstrap.plan\u0026quot; Não criamos nada ainda, executamos um plan apenas para dar uma olhada. Vamos criar esse troço!\n$ terraform apply terraform-bootstrap.plan aws_codecommit_repository.terraform-bootstrap: Creating... aws_codecommit_repository.terraform-bootstrap: Creation complete after 1s [id=terraform-bootstrapx] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Agora, vamos usar o tal repositório!\n\u0026hellip; como dou clone?\nDuas opções:\n Pede informações usando a AWS CLI:  # Se você preferir, pode executar sem o --query para ver toods os campos. $ aws codecommit get-repository --repository-name=terraform-bootstrap --query repositoryMetadata.cloneUrlSsh --output text ssh://git-codecommit.sa-east-1.amazonaws.com/v1/repos/terraform-git  Cria um output no Terraform  # arquivo ./outputs.tf # Você pode imprimir todas as informações output \u0026quot;codecommit-terraform-bootstrap\u0026quot; { value = aws_codecommit_repository.terraform-bootstrap } # Ou apenas filtrar pela url: output \u0026quot;codecommit-terraform-bootstrap-url-ssh\u0026quot; { value = aws_codecommit_repository.terraform-bootstrap.clone_url_ssh } Para ver a saída do output, será necesário um refresh:\n$ terraform refresh aws_codecommit_repository.terraform-bootstrap: Refreshing state... [id=terraform-bootstrapx] Outputs: codecommit-terraform-bootstrap = \u0026quot;ssh://git-codecommit.sa-east-1.amazonaws.com/v1/repos/terraform-git\u0026quot; Agora é só dar o git clone, certo?\n$ REPO=$( aws codecommit get-repository --repository-name=terraform-bootstrap --query repositoryMetadata.cloneUrlSsh --output text ) $ git clone $REPO Cloning into 'terraform-bootstrap'... Warning: Permanently added the RSA host key for IP address '52.94.206.167' to the list of known hosts. marcelo@git-codecommit.sa-east-1.amazonaws.com: Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. Putz, não rolou. Mas eu fiz o upload da minha chave no IAM, certo? Deveria funcionar, certo?\nNão.\nRTFM, em especial a parte 8:\n On your local machine, use a text editor to create a config file in the ~/.ssh directory, and then add the following lines to the file, where the value for User is the SSH key ID you copied earlier:\n Host git-codecommit.*.amazonaws.com User APKAEIBAERJR2EXAMPLE IdentityFile ~/.ssh/codecommit_rsa Ah, lembra daquele tal \u0026ldquo;Id da chave SSH\u0026rdquo;, ele é o seu usuário no CodeCommit, não seu usuário do IAM, o que faz todo sentido, pensando direitinho.\nConfigure seu ~/.ssh/config e tente novamente que vai funcionar.\nInterlude - git bureaucracy Eu normalmente aplico um \u0026lsquo;git clone\u0026rsquo; no repo vazio e mudo o diretório .git de lugar:\n$ git clone $REPO mv terraform-git/.git ./ git add . -A git commit -m 'yay funcinou' git push Mas eu reconheço que isso é feio, então talvez esses comandos mais verboses agradem mais:\n$ git init $ git remote add origin $REPO $ git pull origin # ... ao fim de tudo # $ git push --set-upstream origin master Não vou me incomodar com git aqui, ok?\nNão esqueça de colocar um .gitignore apropriado no seu repositório.\nRequisito 2: Armazenando estado do Terraform usando S3 Se você olhar no diretório corrente, vai encontrar o seguinte:\n$ ls *.tfstate* terraform.tfstate terraform.tfstate.backup Se sua máquina morrer agora (não estou agourando!), é como se o seu Terraform não tivesse existido. As coisas que ele criou irão persistir, mas esse arquivo é o Terraform; ele que mapeia o que ele criou, o que ele controla, quais são os atributos e o que fazer com eles.\nEsse arquivo não pode ficar em uma máquina local.\nOnde colocar?\nEstando na AWs, o melhor lugar para colocar (também chamado de \u0026lsquo;backend') é o AWS S3, o serviço de armazenamento de objetos da AWS.\nComo fazer isso?\nBem, primeiro, precisamos de um Bucket S3. E qual a melhor maneira de criar isso? Usando Terraform, é claro!\nCriando Buckets S3 com Terraform # bucket.tf. Não precisa ser outro arquivo, mas pode ser se você quiser. resource \u0026quot;aws_s3_bucket\u0026quot; \u0026quot;terraform-bootstrap\u0026quot; { # não se esqueça de colocar o seu nome, nomes de buckets são globais na aws. bucket = \u0026quot;terraform-bootstrap-dev-sres\u0026quot; acl = \u0026quot;private\u0026quot; versioning { enabled = true } tags = { Name = \u0026quot;terraform-bootstrap\u0026quot; Environment = \u0026quot;production\u0026quot; } } Aqui, não esqueça das Tags e de habilitar o versioning, que vai te proteger se alguem decidir excluir ou sobrescrever seu programa com algum copy/paste mal feito!\n# vou suprimir as saídas destes comandos porque são altamente irrelevantes. $ terraform plan -out terraform-bootstrap.pan ... $ terraform apply terraform-bootstrap.plan ... Ok! Temos um bucket!\n$ aws s3api list-buckets --query Buckets[].Name --output text terraform-bootstrap-dev-sres Configurando o bucket S3 como Backend Como configurar este bucket como backend?\n# backend.tf terraform { backend \u0026quot;s3\u0026quot; { bucket = \u0026quot;terraform-bootstrap-dev-sres\u0026quot; key = \u0026quot;terraform-bootstrap/tfstate\u0026quot; region = \u0026quot;sa-east-1\u0026quot; } } Após modificaçõçes de backend, será necessário aplicar novamente a operação init. Durante essa operação, ele irá perguntar se você quer copiar o arquivo de estado existente para o novo backend. A resposta é um óbvio sim:\n$ terraform init Initializing the backend... Backend configuration changed! Terraform has detected that the configuration specified for the backend has changed. Terraform will now check for existing state in the backends. Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \u0026quot;s3\u0026quot; backend to the newly configured \u0026quot;s3\u0026quot; backend. No existing state was found in the newly configured \u0026quot;s3\u0026quot; backend. Do you want to copy this state to the new \u0026quot;s3\u0026quot; backend? Enter \u0026quot;yes\u0026quot; to copy and \u0026quot;no\u0026quot; to start with an empty state. Enter a value: yes $ terraform plan -out planfile ... $ terraforma apply planfile ... Após as últimas operações, podemos observar nosso arquivo de estados seguro em seu bucket s3:\n$ aws s3 ls terraform-bootstrap-dev-sres/terraform-bootstrap/ 2021-03-10 23:18:56 18466 2021-03-10 23:52:00 22110 tfstate E com isso completamos os dois primeiros requisitos.\nRequisito 3: Usando dynamodb para controlar a concorrência de execução Se você executar um \u0026lsquo;terraform apply\u0026rsquo; sem um plan, ele vai solicitar um prompt de confirmação para você:\n$ terraform apply Acquiring state lock. This may take a few moments... ... Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Se você abrir outro terminal e tentar executar o mesmo programa, receberá um erro:\nterraform plan Acquiring state lock. This may take a few moments... Error: Error locking state: Error acquiring the state lock: ConditionalCheckFailedException: The conditional request failed Lock Info: ID: 7ec4834e-f004-abd5-6b20-31ba68ba07e7 Path: terraform-bootstrap=dev-sres/terraform-bootstrap/tfstate Operation: OperationTypeApply Who: the-ops-hero@localhost Version: 0.14.7 Created: 2021-03-11 05:06:10.5604146 +0000 UTC Info: Terraform acquires a state lock to protect the state from being written by multiple users at the same time. Please resolve the issue above and try again. For most commands, you can disable locking with the \u0026quot;-lock=false\u0026quot; flag, but this is not recommended. Isso porque o terraform aplica um lock para evitar execução concorrente, que pode ser devastadora em alguns casos.\nObviamente, se alguém baixar o código do seu repositório git e executá-lo em outra máquina, ele conseguirá. Alguém eventualmente receberá um erro porque algo criado já existe.\nPara impedir isso, é importante usar um sistema de controle de concorrência centralizado. A melhor maneira de fazer isso na AWS é usando uma tabela Dynamodb.\nQual a melhor maneira de criar uma tabela DynamoDB? Usando Terraform, oras!\nCriando uma tabela Dynamodb Essa etapa não é que seja difícil, mas falta informação.\nA primeira delas está escondida lá embaixo na documentação oficial:\n dynamodb_table - (Optional) Name of DynamoDB Table to use for state locking and consistency. The table must have a primary key named LockID with type of string. If not configured, state locking will be disabled.\n Então sabemos que a nossa tabela de DynamoDB precisa de um campo LockID. Se não existir, o state locking vai ser desativado.\nO segundo detalhe é o \u0026lsquo;billing_mode\u0026rsquo;. Temos as opções \u0026lsquo;on-demand\u0026rsquo; e \u0026lsquo;provisioned\u0026rsquo;. Em \u0026lsquo;on-demand\u0026rsquo;, ele está livre para consumir recursos que ele quiser. Em \u0026lsquo;provisioned\u0026rsquo;, você precisa fazer as contas. A parte importante é: apenas provisioned está incluso no free tier. Se você estiver apenas brincando de laboratório na AWS e não quer cobrança, use este.\nEntão, se usar \u0026lsquo;provisioned\u0026rsquo;, vamos precisar especificar read e write capacity. Mas quanto?\nO \u0026lsquo;free tier\u0026rsquo; nos garante 25 de cada, além de 25GB de espaço. Na documentação oficial para criação de tabelas, ele relaciona ambos com 20 unidades. Se você quiser ler mais a respeito, siga em frente. Eu arbitrariamente dividi por 10 e coloquei 2 read requests.\n# dynamodb.tf # resource \u0026quot;aws_dynamodb_table\u0026quot; \u0026quot;terraform-dev-sres\u0026quot; { name = \u0026quot;terraform-bootstrap-dev-sres\u0026quot; billing_mode = \u0026quot;PROVISIONED\u0026quot; read_capacity = 2 write_capacity = 2 hash_key = \u0026quot;LockID\u0026quot; attribute { name = \u0026quot;LockID\u0026quot; type = \u0026quot;S\u0026quot; } tags = { Name = \u0026quot;terraform-bootstrap-dev-sres\u0026quot; CreatedBy = \u0026quot;terraform\u0026quot; Environment = \u0026quot;production\u0026quot; } } Configurando a tabela DynamoDB para controlar concorrência Após um combo plan/apply, estamos aptos a modificar nosso backend novamente:\nterraform { backend \u0026quot;s3\u0026quot; { bucket = \u0026quot;terraform-bootstrap-dev-sres\u0026quot; key = \u0026quot;terraform-bootstrap/tfstate\u0026quot; region = \u0026quot;sa-east-1\u0026quot; dynamodb_table = \u0026quot;terraform-dev-sres\u0026quot; } } Novamente, toda modificação de \u0026lsquo;backend\u0026rsquo; demanda um novo init:\n$ terraform init ... mude alguma coisa em algum lugar... $ terraform apply ... Enter a value: Não dê yes.\nExecute o seguinte comando para olhar a sua tabela:\n$ aws dynamodb scan --table-name terraform-dev-sres { \u0026quot;Items\u0026quot;: [ { \u0026quot;LockID\u0026quot;: { \u0026quot;S\u0026quot;: \u0026quot;terraform-bootstrap-dev-sres/terraform-bootstrap/tfstate\u0026quot; }, \u0026quot;Info\u0026quot;: { \u0026quot;S\u0026quot;: \u0026quot;{\\\u0026quot;ID\\\u0026quot;:\\\u0026quot;c2869c54-ff40-2824-ffee-1a409a834d6e\\\u0026quot;,\\\u0026quot;Operation\\\u0026quot;:\\\u0026quot;OperationTypeApply\\\u0026quot;,\\\u0026quot;Info\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;Who\\\u0026quot;:\\\u0026quot;the-ops-hero@localhost\\\u0026quot;,\\\u0026quot;Version\\\u0026quot;:\\\u0026quot;0.14.7\\\u0026quot;,\\\u0026quot;Created\\\u0026quot;:\\\u0026quot;2021-03-11T05:21:19.5567361Z\\\u0026quot;,\\\u0026quot;Path\\\u0026quot;:\\\u0026quot;terraform--bootstrap-dev-sres/terraform-bootstrap/tfstate\\\u0026quot;}\u0026quot; } }, { \u0026quot;Digest\u0026quot;: { \u0026quot;S\u0026quot;: \u0026quot;4cca273212ee805d228e8d662d4f4c6d\u0026quot; }, \u0026quot;LockID\u0026quot;: { \u0026quot;S\u0026quot;: \u0026quot;terraform-bootstrap-dev-sres/terraform-bootstrap/tfstate-md5\u0026quot; } }, { \u0026quot;Digest\u0026quot;: { \u0026quot;S\u0026quot;: \u0026quot;413f17b8b57764a4c7339837f75e7516\u0026quot; }, \u0026quot;LockID\u0026quot;: { \u0026quot;S\u0026quot;: \u0026quot;terraform-bootstrap-dev-sres/terraform-bootstrap/tfstatex-md5\u0026quot; } } ], \u0026quot;Count\u0026quot;: 3, \u0026quot;ScannedCount\u0026quot;: 3, \u0026quot;ConsumedCapacity\u0026quot;: null } Se você der um \u0026lsquo;terraform plan\u0026rsquo; em outra janela, é exatamente isso que você vai ver:\n$ terraform plan Acquiring state lock. This may take a few moments... Error: Error locking state: Error acquiring the state lock: ConditionalCheckFailedException: The conditional request failed Lock Info: ID: c2869c54-ff40-2824-ffee-1a409a834d6e Path: terraform-bootstrap-dev-sres/terraform-bootstrap/tfstate Operation: OperationTypeApply Who: the-ops-hero@localhost Version: 0.14.7 Created: 2021-03-11 05:21:19.5567361 +0000 UTC Info: Terraform acquires a state lock to protect the state from being written by multiple users at the same time. Please resolve the issue above and try again. For most commands, you can disable locking with the \u0026quot;-lock=false\u0026quot; flag, but this is not recommended. Confirme as mudanças.\nSuba seu código para o git.\nSua aventura Terraform com AWS agora está pronta para começar!\n",
    "ref": "/marcelo/blog/surviving-with-terraforming-on-aws-1/"
  },{
    "title": "Kubernetes on-premises - parte 4",
    "date": "",
    "description": "Como criar clusters Kubernetes",
    "body": "Outros capítulos da série Kubernetes on-premises:\n Kubernetes on-prem - parte 1: Introdução. Kubernetes on-prem - parte 2: Redes em Kubernetes - CNI e Calico. Kubernetes on-prem - parte 3: Tráfego de entrada em clusters Kubernetes  Como criar clusters Kubernetes Disclaimer: provavelmente este é o capítulo mais dispensável da série porque é o que eu tenho menos coisas relevantes para compartilhar. De qualquer forma, ele precisava ser escrito!\nAté agora, abordei diversas considerações relevantes que devem ser feitas antes de se instalar um cluster Kubernetes - e, em particular, pontos que particularmente são mais dolorosos para quem não está operando em Cloud Providers.\nMas finalmente, vem a pergunta: como instalar clusters Kubernetes? Qual é o jeito certo?\nQuando recebi essa missão pela primeira vez, eu não estava sequer remotamente preparado para isso. Combine o fato de que a instalação deveria ser realizada em um sistema ainda mais hostil que o normal como o CoreOS Linux (eventualmente renomeado para Container Linux e posteriormente absorvido pela Red Hat) tornou tudo incrivelmente mais difícil.\nA primeira coisa que vem em mente quando pergunto isso para as pessoas é, claro, kubeadm.\nE se eu te contar que, quando começamos, sequer existia kubeadm. Este post descreve o lançamento da primeira versão alpha do Kubeadm. Começamos a pensar na implantação em agosto de 2016.\nPortanto, foi necessário para nós, desde o início, adotar algo muito parecido com o Kubernetes the Hard Way, mas usando o Calico CNI.\nNosso processo até hoje: userdatas Devido à mais completa falta de oferta, foi necessário desenvolvermos nossa própria estrutura de criação de clusters. Ela foi modificada várias vezes ao longo dos anos, mas que segue a mesma ideia.\nNossos clusters são integralmente lançados a partir de arquivos tipo userdata para sistemas operacionais orientados a containers como Flatcar Linux até hoje, de maneira não interativa.\nComo o recurso de Certificate Signing do Control Plane do Kubernetes não estava disponível, fazemos uso de uma Autoridade Certificadora interna com API que gera certificados sob demanda. A comunicação com o API Server exige que a comunicação seja feita por meio do uso de certificados dessa CA, assim como a comunicação dos API Servers está restrita a clientes com certificados válidos.\nPara falar a verdade, uma vez que o processo de saber o que e como deve ser instalado é mapeado, não há grandes mistérios no processo de instalação de clusters Kubernetes. O único grande porém introduzido é a necessidade de gestão e cuidado de certificados digitais para as máquinas. Se for este o caminho desejado para sua instalação on-prem, basicamente as únicas preocupações são:\n Como criar e manter CAs seguras; Como distribuir os certificados gerados para os nós Kubernetes de maneira segura; Garantir que os certificados nunca expirem, monitorando o vencimento e renovando automaticamente;  Faz parte do segundo item, mas devo enfatizar aqui o único problema real da nossa instalação, que até hoje eu não resolvi que é:\n Como fazer o certificado usado para validar os ServiceAccount tokens chegar nos demais API Servers;  Processo de autenticação nos API Servers Para quem não é muito familizarizado com o conceito de ServiceAccounts, basta entender que são entidades Kubernetes que disparam a criação de Tokens que podem ser usados para acessar os API Servers.\nÉ possível habilitar vários métodos de autenticação em um cluster Kubernetes, mas basicamente os dois comumente em clusters fora da nuvem são:\n Certificados; Tokens de Service Accounts.  Os tokens de Service Accounts são criados para viabilizar uma maneira das aplicações em execução nos Clusters acessarem os API Servers sem grandes dificuldades. Eles são montados como volumes em todos os Pods, e se a aplicação for \u0026lsquo;linkada\u0026rsquo; com as libs clientes de Kubernetes, sequer precisará de configuração para ser usada - elas \u0026lsquo;se descobrem\u0026rsquo; usando o modo \u0026lsquo;in-cluster\u0026rsquo; de configuração.\nClaro que, por segurança, os tokens de Service Accounts não contam com permissão para fazer absolutamente nada. Mas esse mecanismo trivializa toda a maneira de fazer chegar as credenciais nas aplicações. Basta que você faça as associações necessárias por meio de diretivas RBAC como clusterrolebindings ou rolebindings.\nPor que estou compartilhando isto aqui? Porque, em algum lugar perdido na documentação, tem uma informação relevante que muita gente desconhece: como é validado um token de Service Account? Como estes são gerados?\nO processo é bem simples: naturalmente, os tokens são gerados pelo Kubernetes Controller Manager. Já que ele tem todos os Controllers que fazem as coisas, nenhuma surpresa aqui.\nA parte importante é este parâmetro de configuração do Controller Manager:\n--service-account-private-key-file string Filename containing a PEM-encoded private RSA or ECDSA key used to sign service account tokens. Ela indica a chave privada do certificado usado para assinar os tokens.\nKube Controller gera, mas quem é acessado é o Kubernetes API Server, certo? Sim, e, para fazer a verificação dos tokens, ele também conta com um parâmetro de confguração:\n--service-account-key-file stringArray File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. If unspecified, --tls-private-key-file is used. Must be specified when --service-account-signing-key is provided Aqui você pode usar tanto a chave privada quanto a chave pública, embora a lógica indique que você deve usar a chave pública.\nOu seja, você precisa de um par chave/certificado para lidar exclusivamente com a validação de tokens de service accounts.\nOk, mas e daí? Por que você está perdendo tanto tempo explicando isso?\nSe você vai levantar um ambiente com um único Master Kubernetes, que executará os três componentes que fazem o Control Plane Kubernetes, isso aqui não é um problema. Mas acredito que essa topologia não é exatamente a desejada nem para um cluster simples de ambiente de desenvolvimento.\nIdealmente, você instalará pelo menos dois Masters Kubernetes para executar os componentes de Control Plane. Como eles vão compartilhar a mesma configuração, você precisa fazer o mesmo par chave/certificado chegar em todos os seus Masters, ou você sentará em uma bomba relógio sem saber.\nEm nossa primeira instalação, eu apenas reproduzi os procedimentos de instalação individual de cada máquina. Consequentemente, cada Master Kubernetes recebeu um par diferente de certificados e os usou para tudo. Como resultado, metade das requisições de Pods que faziam uso de ServiceAccounts falhavam, pois o \u0026lsquo;outro\u0026rsquo; API Server estava configurado com uma chave diferente que reportava o token como inválido. Como metade das vezes funcionava, o problema passou batido por um tempo.\nAlém desse problema, como cada kube-controller-manager está configurado com uma chave privada diferente, os tokens eram gerados de maneira inconsistente, às vezes por uma chave, às vezes por outra.\nEm situações em que isso acontece, é necessário deletar TODOS os tokens de ServiceAccounts e aguardar que sejam regerados.\n Cicatrizes de batalha: se algum dia você for pego por um problema desse tipo por qualquer razão, você irá descobrir que o kube-controller-manager tme certa má vontade para agir na velocidade que você quer.\n  Na verdade, ele tem certa má vontade para agir em larga escala com todos os componentes. É importante saber disso ao definir escalas para o seu cluster. Observe os seguintes parâmetros de configuração:\n --concurrent-deployment-syncs int32 Default: 5 The number of deployment objects that are allowed to sync concurrently. Larger number = more responsive deployments, but more CPU (and network) load --concurrent-endpoint-syncs int32 Default: 5 The number of endpoint syncing operations that will be done concurrently. Larger number = faster endpoint updating, but more CPU (and network) load --concurrent-namespace-syncs int32 Default: 10 The number of namespace objects that are allowed to sync concurrently. Larger number = more responsive namespace termination, but more CPU (and network) load --concurrent-replicaset-syncs int32 Default: 5 The number of replica sets that are allowed to sync concurrently. Larger number = more responsive replica management, but more CPU (and network) load --concurrent-resource-quota-syncs int32 Default: 5 The number of resource quotas that are allowed to sync concurrently. Larger number = more responsive quota management, but more CPU (and network) load --concurrent-service-endpoint-syncs int32 Default: 5 The number of service endpoint syncing operations that will be done concurrently. Larger number = faster endpoint slice updating, but more CPU (and network) load. Defaults to 5. --concurrent-service-syncs int32 Default: 1 The number of services that are allowed to sync concurrently. Larger number = more responsive service management, but more CPU (and network) load --concurrent-serviceaccount-token-syncs int32 Default: 5 The number of service account token objects that are allowed to sync concurrently. Larger number = more responsive token generation, but more CPU (and network) load Levando em consideração que a maioria dos usuários modernos faz uso de Cloud Providers e sequer conta com acesso a qualquer tipo de parametrização do Control Plane, este tipo de informação passa batido. Para ambientes on-prem ou instalações \u0026lsquo;não-managed\u0026rsquo;, é necessário ter noção que existe um \u0026lsquo;throttling\u0026rsquo; interno de operações que pode exigir um certo tuning para sua realidade.\nAlternativas old school ao processo de instalação Como falei, infelizmente, na minha época, Kubernetes \u0026lsquo;the Hard Way\u0026rsquo; na realidade era o \u0026lsquo;the only way\u0026rsquo;. Então minha familiaridade com as técnicas de lançamento de clusters é extremamente limitada pela acomodação e falta de tempo de real P\u0026amp;D do que há no mercado.\nMas vamos tentar cobrir algumas coisas:\nkubeadm A ferramenta \u0026lsquo;de-facto\u0026rsquo; para apoio na instalação de clusters Kubernetes. Cobrada, inclusive, em provas de certificação. Ela faz uma série de semi automatizações de etapas óbvias e configura os elementos seguindo um padrão que, ainda que você opte por não usá-la, vale a pena conhecer para deixar o mais parecido possível.\nComo o post listado acima na época de sua criação, kubeadm admite que as máquinas em que o cluster Kubernetes será configurado já foram criadas e provisionadas de uma maneira mínima.\nA principal crítica é que é relativamente não amistosa para automação.\nkops Uma das primeiras ferramentas que visava automatizar totalmente a criação de clusters que me lembro surgir.\nO kops não apenas configura Kubernetes em máquinas, ele as provisiona também.\nPor essa característica, nunca tive a oportunidade de usá-lo.\nKops opera e é considerado production-grade para AWS; suporte para GCE e Openstack estão em Beta, Azure e DigitalOcean Alpha. Embora ainda exista referências para que um suporte a VMWare Vsphere estava em estágio Alpha também, na verdade o pouco código que existia já foi removido da codebase.\nkubespray O kubespray é um projeto que envolve uma quantidade absurdamente gigantesca de Tasks Ansible que visa prover automação total do processo de instalação, não apenas se preocupando com o lado \u0026lsquo;kubernetes\u0026rsquo; da coisa: ele faz \u0026lsquo;finalizações\u0026rsquo; específicas, dependendo se você está usando um cloud provider ou não, e automatiza inclusive ações relacionadas aos CNI, que o kubeadm não faz.\nAqui uma nota relevante: o kubespray usa o kubeadm; é uma centenas de etapas do seu processo de automação de clusters.\nJá tive a oportunidade de subir alguns clusters usando Kubespray. A experiência gera um tipo de \u0026lsquo;mixed feeling\u0026rsquo;. Como assim?\nO kubespray resolve o problema de deploy de clusters Kubernetes. Ele é \u0026lsquo;production ready\u0026rsquo;. Ele funciona.\nMas ele também tenta fazer uma quantidade gigantesca de coisas para atender a um amplo público alvo. O porte do projeto cresce num ritmo incrível, assim como sua complexidade. Já tentou administrar um playbook ansible com cerca de mil tasks das quais muitas são interligadas? Não? Pois é, esse é o desafio do kubespray.\nEle demora. Afinal, são mais de 1000 tasks ansible.\nÀs vezes, ele falha, porque bugs do Ansible, ou bugs do python, ou bugs em geral.\nÀs vezes, ele falha por alguma razão relevante, mas a depuração é particularmente desafiadora, em especial se você tiver pouca familiaridade com ansible (e não achar a mensagem de erro no Google!).\nÀs vezes, ele não faz o que você precisa, e aí você precisa criar vergonha na cara e tentar implementar a feature na árvore do Kubespray, como eu deveria ter feito porém não fiz. Acredite, é bem mais conveniente tentar fazer um pull request que manter patches para os playbooks, porque eles mudam de release para release.\nO Kubespray vai demandar bem mais do que apenas aprendeer a executá-lo.\nAssim como tudo no universo on-prem, vai demandar mais tempo do que você gostaria de dedicar para poder usá-lo de maneira produtiva no seu ambiente.\nAlternativas modernas ao processo de instalação Eu já conheço pouco das soluções \u0026lsquo;old school\u0026rsquo;. De qualquer forma, já ouvi falar de uma ou outra que acredito que vale a pena ser listada aqui para referência\nKubeone Kubeone, da Kubermatic, não é exatamente novíssimo, mas a versão \u0026lsquo;GA\u0026rsquo; 1.0 do projeto foi lançada em agosto de 2020.\nA ideia é implementar a mesma proposta do Kubespray: ser uma ferramenta para \u0026ldquo;lifecycle management\u0026rdquo; de clusters Kubernetes, seja na nuvem ou em ambientes on-prem.\nTudo nesta solução parece fantástico, extremamente promissor e promete substituir praticamente todo o meu trabalho de automação com custom Terraform/Kubespray. Obviamente, vai demantar testes extensivos para entender as limitações.\nQual a pegadinha? Não sei, não descobri. Fora o fato de que existe uma oferta enterprise por parte da Kubermatic, não consegui perceber de imediato algum problema que torne seu uso inviável.\nSe você vai começar a jornada Kubernetes e não tem absolutamente nada, recomendo olhar este projeto antes. Se gostar, me conta!\nGardener Na linha dos projetos ultra modernos, que tal usar Kubernetes para provisionar clusters Kubernetes? Esta é a proposta do projeto Gardener.\nAqui, o Control plane dos clusters a serem criados pelo Gardener (\u0026ldquo;shoot clusters\u0026rdquo;) executará como Pods em um cluster Kubernetes (\u0026ldquo;seed cluster\u0026rdquo;). Genial, não? Qual o propósito de perder tempo fazendo o deploy de máquinas para Control Plane, load balancers para HA em caso de falhas se você pode executar tudo como Pods.\nA parte curiosa é que este projeto foi desenvolvido pela SAP. Mais curioso ainda é saber que a SAP era um dos top 10 commiters para o projeto Kubernetes em 2019 (caiu um pouco em 2020).\nTalvez a proposta arrojada desencoraje pessoas com menos experiência com Kubernetes. De qualquer forma, está listado aqui porque acho relevante!\nAWS EKS Distro 2020 foi um ano tão estranho que a AWS resolveu liberar o \u0026ldquo;Amazon EKS Distro\u0026rdquo;, uma distribuição código aberto do Kubernetes.\nNão acredita? Está tudo aqui.\nAWS EKS Distro não tem absolutamente nada a ver com os dois exemplos anteriores, e não vai ajudar a provisionar ambientes, mas \u0026ldquo;empacota\u0026rdquo; tudo que é necessário para executar um cluster Kubernetes no padrão AWS. Por exemplo, o guia oficial de como usar o EKS-D em um cluster é usando kops, mencionado acima.\nServe mais como referência sobre como a AWS configura seus ambientes, para se inspirar se deve fazer algo parecido.\nEste ano haverá o lançamento do AWS EKS Anywhere, que possivelmente fará o deploy de clusters automaticamente (?). Mas dificilmente será \u0026lsquo;open source\u0026rsquo;.\n Alguma sugestão de software para incluir na lista? Deixe-me saber! Compartilhe sua experiência comigo que eu apresento aqui com créditos!\n",
    "ref": "/marcelo/blog/kubernetes-on-prem-4/"
  },{
    "title": "Kubernetes on-premises - parte 3",
    "date": "",
    "description": "Tráfego de entrada em clusters Kubernetes",
    "body": "Outros capítulos da série Kubernetes on-premises:\n Kubernetes on-prem - parte 1: Introdução. Kubernetes on-prem - parte 2: Redes em Kubernetes - CNI e Calico.  Clusters Kubernetes: modelo de tráfego auto-contido Na parte 2 da série \u0026ldquo;Kubernetes on-premises\u0026rdquo;, abordei, de maneira bastante superficial, sobre o fato de que o Kubernetes gosta da ideia de abstrair certas implementações, como a execução de contêineres, provisionamento de volumes para armazenamento persistente e especialmente redes.\nPara viabilizar as principais ideias do Kubernetes de provisionamento automático, self healing e alta disponibilidade de seus workloads, o Kubernetes faz uso de uma série de recursos que só são implementados dentro do próprio cluster, como o conceito de Services.\nA simples comunicação de contêineres em um cluster Kubernetes com o \u0026lsquo;resto do mundo\u0026rsquo; não é trivial. Na própria parte 2 listei uma série de considerações que você deve fazer a respeito antes de integrar um cluster Kubernetes com seu datacenter \u0026ldquo;tradicional\u0026rdquo; - e acredite, não cheguei nem perto de exaurir a discussão.\nMas o primeiro desafio real que o Kubernetes te impõe e que alguém com pouca experiência certamente não está preparado é: como fazer o resto do mundo se conectar aos meus sistemas dentro do cluster? Que opções temos?\nService do tipo ClusterIP Para garantir a alta disponibilidade e recuperação rápida de falhas, tudo no Kubernetes gira em torno dos Services, em particular do tipo ClusterIP.\nEu poderia descrever o que são Services e como são implementados a partir do Kube-proxy e do CoreDNS, mas eu prefiro mostrar da seguinte forma:\n Eu tenho três Pods \u0026lsquo;nginx\u0026rsquo;, cada um executando em um servidor:  # kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx1 1/1 Running 0 2m20s app=nginx nginx2 1/1 Running 0 2m11s app=nginx nginx3 1/1 Running 0 2m8s app=nginx  Eu crio um Service associado ao label app=nginx, como visto na coluna SELECTOR. Ele recebe um IP de um range especial, cujo padrão é 10.96.0.0/12.  # kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx ClusterIP 10.96.0.128 \u0026lt;none\u0026gt; 80/TCP 114s app=nginx  Isso quer dizer que o Service está associado aos três Pods, como pode ser visto no objeto Endpoints:  # kubectl get endpoints NAME ENDPOINTS AGE nginx 192.168.18.136:80,192.168.19.8:80,192.168.22.135:80 2m58s  A partir de um outro Pod ou de um Nó do cluster, você pode acessar os Pods nginx (que estão na Namespace teste) usando o nome DNS nginx.teste.svc.cluster.local. Você verá que as conexões são balanceadas entre os três:  # for i in {1..1000} ; do curl -s nginx.teste.svc.cluster.local. ; done | sort | uniq -c 334 Voce se conectou ao pod nginx1 333 Voce se conectou ao pod nginx2 333 Voce se conectou ao pod nginx3 Fantástico, não? O que é ainda melhor, se você deletar um dos pods, você não precisa se preocupar com nada: o Kubernetes irá atualizar os Endpoints do Service o que resultará no Pod excluído ser removido do balanceamento praticamente de forma instantânea:\n# kubernetes delete pod nginx1 pod \u0026quot;nginx1\u0026quot; deleted # kubernetes get endpoints NAME ENDPOINTS AGE nginx 192.168.19.8:80,192.168.22.135:80 19m # for i in {1..1000} ; do curl -s nginx.teste.svc.cluster.local. ; done | sort | uniq -c 500 Voce se conectou ao pod nginx2 500 Voce se conectou ao pod nginx3 Esse exemplo é \u0026ldquo;estúpido\u0026rdquo; porque uso pods criados manualmente e foi feito apenas para entender o conceito. Mas imagine: se os Pods fossem mantidos por um ReplicaSet de um Deployment que fizessem uso de Liveliness e Readiness checks, é o conceito de Service que:\n Aguardam Pods recém criados estarem prontos para serem incluidos no balanceamento; Remove do balanceamento Pods que entram em estado NotReady; Viabilizam as RollingUpdates dos Deployments sem causar impacto aos usuários;  É o conceito de Service, associado aos Controllers que mantém o estado dos objetos do cluster Kubernetes que dão a esta tecnologia o \u0026lsquo;Wow factor\u0026rsquo; de abstrair algo que todo mundo precisa e, no mundo real, é extremamente complexo de fazer.\nMas enfim, me empolguei com o exemplo, e acabei perdendo o foco. O que eu realmente queria falar é que, embora Services sejam o máximo, ele está visível apenas \u0026ldquo;dentro\u0026rdquo; cluster Kubernetes 1 e não te ajuda em nada para resolver o problema de fazer chegar comunicação externa aos seus Pods.\n 1 Isso é verdade do ponto de vista do Kubernetes \u0026lsquo;puro e simples\u0026rsquo;, existem maneiras de integrar tanto a resolução de nomes quanto a divulgação dos endereço IP dos ClusterIPs usando Calico, por exemplo. Quer saber mais sobre isso? Entre em contato comigo e me deixe saber!\n  Cicatrizes de batalha: prestou atenção no CIDR padrão usado pelos Services do tipo ClusterIP? Se, na sua empresa, você usar o range 10.96.x.y para outras coisas, você terá prolemas com o valor padrão, e acredite: mudar isso após a implantação vai ser uma das piores experiências da sua vida.\n Service do tipo NodePort Uma das maneiras encontradas para resolver o problema da comunicação do mundo externo com clusters Kubernetes foram os Services do tipo NodePort.\nEste tipo de configuração literalmente escolhe uma porta de um range pré determinado (30000-32767) e a abre em todos os nós daquele cluster Kubernetes, que direcionará todas as comunicações aos Endpoints associados ao Service.\nSe você achou feio e tosco, é porque é mesmo. É algo que é operacional e \u0026ldquo;dá para conviver\u0026rdquo;, mas é uma solução totalmente desprovida de elegância.\nE se eu disser que é a única solução oferecida nativamente pelo Kubernetes para viabilizar o acesso externo?\nNão acredita?\nService do tipo Loadbalancer Está na ponta da língua das pessoas a resposta de que NodePort não é a única maneira, afinal também temos os Services do tipo Loadbalancer, certo? Então, gostaria de fazer um \u0026lsquo;quote\u0026rsquo; da definição da documentação oficial:\n On cloud providers which support external load balancers, setting the type field to LoadBalancer provisions a load balancer for your Service.\n Ou seja, em cloud providers que suportem, você vai conseguir usar este recurso com facilidade. Fora delas, você está por sua própria conta; os services Loadbalancers não fazem absolutamente nada e o Kubernetes, por si só, não vai te ajudar a resolver este problema 2.\n 2 O projeto Metallb, iniciado três anos atrás, existe com o objetivo de preencher esse \u0026lsquo;gap\u0026rsquo;, mas não é uma solução oficial integrada ao Kubernetes, e é descrito pelo próprio projeto como Beta. Estando dissociado do Kubernetes, a evolução e maturidade de ambos os projetos estão completamente desacopladas.\n Ingress Havendo esgotado as possibilidades de Services para atender ao nosso problema, pelo menos ainda temos Ingress para servir ao menos o tráfego HTTP(S)? do nosso cluster nativamente, certo?\nErrado! Não existe controller nativo no Kubernetes para Ingress! Novamente, da documentação oficial:\n You must have an Ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect.\n O único consolo é que, diferentemente dos Services, em que temos apenas uma única opção (Metallb), a oferta de Ingress Controllers para clusters Kubernetes é bem maior, inclusive com algumas opções pagas que oferecem suporte de fornecedores como Nginx, HAproxy e F5.\nO fato de haver diversas opções funciona de maneira semelhante ao CNI: acaba atrapalhando mais que ajudando aqueles que estão começando agora no universo Kubernetes. Como escolher o que melhor vai atender às suas necessidades?\nHá um agravante: a maior dificuldade de lidar com Ingress Controllers não é nem os softwares em si, e sim o fato de que o Ingress resource do Kubernetes é extremamente espartano em termos de parâmetros de configuração, ao se comparar com o que o \u0026lsquo;mercado\u0026rsquo; demanda. E isso não é acidental:\n This API has intentionally been kept simple and lightweight, but there has been a desire for greater configurability for more advanced use cases. Work is currently underway on a new highly configurable set of APIs that will provide an alternative to Ingress in the future. These APIs are being referred to as the new “Service APIs”. They are not intended to replace any existing APIs, but instead provide a more configurable alternative for complex use cases. For more information, check out the Service APIs repo on GitHub.\n Isso quer dizer duas coisas:\n Em um futuro próximo, será necessário mudar tudo; Não existe um Ingress Controller igual ao outro.  Cada Ingress implementa um conjunto distinto de funcionalidades - e de uma forma diferente. Projetos que dependem de Ingress acabam lançando seus próprios Controllers (como o Istio Service Mesh ou o Kong Api Service). As configurações são tipicamente realizadas por meio de Annotations nos objetos Ingress ou por meio de CRDs. Basicamente cada Ingress Controller é uma história diferente, e será necessário uma quantidade não razoável de leitura e testes para determinar qual o mais apropriado para cada uso.\nOutra coisa que é importante dissociar é o Ingress Controller (o programa que atuará monitorando o Kubernetes e buscando reconciliar as configurações com as mudanças) dos softwares que efetivamente fazem o \u0026ldquo;proxy reverso\u0026rdquo; recebendo as conexões e entregando aos Pods no seu cluster. Existem todo tipo de opção e topologias. A familiaridade prévia e experiências (positivas ou negativas) podem e devem guiar sua escolha. Se você tem ampla experiência com HAproxy, será muito mais fácil tentar identificar problemas, gargalos e oportunidades de \u0026lsquo;tuning\u0026rsquo; se você optar por um Ingress Controller que faça uso do HAproxy que com algo completamente novo, ainda que mais \u0026lsquo;moderno\u0026rsquo; como Traefik ou Envoy.\nEsta planilha, oferecida pelo site Kubedex, contém uma comparação lado-a-lado de alguns dos principais Ingress Controllers; infelizmente não há uma marcação de quando foi a última atualização.\nEste outro post traz uma breve descrição e é relativamente recente (07/2020).\nEu não vou me aprofundar em análises dos diversos Ingress Controllers existentes porque eu tenho experiência real com apenas dois. Vou contar um pouco da minha experiência com eles.\nNginx Ingress Controller Antes de falar qualquer outra coisa, é importante deixar dois fatos muito importantes sobre o Nginx Ingress Controller:\n Embora esteja listado no Github como estando abaixo do projeto Kubernetes, o conjunto de pessoas envolvidas na manutenção e evolução do Ingress Controller é outro completamente diferente do projeto Kubernetes em si, o que faz com que o nível de maturidade do software, assim como o Metallb, seja outro completamente diferente. Este projeto não é o mesmo que oferece suporte oficial pela empresa Nginx. Este projeto usa a versão \u0026lsquo;community\u0026rsquo; do Nginx; a versão mantida pela empresa (que foi comprada pela F5) usa a versão Plus e uma \u0026lsquo;codebase\u0026rsquo; completamente diferente.  As duas considerações acima não significam que o projeto seja ruim, mas é importante ter essa noção antes de implantar o que provavelmente vai se tornar o ponto mais crítico da sua infraestrutura.\nPor exemplo: o Nginx Community Edition não oferece a funcionalidade de reconfiguração dinâmica dos seus upstreams por meio de API; este código está disponível apenas na versão Plus.\nCada \u0026lsquo;reload\u0026rsquo; de configuração demanda a substituição dos workers Nginx por dois novos, enquanto os processos antigos se mantém vivos por tempo suficiente para permitir que as conexões encerrem. Durante este tempo, o Nginx consumirá o dobro de memória RAM.\nDependendo da quantidade de \u0026lsquo;Reloads\u0026rsquo; do seu cluster, o uso deste Ingress Controller pode se mostrar inviável.\nO projeto entendeu que isso era uma séria limitação bem cedo, e implementou sua própria versão de um handler Lua que faz as vezes da API privada para mitigar esse problema.\nAinda assim, cenários como o abaixo podem ocorrer:\nA imagem acima representa o consumo de memória RAM de servidores que executavam exclusivamente Nginx Ingress Controller em um cluster Kubernetes de produção. Os servidores contavam com 40GB de RAM. A versão já contava com o módulo Lua. É possível observar dois problemas:\n Não é possível mitigar todos os Reloads, mesmo com o módulo Lua; O código do Nginx é sólido e battle tested; o código do módulo Lua foi criado para este projeto, e está sujeito a problemas, como memory leaks que fazem o processo Nginx ocupar cada vez mais RAM com o tempo.  Esse memory leak da imagem acima já foi corrigido, e dificilmente a esmagadora maioria das pessoas passaria pela situação acima. Mas acredito que este relato seja suficiente para se entender que não é um simples componente deploy-and-forget para Kubernetes.\nHAproxy Ingress Controller Apenas para referência, eu implanto clusters Kubernetes antes mesmo do primeiro Ingress Controller existir.\nE nessa época, bem antes da empresa HAproxy se interessar por Kubernetes, em uma época em que apenas o Nginx Ingress Controller sem módulo Lua existia, um herói resolveu fazer seu próprio Ingress Controller usando o HAproxy. O nome deste herói é Joao Morais e seu HAproxy Ingress Controller consegue a façanha de ainda hoje ser retornado acima do \u0026ldquo;oficial\u0026rdquo; no Google.\nComo temos a sorte de ter Joao Morais trabalhando na nossa empresa, a escolha de qual Ingress Controller usar é meio que um \u0026lsquo;no-brainer\u0026rsquo;. Ele acompanha o uso diário de nossos servidores que atuam como Ingress, observando as limitações e identificando os problemas de escala e performance que vão surgindo. E acredite: nosso principal cluster Kubernetes oferecem problemas extremamente desafiadores que eu ouso dizer que não seriam atendidos por nenhum dos demais projetos.\nO HAproxy Ingress Controller implementa um set de funcionalidades muito semelhante ao do Nginx Ingress Controller, que possivelmente é o mais usado atualmente. Por isso, é uma excelente segunda opção para testes, mesmo não sendo tão conhecido (embora ainda tenha mais estrelas no Github que o Ingress Controller HAproxy da própria HAproxy!).\nSendo ambos \u0026lsquo;Annotation based\u0026rsquo;, não é difícil manter ambos coexistindo no mesmo cluster. O único incômodo será manter dois conjuntos de annotations distintos para configurar os objetos Ingress.\nA Annotation prefix padrão do Nginx Ingress Controller é nginx.ingress.kubernetes.io, enquanto no HAproxy Ingress Controller é ingress.kubernetes.io.\nComo pontos positivos, posso ressaltar que:\n Se comporta extramemente bem em clusters com objetos completamente mal configurados; Consome quantidade irrisória de CPU, mesmo com contínua necessidade de reconciliação de configurações; Pode solicitar certificados Letsencrypt nativamente para seus backends sem softwares adicionais (salvo engano, foi o primeiro a implementar esta funcionalidade!);  Como pontos negativos, ficam que:\n Também é reload-oriented; situações que demandam \u0026lsquo;full-reload\u0026rsquo; também impactam a memória, mas o tamanho do processo do haproxy tende a ser previsível, em contraste com o tamanho variável do Nginx devido ao módulo Lua; Menos mantenedores e menos colaboradores: como somos da #ResistênciaOPs, nós não colaboramos com qualquer tipo de código para o João, e se ele ganhar na Mega da Virada, pode ser que o projeto morra;  Conclusão Infelizmente, esta é mais uma decisão difícil que um pretendente a administrador Kubernetes precisará tomar antes mesmo de colocar o primeiro sistema para rodar.\nSe sua única preocupação é o direcionamento de tráfego HTTP/HTTPS, é uma questão de escolher qual o Ingress Controller que melhor atende ao seu caso.\nAgora caso a conexão direta com os Pods sem intermediário seja necessária, você incorrerá em problemas. A única opção real é o projeto Metallb, e embora performe bem, tanto a documentação quanto sua implementação não são tão diretas ou triviais de serem aplicadas. Existe muito mais material hoje que em 2018 quando foi lançado, ainda assim vai demandar um esforço bem maior que o necessário para instalar todo o resto do cluster Kubernetes, por exemplo, para fazer uma implantação madura que vá além do \u0026lsquo;funcionou!\u0026rsquo;.\nAlguns Ingress Controllers suportam \u0026lsquo;layer 4\u0026rsquo; além de HTTP/HTTPS. Se isso for uma necessidade, essa questão já serve para \u0026lsquo;afunilar\u0026rsquo; as opções. O Nginx, HAProxy de Joao e o Voyager eu sei que fazem de maneira relativamente simples. Salvo engano o Kong também suporta este recurso.\nPretende usar algum tipo de Service Mesh, provavelmente eles já resolvem o problema Ingress para você. Leia e torça que a solução te atenda totalmente; dificilmente você encontrará soluções intercambiáveis nesse assunto.\nAgora se você está completamente desamparado de ideias e com preguiça de testar, o mais usado - e possivelmente com mais chances de encontrar e receber ajuda - é o Nginx Ingress Controller \u0026ldquo;oficial\u0026rdquo; do Kubernetes. O HAproxy de Joao Morais tende a depender única e exclusivamente dele para apoio, já que nós, que trabalhamos com ele, raramente damos qualquer tipo de ajuda em canal do Slack ou coisas do tipo por sermos funcionários públicos preguiçosos.\nAinda assim, recomendo testes regulares e acompanhamento de pelo menos umas duas ou três opções diferentes, se possível, para evitar que, algum dia, você seja contemplado com um bug fatal que ocorra apenas no seu ambiente e que impeça a atualização para novas versões do seu escolhido (como aconteceu conosco com o Nginx Ingress Controller por volta da versão 0.17).\nMigrar de Ingress Controller a toque de caixa em um fim de semana não é uma experiência que eu recomende a ninguém!\n",
    "ref": "/marcelo/blog/kubernetes-on-prem-3/"
  },{
    "title": "Kubernetes on-premises - parte 2",
    "date": "",
    "description": "Redes em Kubernetes - CNI e Calico",
    "body": "Um pouco atrasado pelos diversos \u0026ldquo;detours\u0026rdquo;, mas seguimos com a continuação da Parte 1!\n Kubernetes on-prem - parte 1: se você perdeu o initial rambling!  Redes no Kubernetes: componentes plugáveis A primeira regra sobre redes no Kubernetes é que você não fala sobre redes no Kubernetes.\nAgora sério: o Kubernetes, conceitualmente, descreve vários aspectos de como deve ocorrer a comunicação entre seus componentes. Mas ele não descreve o como implementar; ele \u0026ldquo;terceiriza\u0026rdquo; essas linhas gerais para outros projetos.\nNão é apenas com a rede que o Kubernetes faz isso; recentemente, comentei sobre como usar volumes CEPH no Kubernetes usando plugins que implementam a Container Storage Interface. E acredito que praticamente todas as pessoas interessads em Kubernetes ouviram a notícia de que o suporte a Docker pelo kubelet é considerado \u0026ldquo;deprecated\u0026rdquo; com o lançamento da versão 1.20. Isso aconteceu basicamente porque Kubernetes implementa a Container Runtime Interface, que desacopla do Kubernetes a função de gerenciamento de containers no sistema operacional e outros detalhes.\nComo o Docker não implementa essa interface, ele atua como um intermediário cujo papel é inserir overhead entre a integração do Kubernetes com o containerd, que é quem efetivamente executa as operações de containers. E se o containerd implementa CRI, por que não falar diretamente com ele, cortando o atravessador?\nEntão, assim como usa CRI e CSI, para redes, o Kubernetes aplica a Container Network Interface (CNI).\nNão é nenhuma surpresa que o projeto faça isso. Não faz sentido implementar código de operação de containers, ou de storages, ou mesmo de redes dentro do próprio Kubernetes (embora ainda haja código desse tipo!) se existem outros projetos que executam estas tarefas de maneira eficiente. Basta apenas que haja interesse, por parte dos demais projetos, de implementar esse conjunto.\nE há. Bastante interesse, diga-se de passagem - são mais de 20 projetos e empresas diferentes oferecendo todo tipo de produto (com todo tipo de preço também).\n Espaço para ironia: quando as opções estão limitadas a duas alternativas, normalmente é bem mais fácil escolher. Por outro lado, quanto tempo você já perdeu tentando escolher um filme para assistir no Netflix?\n Se eu não sei absolutamente nada sobre Kubernetes, e sendo o CNI crítico para o funcionamento - não é uma decisão delegável para mais tarde - como faço para escolher?\nVamos olhar como é o processo de \u0026ldquo;escolha\u0026rdquo; dos administradores Kubernetes.\nCNI em Cloud Providers Usuários dos grandes Cloud Providers também podem escolher usar entre os diversos CNIs existentes. Mas normalmente não há qualquer razão para fazer isso, pois as suas ofertas de clusters \u0026lsquo;managed\u0026rsquo; vêm com CNIs próprios.\nSe estou usando meu cluster EKS e a AWS oferece um plugin CNI que é nativamente compatível com seus VPCs, permitindo que Pods assumam o mesmo endereçamento IP das subnets criadas nas várias Availability Zones, por que você iria querer usar outra coisa? Mesmo se você quiser que o endereçamento dos Pods use outra rede, basta configurar o CNI AWS para isso. Até existe a possibilidade, ainda que a AWS se exima do suporte. Mas normalmente não há uma boa justificativa para isso.\n Nota: Administradores avançados, que preferem, por exemplo, manter controle sobre seus Control Planes, podem optar por levantar clusters Kubernetes em instâncias EC2. Mas aí são administradores avançados, que provavelmente não estão lendo este texto e não tem qualquer dificuldade em escolher seu CNI.\n Resumindo: em Cloud Providers, a escolha do CNI é uma \u0026ldquo;non-issue\u0026rdquo;.\nCNIs \u0026ldquo;Enterprise\u0026rdquo; Diversas grandes empresas do mercado já perceberam que o interesse em Kubernetes é crescente e desenvolveram ofertas próprias de CNIs proprietários compatíveis com seus produtos, como o Contiv para CISCO ACI ou o VMWare NSX-T CNI, normalmente com seu uso exclusivamente atrelado a existência de equipamentos ou licenças de software no seu ambiente.\nGrandes empresas com contratos milionários têm suporte \u0026ldquo;enterprise\u0026rdquo; e a possibilidade de fazer uso de Kubernetes on-prem usando componentes específicos que se integram com alguns de seus sistemas pagos. Em situação de pouca experiência, talvez valha a pena investir nisso. Não é um caminho que eu seguiria, mas eu sou um nerd #resistenciaOPs que trabalha com Linux há 22 anos.\nEu não posso oferecer qualquer opinião sobre eles, nem mesmo ruim sobre CNIs proprietários. Embora seja um grande defensor do uso de software livre em todos os lugares, entendo que, por exemplo, um ponto positivo para seu uso é que pode ser uma forma de angariar \u0026ldquo;aliados\u0026rdquo; (na forma de administradores CISCO ou VMWare, por exemplo) para participar de projetos loucos implantando Kubernetes on-prem, dividindo trabalho que, de outra forma, será feito somente por você.\nResumindo: mesmo se a sua \u0026lsquo;nuvem privada Kubernetes\u0026rsquo; não vier pronta do fornecedor, grandes empresas podem optar por integrar Kubernetes on-prem com softwares proprietários fornecidos por alguns desses.\nCNIs para Kubernetes on premises Infelizmente, não temos aqui um meio termo de dificuldade. Pulamos diretamente de \u0026ldquo;nem precisei pensar sobre isso\u0026rdquo; para \u0026ldquo;leia TUDO sobre TODOS para escolher o que melhor se aplica para você\u0026rdquo;.\nA oferta é enorme e, a cada ano, alguma equipe de heróis geniais de empresas obscuras saem com uma alternativa que seja mais interessante para seus objetivos. E aí está a beleza do universo software livre: se você é um herói genial, é bem provável que ache, em meio aos bilhões de habitantes da terra, outros heróis com finalidades afins para contribuir para o seu projeto.\nVou fazer um leve \u0026lsquo;gossiping\u0026rsquo; dos projetos a que fui exposto ao longo dos últimos quatro anos.\nOs \u0026lsquo;big 4\u0026rsquo; Basicamente 4 CNis são as opções mais recorrentemente faladas no universo Kubernetes. São eles:\nCalico O projeto Calico de longe é a opção mais comum em instalações, principalmente por ser um dos \u0026lsquo;earlier adopters\u0026rsquo; do recurso de NetworkPolicies no Kubernetes.\nAinda hoje a recomendação padrão para aplicação de Networkpolicies em clusters EKS na AWS é feita por meio do deploy de componente específico do Calico, citado na documentação oficial.\nO Azure AKS oferece suporte a Calico nativamente, e mantém uma tabela comparativa de features.\nHá a oferta de uma versão Enterprise do Calico pela Tigera, que é uma empresa praticamente dedicada integralmente a conectividade e segurança de rede para microsserviços. A versão enterprise inclui algumas funcionalidades que tendem a fazer brilhar os olhos dos profissionais de segurança de rede, mas que não tornam a versão \u0026ldquo;community\u0026rdquo; incompleta.\nUma das principais barreiras para o uso do Calico era seu site que remetia à decada de 90, documentação confusa e pouquíssimo material de treinamento; todos esses aspectos tem sido \u0026ldquo;atacados\u0026rdquo; pela Tigera e estão muito melhores, contando até com um Academy Tigera Certified Calico Operator: Level 1 gratuito!\nWeave O Weave net também foi um projeto existente à época da minha implantação original de Kubernetes. Este CNI foi criado pela Weave É uma empresa muito popular entre usuários Kubernetes, principalmente por ter desenvolvido o hoje oficialmente adotado pela AWS eksctl.\nA Weave não tem um foco exclusivo em networking, muito pelo contrário. Atua em diversas frentes de desenvolvimento com produtos direcionados ao deploy e observabilidade de aplicações, além de ter criado a ideia de \u0026lsquo;GitOps\u0026rsquo; e disponibilizado programas como o Flux, Flagger e Cortex. O \u0026lsquo;appeal\u0026rsquo; de manter seu CNI Weave Net se dá pela integração com seu Weave Kubernetes Platform.\nCilium O CNI Cilium sempre foi a principal (e por muito tempo, talvez a única) opção para aqueles interessados pelo uso de eBPF em especial por quem quer se livrar dos fardos que são o iptables e o kube-proxy.\nEmbora a substituição da confusão que é iptables em clusters Kubernetes não necessariamente se traduza em melhora de performance, a evolução natural da tecnologia eventualmente vai \u0026ldquo;chegar lá.\n(O Calico suporte eBPF desde a versão 3.13, salvo engano).\nExiste suporte enterprise para Cilium por meio da empresa Isovalent.\nFlannel / Canal O quarto, Flannel, pela sua simplicidade, acabou se tornando o \u0026ldquo;CNI padrão\u0026rdquo; para deploy de clusters para testes. É usado, ainda hoje, por praticamente todos os clusters nos exames CKA/CKAD.\nNota: Canal é o nome normalmente usado para o uso combinado do Flannel com os recursos para Networkpolicy do Calico; não é um CNI diferente.\nMuitos usuários mais experientes recomendam usar o Flannel para iniciar estudos com Kubernetes por não haver tantas \u0026ldquo;partes móveis\u0026rdquo;, mas é possível que a escassez de documentação e o estado atual do projeto não encorajem seu uso sequer para isso (embora parece que esteja havendo uma retomada).\nAinda que seja amplamente recomendado para aprendizado e para instalações minimalistas para laboratório e angariar conhecimento em Kubernetes, acredito que absolutamente ninguém recomende fazer qualquer uso sério de Flannel em ambientes que tenham a mínima chance de serem \u0026lsquo;graduados\u0026rsquo; à produção.\nOutros Aqui vou listar outros três que já tive algum contato, apenas para referência:\n Kube router: um projeto que tivemos a oportunidade de usar um de seus \u0026lsquo;pedaço\u0026rsquo; quando precisamos de uma solução que substituisse o kube-proxy; Contiv: desenvolvido pela Cisco; Antrea: desenvolvido pela VMWare;  Que CNI escolho e por que Calico? Essa é a primeira grande questão de quem quer instalar Kubernetes em seu datacenter, e praticamente impossível de responder caso você se encontre completamente desassistido de consultoria mais experiente.\nAgora deveria ser a hora em que eu puxo tabelas incríveis e demonstro qual o melhor CNI. Infelizmente eu não posso fazer isso, por uma simples razão: 99% da minha experiência com Kubernetes aconteceu única e exclusivamente com o uso do Calico como CNI.\nNão há como fazer comparações sensatas sem conhecer como as demais tecnologias se comportam \u0026ldquo;no campo de batalha\u0026rdquo;.\nVocê pode até consultar bons artigos de Internet, como a fantástica (e bastante recente) comparação de performance entre alguns dos CNIs mais populares:\n https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-updated-august-2020-6e1b757b9e49  Recomendo a leitura integral desse artigo, até para entender \u0026lsquo;por que Calico?\u0026rsquo;. Realmente é uma boa opção.\nVocê pode (deve!) fazer perguntas técnicas relevantes e averiguar como cada CNI se comporta com relação a:\n VXLAN/Geneve ou Layer 3 com BGP? eBPF ou iptables/ipset e ipvs? Suporte a IPv6? criptografia? data backplane dedicado ou integrado ao Kubernetes? Windows?  Ou sobre preço:\n Você é \u0026ldquo;liso\u0026rdquo; ou pode contratar suporte? Tenho um CNI proprietário disponível que integra com minha infra? Preciso ter contrato de suporte por questão de compliance?  Mas, por fim, normalmente os principais requisitos para a escolha não são tão técnicos, e sim:\n Qual a documentação mais simples (e mais clara); Qual o mais usado - para que eu tenha mais chance de ter ajuda quando encontrar problemas;  Que hoje provavelmente significa que você quer usar Calico no seu cluster.\nEscolhi. Mas e se me arrepender? Não importa o que você escolher, entenda que o CNI não é necessariamente é um casamento, mas mudanças serão dolorosas e irão demandar parada integral do cluster para ajustes, possivelmente incluindo um \u0026lsquo;kubectl delete pods -A\u0026rsquo;. Provavelmente a alternativa mais viável será subir um novo cluster com o novo CNI e migrar os workloads para posterior desativação do anterior.\nVocê pode ler a experiência de um Kubernetes admin que precisou migrar de Weave para Calico por problemas irrecuperáveis com seu Weave CNI.\nAlheio à sua escolha, tenha uma coisa em mente: você precisa dedicar uma quantidade substancial de tempo para entender adequadamente o projeto à sua escolha. Não é possível apenas se dedicar a estudar Kubernetes e achar que o plugin CNI \u0026lsquo;just-works\u0026rsquo; como se fosse algo plug-and-play. Se possível, aloque gente com forte background de rede para entender detalhes da implantação para mapear possíveis bloqueios futuros e dificuldades que virão.\nBônus: consultoria de implantação Até agora, apresentei conteúdos e conselhos genéricos e filosóficos sobre Kubernetes e CNI. Estou extremamente insatisfeito com o texto, então hora de algo mais relevante. Uma \u0026ldquo;consultoria gratuita\u0026rdquo; para pessoas interessadas em colocar a faca nos dentes e nadar contra a maré implantando Kubernetes on-prem.\nParei de falar com gerentes agora; é a vez dos #Ops.\nKubernetes realmente abstrai a rede? Claro que não. Tem um limite até onde essa abstração pode chegar.\nPara criar clusters com kubeadm, existem dois parâmetros que permitem configurar as duas redes relevantes para o Kubernetes:\n A rede de Pods: configurada pelo parâmetro \u0026ndash;pod-network-cidr; A rede de Services: diretiva \u0026ndash;service-cidr, valor default \u0026ldquo;10.96.0.0/12\u0026rdquo;;   Se você não sabe absolutamente nada sobre Kubernetes, um resumo absurdamente conciso sobre:\n os containers da sua aplicação receberão endereços IP da rede de Pods; você provavelmente vai rodar múltiplas instâncias de cada tipo de container (i.e., 2 pods para frontend, 5 para backend, 3 para filas) o Kubernetes Service é um IP virtual associado a um nome DNS que você usará para fazer um \u0026lsquo;balanceamento interno\u0026rsquo; entre containers do mesmo tipo.  Logo, uma aplicação com 2 frontend, 5 backend e 3 filas usarão 10 endereços da rede de Pods e 3 endereços da rede de Services.\n O próprio proceso de instalação do Calico recomenda que você execute o kubeadm especificando a rede de pods:\n Initialize the master using the following command:\nsudo kubeadm init --pod-network-cidr=192.168.0.0/16\n Muito importante (e também meio óbvio): certifique-se que não \u0026lsquo;overlapping\u0026rsquo; entre a rede de pods e a rede serviços.\nPara onde vão essas configurações? Diretamente para os vários elementos do cluster Kubernetes.\n api-server:  # cat /etc/kubernetes/manifests/kube-apiserver.yaml | fgrep service-cluster - --service-cluster-ip-range=10.96.0.0/12  controller:  # cat /etc/kubernetes/manifests/kube-controller-manager.yaml | egrep 'cluster-cidr|service-cluster' - --cluster-cidr=172.16.0.0/16 - --service-cluster-ip-range=10.96.0.0/12  kube-proxy:  # kubectl get configmap -n kube-system kube-proxy -o yaml | grep clusterCIDR clusterCIDR: 172.16.0.0/16 Isso quer dizer que modificar esses endereços após o cluster entrar em estado operacional também não é trivial e sem impactos.\nO que quer dizer que a segunda decisão mais importante para a qual você não está nem um pouco preparado a escolha das redes a serem usadas pelo Kubernetes.\nQue redes usar Aqui cabem algumas decisões críticas sobre como você vai querer o seu cluster e que, provavelmente, precisam ser tomadas bem antes de você aplicar o seu primeiro kubectl.\n1. Seu cluster será uma entidade isolada ou integrada ao seu ambiente corporativo? A primeira decisão crítica. Se o uso de Kubernetes pretende ser auto contido, você pode (e deve!) usar alguma rede privada completamente diferente da usada pela sua empresa e assim não ter que se preocupar com o endereçamento dos IPs de pods: basta alocar uma rede grande o suficiente (como a 172.16.0.0/16 listada acima) como sua rede de Pods e uma outra rede também grande (10.96.0.0/12) para a sua rede de Services.\nMinha recomendação, obviamente, é a de isolar os Pods do seu cluster do resto da sua empresa. Por uma razão bem simples: provavelmente o \u0026lsquo;resto\u0026rsquo; não é Cloud Native, segue o modelo de trabalho tradicional, burocrático, pouco dinâmico e especialmente pouco inteligente. Se você conseguir implantar o cluster Kubernetes como o princípio da versão 2.0 da sua empresa, todos vão ganhar. Aplicações novas usam o modelo novo; aplicações \u0026lsquo;legadas\u0026rsquo; ficam presas do lado oitentista.\nMas eu sei que isso é quase impossível.\n2. Para um cluster integrado, o Pod precisará assumir um endereço IP válido na sua rede corporativa? Esse é o pior cenário possível para a implantação de um cluster Kubernetes.\nNormalmente redes corporativas tem sua alocação de endereçamento de Intranet pensada de uma forma, e clusters Kubernetes precisam de grandes pools de endereços para serem viáveis, em particular se você vai usar o CNI Calico.\nNão há uma boa razão para alocar todo um range gigante para o cluster Kubernetes com o objetivo de tornar os IPs dos Pods acessíveis, pois Pods raramente são acessados diretamente. Se você precisa acessar um sistema no Kubernetes, provavelmente vai fazer uso de Services, Nodeports ou Ingress Controllers para serviços HTTP.\nInclusive, isso sequer é um requisito para que Pods no Kubernetes acessem serviços hospedados fora do cluster: o normal para um IPPool Calico é estar configurado com a diretiva natOutgoing=true para que toda comunicação direcionada para fora saia com o endereço IP do nó em que o Pod está hospedado.\nPortanto, minha recomendação explícita é: não, não aloque IPs válidos da sua rede corporativa para os Pods do cluster Kubernetes.\nExiste uma única situação em que isso é necessário: se for necessário para Compliance e Auditoria.\nImagine o seguinte cenário de mais puro horror: seu cluster executa três sistemas, e cada um desses sistemas acessa um banco de dados Oracle próprio que é hospedado na parte \u0026lsquo;tradicional\u0026rsquo; da sua empresa. Houve um acesso indevido, que foi rastreado para o cluster Kubernetes. Como saber que Pod realizou o acesso, já que os três sistemas rodam pods nos mesmos nós (e tudo que você tem é o endereço do nó Kubernetes)?\nA única maneira de viabilizar isso é com endereçamento válido na sua Intranet e com sistemas de auditoria que registrem que Pod tinha que IP durante que tempo.\nAmbas as coisas (tanto a parte da integração da rede do Kubernetes com a Intranet quanto a auditoria de Pods) são verdadeiros horrores para serem viabilizadas. Eu posso falar isso porque, infelizmente, precisei fazer as duas coisas.\nConfigurar o Calico para integrar com roteadores e, assim, divulgar rotas, por si só não é um problema, (embora tenha sido necessário um PR para viabilizar o que precisamos). Normalmente o problema é a integração com as equipes responsáveis por roteadores e firewalls que trabalham no modelo \u0026ldquo;tradicional\u0026rdquo;.\nJá a auditoria de que IP estava com que Pod em que horário não existe uma solução adequada no universo do software livre sem subscrição.\n3. Quantos Pods você pretende executar no seu cluster? Um outro ponto positivo para isolar (ou pelo menos usar natOutgoing=True) é que você pode instanciar dezenas de clusters entregando os mesmos endereços IP das mesmas redes sem maiores problemas. Se você usar 1000 clusters e puder usar, em todos eles, a rede 172.16.0.0/16 , você não precisa se preocupar com esgotamento de endereços IP.\nSe você pretende criar diversos clusters Kubernetes que conversem entre si em nível de comunicação Pod a Pod, isso pode vir a se tornar um problema para a sua alocação interna de endereços IP.\nAgora, se você caiu no pior cenário possível que é precisar alocar endereços IP válidos na Intranet para seus Pods, vou começar com as más notícias.\nWeave, Flannel e outros plugins CNI normalmente operam em Layer 2 usando técnicas de encapsulamento como VXLAN ou Geneve. O Calico, por padrão, opera em modo Layer 3 usando BGP. Cada nó de workload se torna um \u0026lsquo;roteador BGP\u0026rsquo; executando um Bird que é controlado pelo componente Calico node. Isso inclusive é listado como \u0026lsquo;vantagem\u0026rsquo;, pois fica fácil entender e depurar problemas de comunicação, já que as rotas estão acessíveis.\nMas isso faz com que o Calico seja extremamente perdulário com as alocações de IP. O padrão dos IPPools Calico é segmentar a rede que você configurou usando a diretiva blockSize=26 (para ipv4, para IPv6 é 122). Isto é, ele irá pegar a rede, fatiar em pedaços /26, e sair distribuindo entre hosts.\nSuponha que, para seu primeiro cluster, os controladores dos IPs válidos de rede tenham fornecido uma rede 192.168.69.0/24 para que você use como endereçamento de Pods. Se você não souber deste parâmetro, o Calico irá segmentar a rede em quatro redes /26 e distribuir para quatro nós.\nE depois? Bem, o Calico começará a criar rotas /32 para os endereços dos Pods e o número de rotas (que deveria ser apenas quatro) explodirá tendendo a chegar em uma rota para cada Pod. Na documentação oficial há recomendação expressa para que você dimensione o blockSize para que cada nó receba ao menos uma subrede própria. Essa explosão na tabela de rotas pode gerar algo entre completo descontentamento por parte dos administradores dos roteadores que estão integrando com vocÊ até o não funcionamento e travamento de ativos por atingir algum limite do equipamento. Em uma rede /24, isso possivelmente não será um problema, mas eu consegui chegar nesse problema usando uma rede /19. 8000 rotas talvez seja o suficiente para travar alguns equipamentos.\nUma outra questão é a possível necessidade de limitar endereçamento de Pods para namespaces. Isso pode ser necessário, por exemplo, se o seu administrador Postgres estiver reticente quanto a liberar, no pg_hba, toda a rede 172.16.0.0/16. Eu honestamnete ignoraria tais administradores, mas, se isso não for opção, é possível dedicar IPpools a namespaces ou mesmo Pods. Isso é descrito na documentação do Calico. Se por acaso você precisar dessa funcionalidade, será necessário computar ainda mais volume para a fração da rede a ser alocada para o Kubernetes.\n A necessidade de usar IPs válidos da sua Intranet para endereçamento de Pods causa uma explosão de complexidade para a qual a maioria das pessoas não está preprada ao começar a usar Kubernetes sem algum apoio técnico mais experiente. Por isso, se você quiser contratar uma consultoria avançada para resolver os seus problemas e quiser contratar um especialista você\u0026hellip; Não pode me contratar, pois sou funcionário público federal e trabalho em uma empresa de TI do governo. Foi mal!\nMas bater papo eu posso à vontade! Se ainda tem alguma dúvida, especialmente se for funcionário público de alguma empresa que não qualquer tipo de possibilidade de contratar verba de consultoria, manda uma mensagem no Linkedin que a gente troca uma idea! Estou precisando de mais sugestão de pautas para abordar aqui no devsres.com!\nDepois de bater papo comigo e ainda precisar de consultores especializados, é possível que eu te indique ir conversar com os caras da Getup Cloud, mas provavelmente isso só vai acontecer se eles me chamarem para fazer mais uma edição do Kubicast.\n",
    "ref": "/marcelo/blog/kubernetes-on-prem-2/"
  },{
    "title": "GOVC",
    "date": "",
    "description": "Como explorar o VMWare pela linha de comando",
    "body": "O projeto govmomi oferece uma biblioteca em Go Para interação com as APIs VMWare Vsphere (ESXi ou vCenter). O programa govc é o binário que te permite executar coisas na linha de comando.\nEu nunca implantei VMWare do zero; conheço um emaranhado de conceitos usando a técnica que todo ultrageneralista usa: sair aprendendo uma série de coisas desconexas sobre o objeto de estudo que são o suficiente para desempenhar determinada tarefa, torcendo para que, algum dia, eu possa voltar àquele assunto e completar o que falta.\nIronicamente, tem funcionado.\nO objetivo de hoje? Descobrir tudo que existe no ambiente e onde estão minhas coisas já criadas.\nPor que isso? Para implementar a automação com Terraform. Eu preciso de um monte de informações, e me recuso a usar interfaces gráficas coloridas. Isso é contra os princípios de todo bom #Ops.\nAbrindo a caixa de ferramentas A minha caixa de ferramentas, na verdade, vem com um único martelo. Portanto, daqui para frente, tudo é prego.\nExistem diversas opções viáveis para administrar VMWare a partir da linha de comando. Não é o que eu estou procurando. Eu quero algo que seja capaz de me trazer as informações necessárias sobre o meu ambiente para que eu possa viabilizar o uso do Terraform, já que as estruturas de dados (objetos \u0026lsquo;data') que ele pode importar precisam que eu especifique nomes e IDs.\nPara usar o govc, é necessário configurar algumas variáveis de ambiente e um conjunto de credenciais:\n# # Deixe de ser preguiçoso e importe a CA auto assinada na sua máquina em vez de usar GOVC_INSECURE # export GOVC_INSECURE=true # export GOVC_URL=https://endereco-ip-do-seu-vcenter:443 # # Credenciais. Não preciso dizer que você não deve fazer isso em uma máquina com root compartilhado... # export GOVC_USERNAME=$USER # export GOVC_PASSWORD='Mnh$nh@2020@69' Peça para os administradores VMWare criarem um usuário com \u0026lsquo;readonly\u0026rsquo; no cluster. Não precisamos de mais nada além disso para fazer a exploração.\nExistem maneiras mais inteligentes de fazer a autenticação, como, por exemplo, com um token. Mas eu ainda estou esperando isso aqui acontecer:\n# https://github.com/vmware/govmomi/issues/1824 6 Feb 2020 Some more info on the auth methods here: kubernetes/kubernetes#63209 We should put some of that in the govmomi docs. Apenas para referência, é possível também usar algumas das variáveis abaixo:\n GOVC_TLS_CA_CERTS GOVC_TLS_KNOWN_HOSTS GOVC_TLS_HANDSHAKE_TIMEOUT GOVC_DATACENTER GOVC_DATASTORE GOVC_NETWORK GOVC_RESOURCE_POOL GOVC_HOST  GOVC_DATACENTER, por exemplo, me economizaria todas as vezes em que eu precisei passar o atributo -dc nos comandos abaixo.\nMas vamos ao que importa.\ngovc find Como descobrir TUDO que eu preciso sobre minha infraestrutura contando apenas com minhas credenciais e sabendo que, \u0026ldquo;lá dentro\u0026rdquo;, tem máquinas minhas?\nBasicamente usando o comando find do govc.\nEste comando conta com um parâmetro -type que será a chave para as descobertas.\n# govc find ... The '-type' flag value can be a managed entity type or one of the following aliases: a VirtualApp c ClusterComputeResource d Datacenter f Folder g DistributedVirtualPortgroup h HostSystem m VirtualMachine n Network o OpaqueNetwork p ResourcePool r ComputeResource s Datastore w DistributedVirtualSwitch ... Então vamos lá.\nFase 1: o mínimo necessário Quantos datacenters estão disponíveis? # govc find . -type Datacenter /DATACENTER-PE Um. Ótimo. Menos tapetes para procurar sujeira embaixo.\nQuer informações sobre o que tem pela frente?\n# govc datacenter.info -dc DATACENTER-PE Name: DATACENTER-PE Path: /DATACENTER-PE Hosts: 9 Clusters: 3 Virtual Machines: 90 Networks: 6 Datastores: 9 Quantos clusters de processamento? Direto e reto:\n# govc find -dc DATACENTER-PE -type ClusterComputeResource /DATACENTER-PE/host/cc_pool01 /DATACENTER-PE/host/cc_pool02 /DATACENTER-PE/host/cc_pool03 Quantas redes diferentes? Mais um direto:\n# govc find -dc DATACENTER-PE -type Network ./network/kubernetes_cluster_producao ./network/kubernetes_cluster_homologacao ./network/gerencia ./network/internet ./network/backup ./network/kubernetes_cluster_lab Quantos datastore clusters? Nós não usamos os Datastores diretamente; fazemos uso do conceito de Datastore Clusters.\nAqui confesso que vou ficar devendo um find melhor: o melhor que consegui foi listar todos os Datastores; o nome do DatastoreCluster é o nome da pasta em que o Datastore está, como visto abaixo:\n# govc find -dc DATACENTER-PE -type Datastore ./datastore/datastorecluster01/dsc1_1 ./datastore/datastorecluster01/dsc1_2 ./datastore/datastorecluster01/dsc1_3 ./datastore/datastorecluster02/dsc2_1 ./datastore/datastorecluster02/dsc2_2 ./datastore/datastorecluster02/dsc2_3 ./datastore/datastorecluster03/dsc3_1 ./datastore/datastorecluster03/dsc3_2 ./datastore/datastorecluster03/dsc3_3 # # mas eu só quero os datastoreclusters! # govc find -dc DATACENTER-PE -type Datastore | rev | cut -f2 -d'/' | rev | sort | uniq -c 3 datastorecluster01 3 datastorecluster02 3 datastorecluster03 # # uma alternativa é usar o comando govc ls # govc ls -dc DATACENTER-PE' /DATACENTER-PE/vm /DATACENTER-PE/network /DATACENTER-PE/host /DATACENTER-PE/datastore # govc ls /DATACENTER-HARDCORE/datastore /DATACENTER-PE/datastore/datastorecluster01 /DATACENTER-PE/datastore/datastorecluster02 /DATACENTER-PE/datastore/datastorecluster03 Não há uma maneira trivial de usar o find para esta finalidade, pois o comando \u0026lsquo;govc datacenter.info\u0026rsquo; mostra apenas o número de Datastores, enquanto para os clusters de computação, ele mostra adequadamente que tenho 3 clusters e 9 máquinas.\nMas não tome minha palavra como final. Eu estou muito longe de ser um especialista em VMWare.\nNotas adicionais: mostrando os exemplos acima enquanto explicava a outras pessoas, percebi um problema: do jeito que está listado acima, fica parecendo que, necessariamente, o que vier entre o nome /datastore/ e os Datastores em si (dsc3_1,dsc3_2,dsc3_3, etc.) é necessáriamente o nome do DataStoreCluster.\nIsso não é verdade! Não é que o exemplo acima está errado (ele é inspirado em uma estrutura real), mas ele não representa 100% dos casos. O VMware é um emaranhado de conceitos que se misturam tanto na representação gráfica quanto na representação via linha de comando. Vou representar uma outra estrutura que representa um cluster bem mais complexo:\n# # usando o govc ls: # govc ls /DATACENTER-HARDCORE/datastore /DATACENTER-HARDCORE/datastore/nomealeatorio1 /DATACENTER-HARDCORE/datastore/nomealeatorio2 /DATACENTER-HARDCORE/datastore/discosSSD /DATACENTER-HARDCORE/datastore/discosHDD E aí, temos quatro DatastoreClusters, certo?\nErrado!\nVocê só pode concluir isso se conhecer a infraestrutura previamente. Dei exemplos simples para facilitar o entendimento do programa, mas não dá para simplificar demais o que é absurdamente complexo.\nA ideia é complementar este \u0026lsquo;artigo\u0026rsquo; com uma versão mais avançada, mas acredito que seja melhor já adiantar isso aqui logo.\n# # usando govc ls com -i e -l para entender o que é o que: # govc ls -i -l /DATACENTER-HARDCORE/datastore StoragePod:group-p34927 /DATACENTER-HARDCORE/datastore/nomealeatorio1 Folder:group-s111 /DATACENTER-HARDCORE/datastore/nomealeatorio2 Folder:group-s113 /DATACENTER-HARDCORE/datastore/discosSSD Folder:group-s49 /DATACENTER-HARDCORE/datastore/discosHDD Resultado: Temos três Folders simples que servem de container lógico para agrupar coisas, e um StoragePod, que é o \u0026ldquo;codinome\u0026rdquo; para Datastore!\nEntão, como é que faz para efetivamente descobrirmos os malditos DatastoreClusters?\nSe você usa Folders, primeira coisa a saber é que os StoragePods também são FolderS. Sabendo disso, fica \u0026ldquo;fácil\u0026rdquo;, mesmo em um cluster complexo:\n# govc find -type Folder -dc DATACENTER-HARDCORE -i -l datastore Folder:group-s5 datastore StoragePod:group-p349 datastore/nomealeatorio1 Folder:group-s111 datastore/nomealeatorio2 Folder:group-s113 datastore/discosSSD Folder:group-s49 datastore/discosHDD StoragePod:group-p85069 datastore/discosHDD/dsc_hdd_01 StoragePod:group-p85068 datastore/discosHDD/dsc_hdd_02 StoragePod:group-p85067 datastore/discosHDD/dsc_hdd_03 StoragePod:group-p11286 datastore/discosSSD/dsc_ssd_01 StoragePod:group-p11274 datastore/discosSSD/dsc_ssd_02 StoragePod:group-p11234 datastore/discosSSD/dsc_ssd_03 Espero que tenha esclarecido aqui qualquer dúvida que o exemplo simplório mais em cima possa ter deixado!\nTenho tudo que preciso? O meu principal uso de Terraform, no momento, é o de criar \u0026lsquo;cascas\u0026rsquo; vazias de máquinas virtuais que irão ser instaladas usando PXE.\nSe você achou a ideia idiota, eu tenho minhas razões! Mas tolere essa particularidade e acompanhe abaixo o que é necessário para criar um Terraform resource do tipo vsphere_virtual_machine dessa forma:\nNota: Não entendeu nada dos Terraform listados abaixo? Entre em contato por qualquer canal - Twitter, Linkedin, Facebook, Instagram\u0026hellip; E me deixe saber que você quer saber mais sobre Terraform!\nRecuperação do objeto datacenter Preciso:\n do nome do datacenter;  data \u0026quot;vsphere_datacenter\u0026quot; \u0026quot;cluster_datacenter\u0026quot; { name = lookup(local.cluster_vsphere, \u0026quot;vsphere_datacenter\u0026quot;) } Recuperação do objeto datacenter Preciso:\n do nome do datastore cluster; do id do datacenter (recuperado com a estrutura data.vsphere_datacenter);  data \u0026quot;vsphere_datastore_cluster\u0026quot; \u0026quot;cluster_datastore\u0026quot; { name = lookup(local.cluster_vsphere, \u0026quot;vsphere_datastore_cluster\u0026quot;) datacenter_id = data.vsphere_datacenter.cluster_datacenter.id } Recuperação do cluster de computação Preciso:\n do nome do cluster de computação;   do id do datacenter (data.vsphere_datacenter.nome.id);    data \u0026quot;vsphere_compute_cluster\u0026quot; \u0026quot;compute_cluster\u0026quot; { name = lookup(local.cluster_vsphere, \u0026quot;vsphere_compute_cluster\u0026quot;) datacenter_id = data.vsphere_datacenter.cluster_datacenter.id } Recuperação das redes Preciso:\n do nome da rede; do id do datacenter.  data \u0026quot;vsphere_network\u0026quot; \u0026quot;networks\u0026quot; { for_each = local.cluster_network_profiles name = each.value datacenter_id = data.vsphere_datacenter.cluster_datacenter.id } Resource VM Aqui está o resource que uso para criar as VMs.\nEu posso trocar metade dos lookups por referências diretas. Mas, enquanto fazia, por incrível que pareça, ficava mais fácil para poder acertar a profundidade da estrutura de dados que eu optei usar como \u0026lsquo;input\u0026rsquo; no terraform.tfvars.\n Cicatrizes de batalha: não faça coisas complexas no Terraform, ainda que você consiga apenas porque você pode; você não trabalha sozinho, e alguém vai precisar entender algum dia.\n De qualquer forma, de novo, esqueça o objeto Terraform ilegível por causa da minha tendência natural de fazer coisas absurdas. Observe que criei as máquinas com os três recursos de data listados acima.\nresource \u0026quot;vsphere_virtual_machine\u0026quot; \u0026quot;vm\u0026quot; { for_each = local.vms name = each.value.hostname # Pode pular tudo até o (1). cpu_hot_add_enabled = true cpu_hot_remove_enabled = true memory_hot_add_enabled = true guest_id = \u0026quot;coreos64Guest\u0026quot; num_cpus = lookup(lookup(local.cluster_profiles,each.value.profile), \u0026quot;num_cpus\u0026quot;) memory = lookup(lookup(local.cluster_profiles,each.value.profile), \u0026quot;memory\u0026quot;) disk { label = \u0026quot;${each.value.hostname}-disk\u0026quot; size = lookup(lookup(local.cluster_profiles,each.value.profile), \u0026quot;disk_size\u0026quot;) unit_number = 0 eagerly_scrub = false thin_provisioned = true } # Truque sujos necessários para falar para o Terraform não esperar as máquinas. wait_for_guest_net_timeout = 0 # (1): cluster de computação resource_pool_id = data.vsphere_compute_cluster.compute_cluster.resource_pool_id # (2): datastore cluster datastore_cluster_id = data.vsphere_datastore_cluster.cluster_datastore.id folder = vsphere_folder.folder.path # Nós gostamos de máquinas virtuais com muitas interfaces, e cada uma com uma quantidade variável delas. dynamic \u0026quot;network_interface\u0026quot; { for_each = each.value.network content { # (3): redes. network_id = lookup(lookup(data.vsphere_network.networks, network_interface.key), \u0026quot;id\u0026quot;) } } # O backup adiciona uma tag; precisamos ignorá-la ou o Terraform irá tentar apagá-la. lifecycle { ignore_changes= [ annotation, # \u0026quot;vapp.0.properties\u0026quot;, ] } } Resumindo Para a minha necessidade inicial (criação de cascas para instalação via PXE), a resposta é sim, os comandos govc listados acima são suficientes para que eu recupere todas as informações por meio de objetos data do Terraform.\nPor mais \u0026lsquo;tosco\u0026rsquo; que possa parecer, sim, o Terraform só precisa dos nomes dos objetos, e não de ids obscuros e esquisitos.\n Cicatrizes de batalha: Se você prefere deixar para lá esse papo de linha de comando e quer usar a interface gráfica para ver todos os nomes e transcrever na mão, se prepare para surpresas desagradáveis se os seus administradores de VMWare forem tão trolls quanto os meus.\n O uso de nomes demanda um cuidado especial por causa da tolerância do VMware com caracteres especiais. O VMWare deixa você criar a rede da seguinte maneira na interface gráfica:\n rede_cluster_kubernetes_1_192.168.69.0/24  Legal, né? Mas olha como é o nome dela de verdade:\n# govc find -dc DATACENTER-PE -type Network ... ./network/kubernetes_cluster_producao_192.168.69.0%2f24 ... O nome que precisa ir no Terraform é o listado pelo Govc.\nIsso é tudo? Infelizmente, não.\nMeu problema atual é que preciso \u0026lsquo;particionar\u0026rsquo; um host VMWare em um determinado número de máquinas virtuais, usando o disco local daquela máquina como datastore.\nIsso quer dizer que precisarei fazer uma parte 2 tentando levantar todas essas informações adicionais!\nAlguém tem interesse?\n(Na verdade a demanda era para criar máquinas virtuais e entregar o disco como por meio de Raw Device Mapping, mas aparentemente isso não é possível com o Terraform e talvez nunca seja,)\n",
    "ref": "/marcelo/blog/exploring-vmware-with-govc/"
  },{
    "title": "Ceph CSI no Kubernetes",
    "date": "",
    "description": "Usando CSI para montar volumes RBD e sistemas de arquivo cephfs",
    "body": "Aqui vai o relato fresquinho da experiência de um dos meus trabalhos mais recentes: o deploy de plugins CSI para o provisionamento automático de filesystems CephFS e volumes RBD.\nUma nota infeliz: o cluster Ceph foi instalado e é mantido por outras pessoas, e eu não tenho qualquer tipo de acesso. Então o relato vai ser incompleto, faltando os procedimentos do lado \u0026ldquo;de lá\u0026rdquo;. Se você está esperando aqui ideias para apoiar a instalação e configuração de clusters Ceph ou uso de soluções como o Rook, este não é o texto para você. O foco aqui está no deploy do CSI.\nVou ver se consigo um esboço de documentação combinada com eles e atualizar o texto no futuro!\nArmazenamento no Kubernetes: para quê? O mundo Kubernetes adora coisas stateless e microsserviços; tudo seria muito mais fácil para todos se não precisássemos nos preocupar com ter que guardar dados. Olha só que ideal: o Pod sobe, um sistema de arquivos é instanciado do zero com a aplicação, ele morre, tudo é descartado e todo mundo fica feliz.\nMas nem tudo (na verdade, praticamente nada) é da forma que gostaríamos. Precisamos resolver o problema das coisas stateful em um mundo pensado stateless.\nFelizmente, temos aqui ao lado um cluster Ceph cheio de Terabytes livres pronto para resolver o problema de armazenamento. Quais são as opçõpes disponíveis? Bem, temos:\n Objetos (S3); RBD; CephFS;  Armazenamento de objetos (i.e., \u0026ldquo;S3\u0026rdquo;) A primeira sugestão para este problema é: usa armazenamento de objetos estilo S3, ora!\nÉ verdade, podemos usar esse recurso. Inclusive podemos, com algum grau de trabalho, usar o Ceph para criar uma interface S3-like. E funciona bem! A melhor parte: nenhuma modificação é feita nos sistemas de arquivo do container, sendo a opção ideal que demanda o mínimo do cluster Kubernetes em si. Todas as operações serão conduzidas pela aplicação diretamente contra a API pública do Ceph responsável pelo armazenamento de objetos.\nMas obviamente: se o trabalho não está do lado das operações, está do lado dos desenvolvedores. A aplicação precisa ser codificada para usar objetos.\nMas como você pode imaginar, armazenamento de objetos é um conceito novíssimo na computação, tipo, tem apenas umas poucas décadas. S3 surgiu apenas em 2006, tipo, praticamente ontem, a tecnologia sequer chegou à maioridade civil. Ainda não deu tempo de adaptar todo tipo de aplicação para este tipo de armazenamento. Talvez com a computação quântica chegar a gente consiga migrar tudo para S3 e ao mesmo tempo se ver livre do Cobol (/sarcasm)\nEnfim, precisamos de alternativas ao armazenamento de objetos.\n (Cicatrizes de batalha: Armazenamento de objetos no Ceph funciona, mas não sem alguma dor de cabeça: algumas coisas precisam de adaptação; por exemplo, até outro dia, o Spark Operator tinha dificuldades em ler objetos de S3 sem gerar um trabalhão gigantesco para customizar a imagem com as bibliotecas do Hadoop mais novo, mas isso não é tão relevante assim para o momento, apenas cuidado com os detalhes e a versão das libs que você vai usar!).\n Armazenamento de bloco (block storage) Precisamos de uma estratégia para instanciar dispositivos de bloco para servir como discos dos nossos Pods. O caso de uso mais simples é criar um disco dedicado para servir de volume de dados dedicado para um único Pod (embora você possa ser mais arrojado aqui).\nPara isso, temos o Ceph Block Device, conhecido como RBD, que vai atuar em uma correspondência direta com serviços de nuvem como AWS EBS ou os Azure VHD page blobs (ainda não superei esse nome).\nSistema de arquivos de rede Além dos blocos, uma outra solicitação comum por sistemas de arquivo de rede distribuídos ao estilo NFS (que inclusive é uma opção no Kubernetes a um custo de complexidade relativamente baixo).\nPodemos usar o Ceph Filesystem ou CephFS para esta finalidade.\nQual o jeito \u0026ldquo;certo\u0026rdquo; de usar Ceph no Kubernetes? Das três opções que temos, o armazenamento de objetos depende única e exclusivamente da aplicação, e por isso é uma favorita de 10 em cada 10 administradores de infraestrutura.\nAs outras duas demandam algum conhecimento e configuração tanto do lado Kubernetes quanto do lado Ceph.\nLevando em consideração que começamos nossa história com Kubernetes na versão 1.4, a integração com Cephfs e RBD originalmente implantada faz uso do código nativo do Kubernetes (também chamado de \u0026ldquo;in-tree\u0026rdquo;) para a montagem dos volumes, e segue assim até hoje. Entretanto, para um novo projeto, decidi fazer uso da Container Storage Interface (CSI) para fazer as novas integrações.\nA CSI graduou para stable com o Kubernetes 1.13, e a ideia é a mesma dos demais CxI: desacoplar o código do Kubernetes dos diferentes providers de coisas. Não há razão para não usá-la exceto:\n preguiça; incompatibilidade; instabilidade;  Obviamente somos profissionais da #ResistênciaOPS, então preguiça não é problema; vamos avaliar o resto!\nCeph CSI Algumas informações relevantes estão disponíveis no site do projeto Ceph CSI. A primeira delas é que saiu uma nova release no dia em que eu comecei a escrever essa documentação, então minha implantação já está defasada (nota: já atualizei)!\nA segunda é a \u0026lsquo;support matrix\u0026rsquo; que apresenta as funcionalidades e o estado delas, bem como as versões compatíveis.\nA primeira informação relevante a ser extraída é que você só pode usar o Ceph CSI se seu cluster Ceph for ao menos 14.0.0 (Nautilus); isso foi uma barreira para minhas iniciativas até pouco tempo atras.\nReproduzindo a tabela encontrada em 09/12/2020:\n   Plugin Features Feature Status CSI Driver Version CSI Spec Version Ceph Cluster Version Kubernetes Version     RBD Dynamically provision, de-provision Block mode RWO volume GA \u0026gt;= v1.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.14.0    Dynamically provision, de-provision Block mode RWX volume GA \u0026gt;= v1.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.14.0    Dynamically provision, de-provision File mode RWO volume GA \u0026gt;= v1.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.14.0    Provision File Mode ROX volume from snapshot Alpha \u0026gt;= v3.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=v14.2.2) \u0026gt;= v1.17.0    Provision File Mode ROX volume from another volume Alpha \u0026gt;= v3.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=v14.2.2) \u0026gt;= v1.16.0    Provision Block Mode ROX volume from snapshot Alpha \u0026gt;= v3.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=v14.2.2) \u0026gt;= v1.17.0    Provision Block Mode ROX volume from another volume Alpha \u0026gt;= v3.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=v14.2.2) \u0026gt;= v1.16.0    Creating and deleting snapshot Alpha \u0026gt;= v1.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.17.0    Provision volume from snapshot Alpha \u0026gt;= v1.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.17.0    Provision volume from another volume Alpha \u0026gt;= v1.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.16.0    Expand volume Beta \u0026gt;= v2.0.0 \u0026gt;= v1.1.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.15.0    Metrics Support Beta \u0026gt;= v1.2.0 \u0026gt;= v1.1.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.15.0    Topology Aware Provisioning Support Alpha \u0026gt;= v2.1.0 \u0026gt;= v1.1.0 Nautilus (\u0026gt;=14.0.0) \u0026gt;= v1.14.0   CephFS Dynamically provision, de-provision File mode RWO volume Beta \u0026gt;= v1.1.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=14.2.2) \u0026gt;= v1.14.0    Dynamically provision, de-provision File mode RWX volume Beta \u0026gt;= v1.1.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=v14.2.2) \u0026gt;= v1.14.0    Dynamically provision, de-provision File mode ROX volume Alpha \u0026gt;= v3.0.0 \u0026gt;= v1.0.0 Nautilus (\u0026gt;=v14.2.2) \u0026gt;= v1.14.0    Creating and deleting snapshot Alpha \u0026gt;= v3.1.0 \u0026gt;= v1.0.0 Octopus (\u0026gt;=v15.2.3) \u0026gt;= v1.17.0    Provision volume from snapshot Alpha \u0026gt;= v3.1.0 \u0026gt;= v1.0.0 Octopus (\u0026gt;=v15.2.3) \u0026gt;= v1.17.0    Provision volume from another volume Alpha \u0026gt;= v3.1.0 \u0026gt;= v1.0.0 Octopus (\u0026gt;=v15.2.3) \u0026gt;= v1.16.0    Expand volume Beta \u0026gt;= v2.0.0 \u0026gt;= v1.1.0 Nautilus (\u0026gt;=v14.2.2) \u0026gt;= v1.15.0    Metrics Beta \u0026gt;= v1.2.0 \u0026gt;= v1.1.0 Nautilus (\u0026gt;=v14.2.2) \u0026gt;= v1.15.0    Tirando o fato de que o provisionamento dinâmico de CephFS se encontra em estágio Beta, não há nenhum grande empecilho à implementação.\nHelm charts Nenhum procedimento de instalação é listado de imediato, mas tanto o plugin CSI do rbd quanto do cephfs contam com Helm charts disponíveis para instalação!\n ceph-csi-rbd ceph-csi-cephfs  Qual a primeira coisa que devemos fazer com Helm charts alheios? Baixar e fuçar:\n# # Rede isolada :( # source proxy-me-up.sh # helm repo add ceph-csi https://ceph.github.io/csi-charts # helm repo update ... ...Successfully got an update from the \u0026quot;ceph-csi\u0026quot; chart repository ... # helm pull ceph-csi/ceph-csi-rbd # helm pull ceph-csi/ceph-csi-cephfs # # Nunca esquecer de desconfigurar as variáveis de proxy para não entrar em pânico quando der kubectl e receber um timeout. # unset ${!HTTP*} ${!http*} Mas voltando ao chart: uma coisa que já me deixa triste éo fato de que não há uma maneira de você especificar um Docker Registry que faça \u0026lsquo;override\u0026rsquo; global para todas as imagens - aparentemente estou acostumado demais com os fantásticos Helm charts da Bitnami.\nE aqui vai uma lembrança nada a ver com o assunto Ceph CSI: em 20/11/2020, o Docker Hub ativou rate limit para usuários anônimos; portanto deixar o seu cluster baixar imagens diretamente de registries públicos, mesmo para teste, não é mais uma opção.\nA primeira coisa a fazer é levantar quais as imagens usadas pelos Charts e quais os parâmetros que vou precisar customizar para usar meu Registry privado.\n# # Eu prefiro isso a dar helm show values # for i in ceph*.tgz ; do tar xvf $i ; done # fgrep -A1 repository ceph-csi-cephfs/values.yaml ceph-csi-rbd/values.yaml ceph-csi-cephfs/values.yaml: repository: k8s.gcr.io/sig-storage/csi-node-driver-registrar ceph-csi-cephfs/values.yaml- tag: v2.0.1 -- ceph-csi-cephfs/values.yaml: repository: quay.io/cephcsi/cephcsi ceph-csi-cephfs/values.yaml- tag: v3.2.0 -- ceph-csi-cephfs/values.yaml: repository: k8s.gcr.io/sig-storage/csi-provisioner ceph-csi-cephfs/values.yaml- tag: v2.0.4 -- ceph-csi-cephfs/values.yaml: repository: k8s.gcr.io/sig-storage/csi-attacher ceph-csi-cephfs/values.yaml- tag: v3.0.2 -- ceph-csi-cephfs/values.yaml: repository: k8s.gcr.io/sig-storage/csi-resizer ceph-csi-cephfs/values.yaml- tag: v1.0.1 -- ceph-csi-cephfs/values.yaml: repository: k8s.gcr.io/sig-storage/csi-snapshotter ceph-csi-cephfs/values.yaml- tag: v3.0.2 -- ceph-csi-rbd/values.yaml: repository: k8s.gcr.io/sig-storage/csi-node-driver-registrar ceph-csi-rbd/values.yaml- tag: v2.0.1 -- ceph-csi-rbd/values.yaml: repository: quay.io/cephcsi/cephcsi ceph-csi-rbd/values.yaml- tag: v3.2.0 -- ceph-csi-rbd/values.yaml: repository: k8s.gcr.io/sig-storage/csi-provisioner ceph-csi-rbd/values.yaml- tag: v2.0.4 -- ceph-csi-rbd/values.yaml: repository: k8s.gcr.io/sig-storage/csi-attacher ceph-csi-rbd/values.yaml- tag: v3.0.2 -- ceph-csi-rbd/values.yaml: repository: k8s.gcr.io/sig-storage/csi-resizer ceph-csi-rbd/values.yaml- tag: v1.0.1 -- ceph-csi-rbd/values.yaml: repository: k8s.gcr.io/sig-storage/csi-snapshotter ceph-csi-rbd/values.yaml- tag: v3.0.2 Daqui podemos observar que:\n A imagem do plugin CSI é a mesma para ambos;  Será que dá para consolidar ambos em um único pod de instalação? Isso pode ser útil para evitar ter que consumir metade do Pod Limit dos nodes com pods de \u0026lsquo;overhead\u0026rsquo;. Mas este não é meu problema no momento, vou optar pela comodidade do Helm chart pronto.\n Os plugins precisam de diversos outros componentes CSI para funcionarem.  E com um bônus: agora eles aparentemente alinharam as versões dos 5 componentes extras entre os 2 charts.\n Cicatrizes de batalha: se você, como eu, gosta de acompanhar os projetos individualmente e usar sempre a versão mais recente, é bastante provavel que você não possa fazer isso aqui. A evolução dos componentes CSI caminha por uma trilha diferente dos drivers CSI dos fornecedores. Você necessariamente precisa usar as versões \u0026lsquo;homologadas\u0026rsquo; por estes ou vai se dar mal.\n Apenas para referência, a versão na qual fiz todos os testes (3.1.2, de apenas 20 dias atrás) usavam as seguintes versões, bem mais antigas:\n# # jsonpath k get pods -o jsonpath='{range .items[*].spec.containers[*]}{.image}{\u0026quot;\\n\u0026quot;}{end}' | sort | uniq | cut -f3 -d'/' | sort | uniq cephcsi:v3.1.2 csi-attacher:v2.1.1 csi-node-driver-registrar:v1.3.0 csi-provisioner:v1.6.0 csi-resizer:v0.5.0 csi-snapshotter:v2.1.0 csi-snapshotter:v2.1.1 Onde tem a informação do que é compatível com o que?\nBem, além da tabela que copiei acima indicando as versões do CSI demandadas, você pode olhar\u0026hellip; no Helm chart. Ou nos yamls de exemplo. Não parece haver uma tabela de referência em outro lugar no site. Vou entrar no Slack do projeto e averiguar - não havendo, quem sabe contribuir?\nAnálise dos componentes do CSI Eu gosto de entender os templates dos Helm charts para implantações; se o festival de go-templates te deixar pouco confortável, possivelmente eles replicam o que está nestes exemplos:\n Deploy RBD Deploy Cephfs  A partir da análise dos manifests, dá para entender a dinâmica dos componentes. Ambos usam uma organização de componentes semelhante:\n Deployment de Provisioner; Daemonset de nodePlugin.  Eu poderia copiar as ASCII Arts do projeto CSI que são bem legais, mas acho que a Red Hat fez um desenho que ilustra bem a ideia dos componentes descritos nas próximas sessões:\nProvisioners Os provisioners são os \u0026lsquo;controller\u0026rsquo; propriamente ditos, que irão monitorar a API do Kubernetes e executar as principais ações relacionadas aos volumes do ponto de vista do cluster;\nSão executados como Deployments a título de redundância, em que apenas um de seus Pods está ativo em um dado momento. Contém diversos containers; os comuns a ambos os drivers são:\nOs provisioners são Pods com múltiplos containers, e precisam \u0026ldquo;carregar\u0026rdquo; os seguintes módulos para implementar a tradução de chamadas Kubernetes via CSI:\n csi-provisioner: o external provisioner do Kubernetes CSI; o controller responsável pela monitoração dos PersistentVolumeClaims criados e dispara as funções de criar e remover os volumes dos plugins CSI do Ceph. csi-attacher: o external-attacher fica com as chamadas de API CSI responsáveis pelo uso dos volumes por nodes; csi-resizer: o external-resizer, que intuitivamente implementa as chamadas para redimensionamento de PVCs; csi-snapshotter: o external-snapshotter, também intuitivamente responsável pela monitoração de CRDs de Snapshot e gerenciar seu ciclo de vida. Este plugin virou GA apenas no Kubernetes 1.20, mas o Ceph CSI indica o uso deste recurso como Alpha, então vale a precaução.  Além destes componentes externos ao projeto, os CSI também usam:\n liveness-prometheus: o próprio plugin cephcsi executando em modo \u0026lsquo;\u0026ndash;type=liveness\u0026rsquo;, dedicado a garantir o estado do Pod e a exportar métricas;  Por fim, uma nova versão do plugin cephcsi é executada das seguintes formas, dependendo do tipo de componente usado.\nPara o CephFS:\n csi-cephfsplugin: um único container, executado em modo \u0026lsquo;\u0026ndash;type=cephfs\u0026rsquo;, responsável pela comunicação com os containers que executam os drivers nos nós;  Para o RBD:\n csi-rbdplugin: um container em modo \u0026lsquo;\u0026ndash;type=rbd\u0026rsquo;. csi-rbdplugin-controller: um container em modo \u0026lsquo;\u0026ndash;type=controller\u0026rsquo;.  NodePlugins Os nodePlugins são executados em todas as máquinas de workload, que são aquelas que irão efetivamente mapear os volumes para pods.\nSão executados como Daemonsets para subir em todas as máquinas do cluster. Recebem diretamente a tradução das chamadas CSI realizadas ao driver \u0026lsquo;provisioner\u0026rsquo; e implementam as operações de montagem dos sistemas de arquivos e dispositivos de blocos.\nOs Pods de ambos executam três containers:\n driver-registrar: o node-driver-registrar registra o driver CSI no Kubelet daquele host; liveness-prometheus: o próprio plugin cephcsi executando em modo \u0026lsquo;\u0026ndash;type=liveness\u0026rsquo;, de maneira semelhante ao executado nos provisioners;  O terceiro container é o driver propriamente dito:\n csi-rbdplugin ou csi-cephfsplugin: o driver cephcsi, executando no modo apropriado, e responsável pelas operações de mapeamento dos dispositivos e montagem dos sistemas de arquivo cephfs.  Considerações de segurança adicionais: este Pod, pela sua natureza, demanda privilégios especiais; executa com as diretivas \u0026lsquo;hostPID=true\u0026rsquo;, \u0026lsquo;hostNetwork=true\u0026rsquo; além de mapear diversos volumes.\nObservações relevantes Entender devidamente a função de cada componente demanda leitura do documento de design dos CSI.\nImportante: como o driver CSI do Ceph e os demais componentes têm seu desenvolvimento dissociado um do outro, eles evoluem em ritmos diferentes. Consequentemente, versões mais novas de algun dos 4 componentes \u0026lsquo;CSI\u0026rsquo; do lado do Kubernetes não necessariamente são compatívels com as implementações dos plugins.\nOnde tem a informação do que é compatível com o que? Em lugar nenhum. É necessário olhar os helm charts dos releases ou os manifests de exemplo disponibilizados pelo projeto.\nInstalação Pré-requisitos Como falei lá no começo, não tenho acesso ao Ceph (apenas temporariamente mwahaha), então precisei interfacear com os administradores do cluster Ceph para que me repassassem as informações necessárias.\nPara implantar os CSI, precisamos de dois tipos de informações:\n  Parâmetros de configuração:\n Cluster_ID: esta informação é crítica e corresponde ao identificador único do Ceph para o cluster Kubernetes. Aviso de antemão que depurar configurações divergentes de cluster_id entre cluster ceph e kubernetes por incrível que pareça não é tão trivial. Endereçamento IP do cluster: tanto dos monitores quando dos demais componentes. A rede é importante para possível liberação de regra em firewall, mas para o processo de instalação mesmo, a única coisa que precisamos configurar é os endereços IP de todos os Monitors do cluster. Portas: Idealmente, na configuração, devemos evitar especificar as portas e deixar o cliente usar da maneira que der. Por que isso? Os Ceph Monitors usam duas portas por padrão: 3300 e 6789. Cada porta atende a uma versão diferente do protocolo msgr, sendo a porta 3300 a mais nova. Dependendo do tipo de uso, uma versão será usada, ou a outra. subvolumeGroup: parâmetro relevante para o CephFS. O valor default é \u0026ldquo;csi\u0026quot;; algo diferente demandará configuração.    Credenciais com as permissões corretas.\n  Os parâmetros (1.) você irá passar para o Helm no ato da instalação. Você não precisa das credenciais para a instalação dos plugins, então é possível dissociar estas duas etapas.\nAs credenciais (2.) devem ser configuradas no processo de criação das StorageClasses. Elas não são configuradas pelos Helm charts - eles instalam os plugins CSI apenas.\nComo infelizmente meu cluster ainda não conta com acesso ao Git interno (um mês não é tempo suficiente para criação de regras de firewall) mas os meus prazos precisam ser cumpridos, eu não poderei usar GitOps para fazer o deploy dos CSI.\n(Quando eu finalmente conseguir implantar o Fluxv2, eu prometo que vou descrever aqui).\nInstalação do Ceph CSI RBD É possível executar o helm install passando um arquivo de values, que é mais estético e prático. Eu prefiro chamar na linha de comando dezenas de \u0026ndash;set como a tripa gigante abaixo. De brinde, segue a maneira não descrita pela documentação oficial para aplicar \u0026ndash;set em Lists na linha de comando em Helm charts.\nPara este comando, precisamos dos seguintes pré-requisitos:\n ID do cluster: torça para que não passem errado para você; IPs dos monitores.  Estas informações irão virar um campo \u0026lsquo;config.json\u0026rsquo; em um Configmap chamado ceph-csi-config-rbd.\n# export PRIVATE_REPO=hub.intra/resistenciaOPS # helm install \\ --namespace \u0026quot;csi\u0026quot; \\ --create-namespace=true \\ ceph-csi-rbd \\ /srv/k8s-bootstrap/charts/ceph-csi-rbd \\ --set configMapName=ceph-csi-config-rbd \\ --set kmsConfigMapName=ceph-csi-rbd-encryption-kms-config \\ --set csiConfig[0].clusterID=6969197-ddf7-418f-8f07-3e6744c6af80 \\ --set csiConfig[0].monitors[0]=192.168.69.1 \\ --set nodeplugin.registrar.image.repository=$PRIVATE_REPO/csi-node-driver-registrar \\ --set nodeplugin.plugin.image.repository=$PRIVATE_REPO/cephcsi \\ --set nodeplugin.nodeSelector.'node-role\\.kubernetes\\.io/worker'='' \\ --set provisioner.provisioner.image.repository=$PRIVATE_REPO/csi-provisioner \\ --set provisioner.attacher.image.repository=$PRIVATE_REPO/csi-attacher \\ --set provisioner.resizer.image.repository=$PRIVATE_REPO/csi-resizer \\ --set provisioner.snapshotter.image.repository=$PRIVATE_REPO/csi-snapshotter \\ --set provisioner.nodeSelector.'node-role\\.kubernetes\\.io/master'='' \\ --set provisioner.tolerations[0].key=node-role.kubernetes.io/master \\ --set provisioner.tolerations[0].effect=NoSchedule \\ --set provisioner.replicaCount=2 Os únicos comentários relevantres sobre o comando acima são:\n set configMapName=ceph-csi-config-rbd: as duas versões agora usam o mesmo nome de configmap. Tentar instalar as duas com o helm irá gerar conflito. As alternativas são: modificar o nome do configmap e torná-los independente de novo, ou configurar a diretiva \u0026lsquo;externallyManagedConfigmap\u0026rsquo;, que provavelmente é a melhor. Sou preguiçoso e fui com a primeira. set nodeplugin.nodeSelector: não tenho interesse de executar o plugin em todas as máquinas do Cluster, apenas naquelas que eu considero \u0026lsquo;workers\u0026rsquo;. Desta forma, não irão subir, por exemplo, 2 Pods para isso no Master. set provisioner.nodeSelector/provisioner.tolerations: já o provisioner é o contrário: eu quero que ele rode apenas nos nós de Control Plane. set provisioner.replicaCount=2 porque só tenho dois masters.  Por que eu quero que ele rode nos nós de Control plane?\n# k exec -it -c csi-rbdplugin ceph-csi-rbd-provisioner-6dff8cf8f4-7jxft -- whoami root Eu poderia rodar um comando \u0026lsquo;helm status\u0026rsquo; para mostrar para vocês o resultado da execução, mas o Helm 3 quebrou o helm status, e a issue em aberto há um ano e meio não me deixa muito otimista quanto à resolução.\nSegue um resultado com o que ele pode vir a instalar, dependendo dos parâmetros:\n# helm get manifest ceph-csi-rbd | yq -r '\u0026quot;\\(.kind):\\(.metadata.name)\u0026quot;' ServiceAccount:ceph-csi-rbd-nodeplugin ServiceAccount:ceph-csi-rbd-provisioner ConfigMap:ceph-csi-config-rbd ConfigMap:ceph-csi-encryption-kms-config ClusterRole:ceph-csi-rbd-provisioner ClusterRole:ceph-csi-rbd-provisioner-rules ClusterRoleBinding:ceph-csi-rbd-provisioner Role:ceph-csi-rbd-provisioner RoleBinding:ceph-csi-rbd-provisioner Service:ceph-csi-rbd-nodeplugin-http-metrics Service:ceph-csi-rbd-provisioner-http-metrics DaemonSet:ceph-csi-rbd-nodeplugin Deployment:ceph-csi-rbd-provisioner Se quiser mais detalhes, pode pedir assim:\n# for i in $( helm get manifest ceph-csi-rbd | yq -r '\u0026quot;\\(.kind):\\(.metadata.name)\u0026quot;' ) ; do kubectl get ${i/:/ } -o yaml ; read ; done Instalação do Ceph CSI Cephfs Muito semelhante ao anterior:\n# export PRIVATE_REPO=hub.intra/resistenciaOPS # helm install \\ --namespace \u0026quot;csi\u0026quot; \\ --create-namespace=true \\ ceph-csi-cephfs \\ /srv/k8s-bootstrap/charts/ceph-csi-cephfs \\ --set configMapName=ceph-csi-config-cephfs \\ --set csiConfig[0].clusterID=6969d032-c77e-4c55-a560-d559c2f41058 \\ --set csiConfig[0].monitors[0]=192.168.69.1 \\ --set nodeplugin.registrar.image.repository=$PRIVATE_REPO/csi-node-driver-registrar \\ --set nodeplugin.plugin.image.repository=$PRIVATE_REPO/cephcsi \\ --set nodeplugin.nodeSelector.'node-role\\.kubernetes\\.io/worker'='' \\ --set provisioner.provisioner.image.repository=$PRIVATE_REPO/csi-provisioner \\ --set provisioner.attacher.image.repository=$PRIVATE_REPO/csi-attacher \\ --set provisioner.resizer.image.repository=$PRIVATE_REPO/csi-resizer \\ --set provisioner.snapshotter.image.repository=$PRIVATE_REPO/csi-snapshotter \\ --set provisioner.nodeSelector.'node-role\\.kubernetes\\.io/master'='' \\ --set provisioner.tolerations[0].key=node-role.kubernetes.io/master \\ --set provisioner.tolerations[0].effect=NoSchedule \\ --set provisioner.replicaCount=2 Basicamente as mesmas considerações da instalação anterior valem para cá.\nEle cria basicamente as mesmas coisas:\n# helm get manifest ceph-csi-cephfs | yq -r '\u0026quot;\\(.kind):\\(.metadata.name)\u0026quot;' ServiceAccount:ceph-csi-cephfs-nodeplugin ServiceAccount:ceph-csi-cephfs-provisioner ConfigMap:ceph-csi-config-cephfs ClusterRole:ceph-csi-cephfs-provisioner ClusterRole:ceph-csi-cephfs-provisioner-rules ClusterRoleBinding:ceph-csi-cephfs-provisioner Role:ceph-csi-cephfs-provisioner RoleBinding:ceph-csi-cephfs-provisioner Service:ceph-csi-cephfs-nodeplugin-http-metrics Service:ceph-csi-cephfs-provisioner-http-metrics DaemonSet:ceph-csi-cephfs-nodeplugin Deployment:ceph-csi-cephfs-provisioner Criação das StorageClasses Vou usar as pouco inspiradas StorageClasses abaixo:\n rbd: aqui há um campo importante que não há em outro lugar: o pool RBD. A única imageFeature suportada é \u0026lsquo;layering\u0026rsquo;.  apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd allowVolumeExpansion: true mountOptions: - discard parameters: clusterID: 6975d032-c77e-4c55-a560-d559c2f41058 csi.storage.k8s.io/controller-expand-secret-name: ceph-rbd csi.storage.k8s.io/controller-expand-secret-namespace: csi csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/node-stage-secret-name: ceph-rbd csi.storage.k8s.io/node-stage-secret-namespace: csi csi.storage.k8s.io/provisioner-secret-name: ceph-rbd csi.storage.k8s.io/provisioner-secret-namespace: csi imageFeatures: layering pool: kubernetes provisioner: rbd.csi.ceph.com reclaimPolicy: Delete volumeBindingMode: Immediate  cephfs:  apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-cephfs allowVolumeExpansion: true mountOptions: - debug parameters: clusterID: 6969d032-c77e-4c55-a560-d559c2f41058 csi.storage.k8s.io/controller-expand-secret-name: ceph-cephfs csi.storage.k8s.io/controller-expand-secret-namespace: csi csi.storage.k8s.io/node-stage-secret-name: ceph-cephfs csi.storage.k8s.io/node-stage-secret-namespace: csi csi.storage.k8s.io/provisioner-secret-name: ceph-cephfs csi.storage.k8s.io/provisioner-secret-namespace: csi fsName: cephfs mounter: kernel provisioner: cephfs.csi.ceph.com reclaimPolicy: Delete volumeBindingMode: Immediate Aqui indicamos as secrets que serão usadas para provisionamento, no caso ceph-cephfs e ceph-rbd.\nCriação da secrets Para RBD, é necessário especificar uma userID e uma userKey:\n# k create secret generic ceph-rbd \\ --type='kubernetes.io/rbd' \\ --from-literal=userID='usuario' \\ --from-literal=userKey='chave' Já o CephFS precisa de dois conjuntos de credenciais distintas: além de uma userID com sua chave, é necessário usar uma chave de admin:\n# k create secret generic ceph-cephfs \\ --type='kubernetes.io/cephfs' \\ --from-literal=userID='usuario' \\ --from-literal=userKey='chave' \\ --from-literal=adminID='usuario' \\ --from-literal=adminKey='chave' Por que isso? A diferença é descrita por estes dois crípticos comentários:\n# Required for statically provisioned volumes userID: \u0026lt;plaintext ID\u0026gt; userKey: \u0026lt;Ceph auth key corresponding to ID above\u0026gt; # Required for dynamically provisioned volumes adminID: \u0026lt;plaintext ID\u0026gt; adminKey: \u0026lt;Ceph auth key corresponding to ID above\u0026gt; A melhor explicação para a diferença está aqui, o que traz certo alívio em ver que não é nada fantástico: \u0026lsquo;statically provisioned\u0026rsquo; são volumes que você criou na mão, e não o provisioner.\nTestes Neste primeiro momento, não queremos testar funcionalidades avançadas. Os objetivos de testes são simples:\n Se o componente provisioner de cada plugin CSI está conseguindo provisionar adequadamente o PV para o PVC solicitado; Se o componente que executa em cada nó está conseguindo montar os volumes ou sistemas de arquivo.  Para cada teste, vamos subir um tipo diferente de objeto Kubernetes:\n Para testar o cephfs, vamos subir um deployment com pod anti affinity para que cada um suba em um nó diferente; Para testar o rbd, vamos subir um statefulset (com o mesmo anti affinity) que provisionará volumes por meio de diretivas volumeClaimTemplate.  Como temos 4 nós, o número de réplicas em cada um deles será 4.\nDeployment de teste de CephFS Aqui, nada de muito especial exceto a diretiva de anti-affinity usando a topologyKey que determina que um Pod sob este Deployment não subirá em um nó que já tenha um Pod deste deployment.\napiVersion: apps/v1 kind: Deployment metadata: labels: app: cephfs-testfs name: cephfs-test namespace: csi spec: replicas: 4 selector: matchLabels: app: cephfs-testfs strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: cephfs-testfs spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - cephfs-testfs topologyKey: kubernetes.io/hostname containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx volumeMounts: - mountPath: /var/lib/www name: mypvc volumes: - name: mypvc persistentVolumeClaim: claimName: pvc-cephfs Antes de executar o Deployment acima, será necessário criar o PVC:\nAqui, o PVC não será criado automaticamente, você deve fazer isso.\n# cat pvc-cephfs.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-cephfs spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi storageClassName: ceph-cephfs O que acontece ao criar o PVC?\n# k get pvc,pv NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc-cephfs Bound pvc-f30393a6-42ee-4ffd-b3e5-784f1e940adc 1Gi RWX ceph-cephfs 2d11h NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-f30393a6-42ee-4ffd-b3e5-784f1e940adc 1Gi RWX Delete Bound csi/pvc-cephfs ceph-cephfs 2d11h Agora pode-se criar o deployment, que distribuirá os Pods adequadamente entre os nós, cada um montando o mesmo PVC oferecendo leitura e escrita concorrente:\n# k get pods -l app=cephfs-test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cephfs-test-1006230814-6winp 1/1 Running 0 24h 192.168.22.164 worker1.intra \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cephfs-test-1006230814-fmgu3 1/1 Running 0 24h 192.168.19.42 worker2.intra \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cephfs-test-1006230814-6ekbw 1/1 Running 0 24h 192.168.8.146 worker3.intra \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cephfs-test-1006230814-fz9sd 1/1 Running 0 24h 192.168.20.24 worker3.intra \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Statefulset de teste para RBD Para quem não conhece, um statefulset é um Deployment diferente, que cria pods com nomes pré-definidos em uma ordem pré definida.\nA ideia é garantir uma identidade própria e uma \u0026ldquo;unicabilidade\u0026rdquo; aos Pods (tentei transcrever a definição da melhor forma possível, foi isso que consegui, desculpem!).\nA relevância de ser ou não Statefulset para este teste é menor; o que é importante mesmo é o fato de que existe um campo volumeClaimTemplate para pod que garante que o Kubernetes auto-instancie os PVCs necessários para o teste, tornando esta abordagem perfeita para o sysadmin preguiçoso!\napiVersion: apps/v1 kind: StatefulSet metadata: name: ceph-rbd-test spec: selector: matchLabels: app: ceph-rbd-test serviceName: \u0026quot;nginx\u0026quot; replicas: 4 template: metadata: labels: app: ceph-rbd-test spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - ceph-rbd-test topologyKey: kubernetes.io/hostname containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ] storageClassName: \u0026quot;ceph-rbd\u0026quot; resources: requests: storage: 1Gi O que acontece ao criar o statefulset?\n# k get pods -l app=ceph-rbd-test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ceph-rbd-test-0 1/1 Running 0 24h 192.168.22.167 worker1.intra \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ceph-rbd-test-1 1/1 Running 0 24h 192.168.19.41 worker2.intra \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ceph-rbd-test-2 1/1 Running 0 24h 192.168.8.148 worker3.intra \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ceph-rbd-test-3 1/1 Running 0 24h 192.168.20.23 worker3.intra \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Os PVCs são criados automaticamente, assim como os PVs:\n# k get pvc,pv NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/www-ceph-rbd-test-0 Bound pvc-4f1ca9bf-2c56-4ce4-8a6c-5bca290fd058 1Gi RWO ceph-rbd 24h persistentvolumeclaim/www-ceph-rbd-test-1 Bound pvc-0bd15bb4-e782-46ec-90ce-10a779c95451 1Gi RWO ceph-rbd 24h persistentvolumeclaim/www-ceph-rbd-test-2 Bound pvc-9f92d4e4-25e8-4f7c-9eab-2fc16c38b759 1Gi RWO ceph-rbd 24h persistentvolumeclaim/www-ceph-rbd-test-3 Bound pvc-26548c61-f6fd-424f-a6e2-272ff559bf41 1Gi RWO ceph-rbd 24h NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-0bd15bb4-e782-46ec-90ce-10a779c95451 1Gi RWO Delete Bound csi/www-ceph-rbd-test-1 ceph-rbd 24h persistentvolume/pvc-26548c61-f6fd-424f-a6e2-272ff559bf41 1Gi RWO Delete Bound csi/www-ceph-rbd-test-3 ceph-rbd 24h persistentvolume/pvc-4f1ca9bf-2c56-4ce4-8a6c-5bca290fd058 1Gi RWO Delete Bound csi/www-ceph-rbd-test-0 ceph-rbd 24h persistentvolume/pvc-9f92d4e4-25e8-4f7c-9eab-2fc16c38b759 1Gi RWO Delete Bound csi/www-ceph-rbd-test-2 ceph-rbd 24h persistentvolume/pvc-f30393a6-42ee-4ffd-b3e5-784f1e940adc 1Gi Com a criação dinâmica dos PVCs, nem é necessário olhar os status dos Pods, pois todos os volumes foram criados.\nObservação importante: PVCs instanciados por statefulsets não são deletados automaticamente se o statefulset for deletado. Lembrar disso é importante para evitar acúmulo de lixo no cluster Ceph por testes esquecidos.\nErros comuns Praticamente todos os erros vêm de duas coisas:\n Falha na conectividade com o cluster Ceph: que, ironicamente, não são relatadas exatamente de maneira trivial, mas por mensagens genéricas como \u0026ldquo;rpc error: code = Unknown desc = operation timeout: context deadline exceeded\u0026rdquo;; Falha na configuração dos plugins CSI: de longe a mais comum.  A experiência que tive na última semana trabalhando para viabilizar esta implantação é que depurar a integração com o Ceph é um pesadelo. Não sei apontar de quem exatamente é a culpa, se das mensagens pouquíssimo claras do plugin ou se da nossa deficiência interna em obter logs relevantes dos componentes do Ceph.\nBatemos em todo tipo de dificuldade:\n Erro de credencial; Cluster_id incorreto; Pool incorreto; Erro de permissão; Erro de protocolo (especificando a porta 3300, que era incompatível); Usuários com mesmo conjunto de permissão recebendo respostas diferentes para suas requisições (um falhava, o outro funcionava); Possível bug obscuro em que uma das máquinas simplesmente não montava o volume;  É importante sempre \u0026lsquo;limpar\u0026rsquo; todo o ambiente entre cada testes (possivelmente a sujeira foi a causa do bug obscuro relatado acima).\nMas o processo de depuração foi basicamente auto tune, por meio de \u0026lsquo;guessing games\u0026rsquo;, dos parâmetros configurados.\nUma coisa importante é: não confie no provisionamento automático dos PVs para garantir que a implementação está operacional. O problema pode vir na forma da montagem dos volumes nas máquinas - lembrar que o CSI tem dois componentes, o \u0026lsquo;Provisioner\u0026rsquo; que traduz as requisições CSI e o \u0026lsquo;NodePlugin\u0026rsquo; que executa em cada nó.\nNão foi a melhor das experiências, e pretendo voltar (quando tiver acesso ao Ceph) para averiguar uma maneira mais inteligente para fazer essa depuração.\n",
    "ref": "/marcelo/blog/ceph-csi-on-kubernetes/"
  },{
    "title": "Kubernetes on-premises - parte 1",
    "date": "",
    "description": "Relato histórico das decisões tomadas na implantação de um cluster Kubernetes on-premises.",
    "body": "Recentemente, participei do Kubicast, em que pude desabafar um pouco com o João Brito da Getup sobre como é criar e um cluster Kubernetes em um ambiente tecnologicamente inóspito e oposto ao universo super estável, escalável e redundante dos cloud providers.\nRecebi algumas mensagens de alguns poucos valentes que precisam trilhar esse caminho. Mas, antes, vou ressaltar uma coisa importante: neste primeiro momento, eu vou abordar a história do que foi feito e compartilhar a experiência. Provavelmente existem alternativas melhores para um começo menos pedregoso; eu vou tentar listá-las à medida que for abordando os assuntos, mas não é um requisito no momento.\nPonto de partida: infraestrutura Não queria perder tempo discutindo infraestrutura, mas, querendo ou não, entender o tamanho das suas limitações é importante para planejar o que se quer fazer.\nO que estava disponível para conduzir os meus trabalhos eram:\n Máquinas físicas (\u0026ldquo;bare metal\u0026rdquo;) para atuar como nós para as cargas (\u0026ldquo;workers\u0026rdquo;); VMWare VSphere com um datacenter dedicado e cluster datastores pré configurados para a criação de máquinas virtuais para todo o resto.  Não posso compartilhar algo que não fiz; portanto, não vou indicar soluções de virtualização ou orquestração de máquinas virtuais que você pode implantar em máquinas físicas. Mas isso é apenas por pura falta de experiência.\nAlém do que você \u0026ldquo;tem\u0026rdquo;, você precisa avaliar o que você não tem:\nEu, a princípio, não tinha disponível:\n Balanceadores de carga; Acesso ao firewall; Acesso ao roteador; Acesso aos switches para configuração dinâmica das portas (para coisas tipo VLAN);  Dependendo do tamanho da empresa e da hostilidade das equipes diante das tecnologias que você está implantando, você pode ter diversos problemas de integração do seu cluster Kubernetes ao resto da empresa.\nO que eu julgo mais importante nesta etapa é usar o máximo da automação pré-existente. Infelizmente, para mim, não havia nenhuma, e quantidades extenuantes de procedimentos manuais foram executados para garantir atendimento de prazos, o que cobra a conta mais tarde.\nHoje usamos uma combinação de Terraform com instalação via PXE usando Matchbox de maneira muito semelhante ao disposto pelo projeto Typhoon\nDefinições do cluster Existem algumas perguntas que você deve se fazer antes de construir o cluster. Podemos englobar todas elas em uma discussão sobre \u0026ldquo;topologia\u0026rdquo;, embora seja muito mais que isso.\nVou elencar uma lista (não exaustiva) de algumas das decisões mais importantes que pretendo trabalhar nesta série de posts:\n Topologia do Controlplane; Como fazer o deploy do cluster; Como fazer o deploy dos \u0026lsquo;penduricalhos\u0026rsquo; (outros utilitários a serem usados no cluster). Sistema operacional (sim, por incrível que pareça!); Modelo de redes do cluster (i.e., qual o CNI e que recursos avançados deste usar); Modelo de tráfego de entrada - um dos maiores desafios para os clusters on-premises;  Topologia do cluster - Controlplane: Esta definição prévia é importante porque o \u0026lsquo;salto\u0026rsquo; de um modelo mono-master para outro multi-master acaba sendo mais trabalhoso que simplesmente implantar um multi-master de primeira.\nPara fazer testes breves e criar familiaridade com a tecnologia, vale a pena testar o modelo mais simples. Mas muito cuidado ao implantar \u0026lsquo;pilotos\u0026rsquo; e \u0026lsquo;provas de conceito\u0026rsquo; simplórias demais, especialmente se sua organização tem a mania de transformar estes ambientes em produção por decreto.\nO que se encontra de mais comum em topologia de Controlplane é uma organização \u0026lsquo;stacked\u0026rsquo; em que são criados 3 máquinas que combinam Master com ETCDs e uma solução de loadbalancer externa.\nOptamos por:\n 2 VMs para loadbalancers dedicados para os Kubernetes Masters; 2 VMs para Kubernetes masters; 5 VMs de ETCDs dedicados para atender ao cluster Kubernetes; 3 VMs de ETCDs adicionais dedicados para o Calico;  Imagino que não seja necessário mencionar, mas um \u0026lsquo;Kubernetes Master\u0026rsquo; é a máquina que irá executar (no mínimo) os três principais componentes do cluster Kubernetes: o Apiserver (em que múltiplos podem estar ativos no mesmo momento) e os par Scheduler/Controller, os quais apenas um por cluster atua como ativo, funcionando a base de leases para identificar os líderes e assim garantir o failover.\nMaster loadbalancers Em um ambiente multi-master, você precisa de uma maneira de distribuir a carga entre eles que não seja DNS round robin. Algumas empresas contam com hardware dedicado para esta finalidade, que faz healthcheck e coisas elegantes do tipo.\nPor estarem fora do meu \u0026lsquo;alcance\u0026rsquo;, optamos por subir duas VMs com Nginx atuando como proxy para os Apiservers, usando Keepalived para implementar VRRP para o IP virtual associado ao nome do host.\nNota: Interessante notar que projetos como o Kubespray fazem o deploy automático de um Daemonset com Nginx que atua como um proxy interno da comunicação entre os nós e o Apiserver. Se o acesso aos Apiservers por produtos externos ao cluster não for um requisito, de repente nem seria necessário subir essas máquinas. Não é o nosso caso, mas pode ser o seu! Ainda que não use Kubespray, pode usar a ideia como inspiração.\nMasters não \u0026lsquo;stacked\u0026rsquo; nos ETCDs A documentação do Kubernetes descreve, em sua sessão de High Availability, duas topologias básicas:\n Stacked control plane + ETCD nodes; External ETCD nodes  A esmagadora maioria das pessoas opta por uma topologia \u0026lsquo;stacked\u0026rsquo; em que tanto o Kubernetes Master quanto o ETCD são instalados na mesma máquina. É uma topologia óbvia e conveniente. Optamos por não o fazer, pois preferimos usar um cluster ETCD de 5 membros em vez de 3.\nPor que não subir 5 masters, então?\nPorque as máquinas ETCD praticamente não demandam recursos. Um nó ETCD pode rodar, por exemplo, com 4GB de RAM e 2 processadores. Já um Kubernetes Master demanda uma quantidade substancialmente maior de recursos. Como os recursos virtuais eram relativamente exíguos à época, optamos por apenas duas máquinas como Master Kubernetes enquanto criávamos mais máquinas ETCD com \u0026ldquo;o que sobrou\u0026rdquo;.\nObviamente, você deve dimensionar as máquinas \u0026lsquo;de acordo com a necessidade\u0026rsquo;, mas qual um bom ponto de partida? Existia uma tabela interessante na documentação do Kubernetes que fazia uma correlação entre os tipos de instância dos Cloud Providers que você deveria usar de acordo com o tamanho do seu cluster. Ela foi removida na versão 1.19, mas ainda está disponível para as versões anteriores (ou se você selecionar o idioma japonês!), e eu acho legal para ter uma ideia:\n On GCE/Google Kubernetes Engine, and AWS, kube-up automatically configures the proper VM size for your master depending on the number of nodes in your cluster. On other providers, you will need to configure it manually. For reference, the sizes we use on GCE are\n  1-5 nodes: n1-standard-1 6-10 nodes: n1-standard-2 11-100 nodes: n1-standard-4 101-250 nodes: n1-standard-8 251-500 nodes: n1-standard-16 more than 500 nodes: n1-standard-32   And the sizes we use on AWS are\n  1-5 nodes: m3.medium 6-10 nodes: m3.large 11-100 nodes: m3.xlarge 101-250 nodes: m3.2xlarge 251-500 nodes: c4.4xlarge more than 500 nodes: c4.8xlarge  É importante levar em consideração o que ele chama de um nó, que seria uma máquina com não mais que 100 pods - sua realidade pode ser drasticamente diferente do esperado.\nETCDs do Kubernetes O tamanho mínimo aceitável para executar ETCD que ofereça alguma tolerância a falhas corresponde a três nós. Porque cinco então?\nUm cluster de cinco membros permitirá a falha concorrente de dois membros. Em Cloud Providers como a AWS, você provavelmente pode sobreviver com um cluster de três máquinas espalhadas em três Availability Zones diferentes; dificilmente uma Region irá experimentar uma falha em duas AZs ao mesmo tempo.\nEm nosso ambiente, devido a problemas de performance ao usar discos virtuais com o cluster datastore oferecido via SAN, optamos por usar discos NVME locais do host VMWare em que a máquina ETCD está hospedada. Esta dependência do host faz com que valha a pena gastar recursos subindo duas máquinas a mais para essa finalidade, pois não há a flexibilidade de live migration ou outros recursos do tipo.\nEu pretendo abordar ETCD em mais detalhes em outro post, mas aqui vale a ressalva: o cluster ETCD é *mais importante que qualquer outra coisa no ambiente (talvez, exceto, do que você usa para armazenar os dados das aplicações).\nExecutar um cluster on-premise significa arcar com plena administração desta solução, que não é particularmente trivial: é necessário cuidado extremo com os backups (e seus testes), automação afiadíssima para reconstrução do cluster com velocidade em caso de tragédias e monitoração bastante exagerada para tentar identificar a proximidade dos possíveis gargalos de performance antes que sintomas se abatam sobre o cluster.\nETCDs do Calico O uso de ETCDs para o Calico hoje é opcional, já que o uso do próprio Kubernetes como datastore já é considerado maduro e estável o suficiente. Optar por Kubernetes Datastore normalmente demanda implantar um componente adicional do Calico (Typha) para desonerar os apiserver. Embora a recomendação seja para \u0026lsquo;clusters grandes\u0026rsquo;, melhor já ir se acostumando com ele do que ter que implementar emergencialmente porque a performance do cluster está degradada.\nUma vantagem de ter um cluster dedicado para o Calico é que, se você eventualmente quiser fazer um \u0026lsquo;offload\u0026rsquo; da carga do ETCD do Kubernetes removendo, por exemplo, os Event objects como recomendado aqui, as máquinas já estão prontas!\nUma desvantagem de usar ETCD como backend para o Calico é o fato de que todos os nós precisarão de comunicação estável com o cluster ETCD. Em algumas literaturas, uma recomendação comum de segurança é a de sugerir que as máquinas de ETCD estejam em uma rede isolada das demais máquinas, oferecendo acesso apenas ao Kubernetes Apiserver. Dependendo do tipo de rede que você precisa lidar e a quantidade de intermediários atuando entre os barramentos, isso dificilmente é uma opção neste caso.\n",
    "ref": "/marcelo/blog/kubernetes-on-prem-1/"
  },{
    "title": "Calico, Linux e tabelas de rota",
    "date": "",
    "description": "Features não documentadas vão te surpreender quando você menos espera",
    "body": "Mudança no ambiente! Atualização de cada um dos 32 componentes do cluster Kubernetes. Tranquilo! O que pode dar errado?\nApós a estabilização do ambiente, começamos a atualização dos servidores que atuam como balanceadores de carga de entrada para o cluster - aqueles que executam o Haproxy Ingress Controller do dev hero João Morais.\nA métrica de erros começa a estourar do nada; e o melhor, em tese, não estamos recebendo nenhum erro - algo quebrou, e algo completamente desconhecido.\nE agora?\nPreâmbulo - rotas assimétricas Uma coisa que comumente faz falta em equipes de desenvolvimento de software ou mesmo de infra-estrutura é entendimento adequado de como redes funcionam (e de seus detalhes de implementação nos sistemas operacionais).\nNão é raro eu ter que explicar que o problema de conectividade em determinada situação é causado por rotas assimétricas e não bloqueio por firewall.\nNeste caso em particular, temos Pods de sistemas em execução no cluster Kubernetes acessando aplicações por meio de suas URLs públicas. Portanto:\n a comunicação chega aos balanceadores por suas interfaces públicas; os balanceadores também fazem parte do cluster Kubernetes, portanto seu caminho de retorno ocorrerá pelas interfaces privadas;  Até aí tudo bem, fácil de entender.\nO que só anos de experiência como sysadmin Linux traz é o conhecimento de que, por padrão, se a situação de cima ocorrer, uma máquina Linux irá necessariamente descartar o pacote sem qualquer tipo de informação. A Red Hat descreve isso aqui, mas vale a pena copiar porque isso é feito:\n Current recommended practice in RFC3704 is to enable strict mode to prevent IP spoofing from DDos attacks. If using asymmetric routing or other complicated routing, then loose mode is recommended.\n A solução \u0026lsquo;ruim\u0026rsquo; é modificar este comportamento do kernel.\nA solução \u0026lsquo;correta\u0026rsquo; é interferir no roteamento, forçando a resposta a sair por onde veio.\nPreâmbulo do preâmbulo - roteamento no Linux Em uma máquina regular sem \u0026lsquo;efeitos especiais\u0026rsquo;, se você executar o comando abaixo, provavelmente vai receber o seguinte resultado:\n# ip rule list 0: from all lookup local 32766: from all lookup main 32767: from all lookup default Aqui vale uma observação interessante: por \u0026lsquo;default\u0026rsquo;, temos três tabelas definidas nas regras: \u0026lsquo;local\u0026rsquo;, \u0026lsquo;main\u0026rsquo; e \u0026lsquo;default\u0026rsquo;.\nSe alguém te perguntar em inglês \u0026lsquo;which is the default routing table used\u0026rsquo;, qual tabela você chutaria que é a \u0026lsquo;default\u0026rsquo;?\nA resposta, é claro, a tabela main! Por que você escolheria \u0026lsquo;default\u0026rsquo;, ser ignóbil?\n# ip route show table main default via 172.31.48.1 dev eth0 172.31.48.0/20 dev eth0 proto kernel scope link src 172.31.56.202 Em algumas distribuições Linux, como o CoreOS/Flatcar, embora conste na tabela de \u0026lsquo;rules\u0026rsquo;, ela sequer é criada:\n# sudo ip route show table default Error: ipv4: FIB table does not exist. Dump terminated Uma nota relevante: esses nomes só são usados porque são especificados em algum lugar; tabelas de roteamento são representadas, no Kernel, por números. Este lugar é o arquivo /etc/iproute2/rt_tables:\n# cat /etc/iproute2/rt_tables # # reserved values # 255 local 254 main 253 default 0 unspec Aqui você pode prestigiar um pouco da história viva do Linux: em algum ponto no passado distante, a última tabela suportada pelo kernel era necessariamente a 255, usada para \u0026lsquo;local\u0026rsquo;. Mas vivemos tempos modernos, e você pode escolher qualquer número agora que não seja maior que 2147483647 (2^31-1).\nDe volta às rotas assimétricas Para permitir o reencaminhamento dos pacotes para sua interface de origem, eu devo criar uma tabela e colocar uma prioridade superior à tabela padrão do sistema (que é \u0026lsquo;main') para evitar o descarte.\nSe o número 0, 253, 254 e 255 são reservados, e eu tenho até o número 2147483647 para minha tabela, que número eu escolheria?\nTabela 1, é claro. Quem precisa de tantos números?\n# cat /etc/systemd/system/routingpolicy.service [Unit] Description=Configure routes After=network-online.target Requires=network-online.target [Service] Type=oneshot RemainAfterExit=true ExecStart=-/usr/bin/ip route add default via 10.0.0.1 dev ens256 tab 1 ExecStart=-/usr/bin/ip route add 10.0.0.1/8 dev ens256 tab 1 ExecStart=-/usr/bin/ip rule add from 10.0.0.69/32 tab 1 priority 100 A configuração acima resolve meu problema de rota assimétrica, alterando as regras da seguinte maneira:\n# ip rule list 0: from all lookup local 100: from 10.0.0.69 lookup 1 32766: from all lookup main 32767: from all lookup default O que chegar pela Interface com IP 10.0.0.69, ele segue a tabela de rotas 1.\nEsta é a tabela de rota 1, criada pela configuração da unit de systemd acima:\n# ip route show table 1 default via 10.0.0.1 dev ens256 10.0.0.0/8 dev ens256 scope link Diagnóstico Não foi um \u0026lsquo;senhor\u0026rsquo; processo de diagnóstico; sabia-se que algum dos componentes estava interferindo. A lógica aponta para o Calico, já que ele é responsável por amplas modificações nas configurações de rede da máquina.\nA parte triste é que isso não está listado em nenhum release notes. Então não apenas é bem difícil de diagnosticar o que aconteceu, mas também em se preparar para o que iria acontecer.\nA descrição do problema está na página de configuração do felix, a partir da versão 3.14:\n RouteTableRange (FELIX_ROUTETABLERANGE): Calico programs additional Linux route tables for various purposes. RouteTableRange specifies the indices of the route tables that Calico should use. [Default: 1-250]\n Portanto, o Calico irá limpar qualquer conteúdo associado a tabelas existentes da 1 até 250. Como usamos a tabela 1, tivemos problema.\nE ele não limpa apenas uma vez as instruções das tabelas; ele constantemente ajusta, ainda que não vá fazer nada com elas.\n Tá vendo? Deveríamos ter escolhido a tabela 2147483647.\n",
    "ref": "/marcelo/blog/calico-and-route-tables/"
  },{
    "title": "Prometheus node exporter com TLS",
    "date": "",
    "description": "Restringindo acesso ao endpoint de métricas da sua máquina",
    "body": "Ah, Prometheus. O mundo \u0026ldquo;cloud-native\u0026rdquo; simplesmente adora.\nDesde sua recepção pela CNCF em 2016 como primeiro projeto - após o Kubernetes - a ser incubado, a adesão e admiração tem sido crescente, a ponto de você encontrar issues como esta em projetos aleatórios na Internet:\n jnovack commented on Apr 1, 2018 I\u0026rsquo;m pretty sure Prometheus-compatible metrics exposure is almost a requirement in 2018 for project adoption.\n (Ps: o projeto em questão ainda não exporta métricas)\nE isso antes mesmo de ser considerado um projeto \u0026ldquo;graduado\u0026rdquo;\nLogo, como somos modistas, vamos substituir todas as monitorações internas por Prometheus porque a gente pode.\n(Algum dia faço uma análise se isso é estúpido ou genial!)\nO primeiro passo é implantar o Prometheus Node Exporter para exportar métricas gerais do nó (i.e., substituir o tradicional cpu/memória/disco).\n O mínimo que você precisa saber sobre Prometheus para não passar vergonha  Você executa o programa na máquina; Ele abre um endpoint HTTP em uma porta (para o Node Exporter, o default é 9100); Os softwares que você instala e geram métricas são chamados de \u0026ldquo;exporters\u0026rdquo; (dã). Você precisa configurar o servidor para \u0026lsquo;ir buscar\u0026rsquo; as métricas; o nome jurídico disso em prometês é \u0026ldquo;scrape\u0026rdquo;.  O que torna o Prometheus uma ideia genial como solução de monitoração é que o foco não são os exporters, mas sim a instrumentação do código das aplicações usando as bibliotecas do projeto. Assim, sua aplicação pode oferecer nativamente um endpoint para métricas sem precisar de um exporter.\nPrometheus é uma solução de monitoração que vem com sotaque \u0026ldquo;Dev\u0026rdquo;, não \u0026ldquo;Ops\u0026rdquo;.\nPrometheus e TLS O acesso aos endpoints de métricas dos exporters corriqueiramente é feito usando protocolo HTTP simples nas portas expostas, o que não incomoda a maioria das pessoas.\nObviamente isso não é o mais recomendado; você não quer suas métricas expostas por aí, ou mesmo uma saraivada de portas abertas no seu host exposto à Internet, por exemplo.\nAinda há um agravante: determinados exporters (o próprio Node Exporter é um desses), dependendo do tipo de carga a que a máquina está submetida, podem, por si só, onerar excessivamente a máquina em caso de muitas consultas repetidas.\nNós já conseguimos simular um DoS em um host Kubernetes acessando continuamente o endpoint de métricas daquela máquina exposto via HTTP. Logo, deixar estes endpoints expostos não é uma alternativa.\nO problema aqui é: a maioria dos exporters não implementa a opção de controle de acesso ou mesmo HTTPS/TLS. Você precisa se virar.\nExistem várias alternativas para resolver este problema:\n A documentação oficial sugere você instalar e configurar um Nginx; Uma issue no projeto Prometheus Node Exporter sugere o projeto Ghostunnel; Para aplicações que executam em clusters Kubernetes, existe a alternativa do Kube RBAC Proxy.  O que usamos hoje é uma solução interna baseada em Nginx que lê um configmap e expõe diversos exporters em uma única porta, usando o truque de \u0026ldquo;path redirect\u0026rdquo; dos \u0026ldquo;scrapers\u0026quot;; os diversos exporters são configurados para bind em localhost apenas.\nPrometheus Node Exporter com suporte nativo TLS Porém, se você prestou atenção na issue em que é sugerido o Ghostunell, vai observar que ela faz menção à implementação nativa de um endpoint TLS HTTPS para o Prometheus Node Exporter!\nCalma, esta funcionalidade ainda está descrita como experimental, e a recomendação ainda é você manter seu proxy nos ambientes de produção, ok?\nDe jeito nenhum! Aqui é bleeding edge, p@r#!\u0026amp;!\nVamos implantar isso já!\nConfigurando o Prometheus Node Exporter TLS A documentação é tão extensa que eu vou copiar integralmente aqui:\nTLS endpoint ** EXPERIMENTAL ** The exporter supports TLS via a new web configuration file. ./node_exporter --web.config=web-config.yml See the https package for more details. Ok, eles criaram um README.md no diretório com o código.\nAs orientações aqui acompanham um modelo de arquivo de configuração (o tal web-config.yml) que é indicado no exemplo acima, bem como um arquivo com a configuração mínima necessária:\n# web-config.yml # Minimal TLS configuration example. Additionally, a certificate and a key file # are needed. tls_server_config: cert_file: server.crt key_file: server.key A configuração acima configura o endpoint TLS. Mas, obviamente, não faz qualquer tipo de restrição ao acesso - apenas protege as informações usando criptografia.\nO que, no nosso caso, potencializa o ataque de DoS com o consumo extra de CPU do TLS! Só isso não nos serve! Precisamos limitar o acesso a um conjunto de certificados digitais.\nObservando o arquivo de configuração de exemplo, temos o seguinte trecho que interessa:\n # Server policy for client authentication. Maps to ClientAuth Policies. # For more detail on clientAuth options: [ClientAuthType](https://golang.org/pkg/crypto/tls/#ClientAuthType) [ client_auth_type: \u0026lt;string\u0026gt; | default = \u0026quot;NoClientCert\u0026quot; ] Ok, é isso que estamos procurando: ClientAuth. A opção padrão, naturalmente, é NoClientCert, ou seja, não fazer qualquer tipo de exigência, como observamos no exemplo acima.\nQuais são os valores aceitos por este parâmetro?\nBem, era querer demais que estivesse nessa documentação, não é mesmo? Siga o link e olhe direto na biblioteca, seu preguiçoso!\nAo clicar no link descrito no modelo de arquivo de configuração, somos recebidos por uma página com a seguinte informação:\n// ClientAuthType declares the policy the server will follow for TLS Client Authentication. type ClientAuthType int const ( NoClientCert ClientAuthType = iota RequestClientCert RequireAnyClientCert VerifyClientCertIfGiven RequireAndVerifyClientCert ) Opa, estes devem ser os valores válidos para a configuração, certo?\nErrado!\n$ cat /etc/prometheus/web-config.yml tls_server_config: cert_file: /etc/ssl/private/tls.crt key_file: /etc/ssl/private/tls.key client_auth_type: \u0026quot;RequireAnyClientCert\u0026quot; client_ca_file: /etc/ssl/private/tls.ca $ systemctl restart prometheus-node-exporter $ journalctl -fu prometheus-node-exporter ... Oct 29 23:37:21host.intranet docker[59791]: level=error ts=2020-10-30T02:37:21.312Z caller=node_exporter.go:194 err=\u0026quot;Invalid ClientAuth: RequireAnyClientCert\u0026quot; Oct 29 23:37:21 host.intranet systemd[1]: prometheus-node-exporter.service: Main process exited, code=exited, status=1/FAILURE Aproveitando que estamos aqui, nem tente remover o o parâmetro client_auth_type e deixar o client_ca_file no arquivo de configuração, você vai receber este erro aqui:\nOct 29 23:40:08 host.intranet docker[61689]: level=error ts=2020-10-30T02:40:08.981Z caller=node_exporter.go:194 err=\u0026quot;Client CA's have been configured without a Client Auth Policy\u0026quot; O que nem é condenável. Por que você especificaria uma CA sem especificar um Client Auth? Seu idiota!\nEnfim, quais são os parâmetros?\nA melhor maneira de descobrir isso, obviamente, é olhando no código:\n https://github.com/prometheus/node_exporter/blob/master/https/tls_config.go#L145  \tswitch c.ClientAuth { case \u0026quot;RequestClientCert\u0026quot;: cfg.ClientAuth = tls.RequestClientCert case \u0026quot;RequireClientCert\u0026quot;: cfg.ClientAuth = tls.RequireAnyClientCert case \u0026quot;VerifyClientCertIfGiven\u0026quot;: cfg.ClientAuth = tls.VerifyClientCertIfGiven case \u0026quot;RequireAndVerifyClientCert\u0026quot;: cfg.ClientAuth = tls.RequireAndVerifyClientCert case \u0026quot;\u0026quot;, \u0026quot;NoClientCert\u0026quot;: cfg.ClientAuth = tls.NoClientCert default: return nil, errors.New(\u0026quot;Invalid ClientAuth: \u0026quot; + c.ClientAuth) } if c.ClientCAs != \u0026quot;\u0026quot; \u0026amp;\u0026amp; cfg.ClientAuth == tls.NoClientCert { return nil, errors.New(\u0026quot;Client CA's have been configured without a Client Auth Policy\u0026quot;) } Agora sim, sabemos exatamente quais são os parâmetros aceitos! E descobrimos que os caras do Prometheus Node Exporter queriam apenas zoar com a nossa cara:\n Para usar tls.RequestClientCert, use \u0026ldquo;RequestClientCert\u0026rdquo;; Para usar tls.VerifyClientCertIfGiven, configure \u0026ldquo;VerifyClientCertIfGiven\u0026rdquo;; Para usar tls.RequireAndVerifyClientCert, configure \u0026ldquo;RequireAndVerifyClientCert\u0026rdquo;;  Agora:\n Para usar tls.RequireAnyClientCert, use \u0026ldquo;RequireClientCert\u0026rdquo; - sacaram a fuleragem aqui?  Pffff!\nPassada esta etapa, a segunda pergunta: qual a diferença entre eles?\nÉ possível cavocar isso em lugares obscuros, mas eu vou sugerir lemos a incrível documentação escrita pelo projeto Traefik a respeito:\nThe clientAuth.clientAuthType option governs the behaviour as follows: * NoClientCert: disregards any client certificate. * RequestClientCert: asks for a certificate but proceeds anyway if none is provided. * RequireAnyClientCert: requires a certificate but does not verify if it is signed by a CA listed in clientAuth.caFiles. * VerifyClientCertIfGiven: if a certificate is provided, verifies if it is signed by a CA listed in clientAuth.caFiles. Otherwise proceeds without any certificate. * RequireAndVerifyClientCert: requires a certificate, which must be signed by a CA listed in clientAuth.caFiles. Basicamente:\n RequestClientCert: \u0026ldquo;exige\u0026rdquo; o certificado do cliente, mas se o cliente não enviar, tudo bem (?!); RequireAnyClientCert: \u0026ldquo;exige\u0026rdquo; o certificado, dá erro se nenhum for enviado, mas não verifica se ele é assinado pela CA especificada em client_ca_file! VerifyClientCertIfGiven: se o usuário enviar um cerfificado, ele é verificado contra a CA especificada. Se não, prossegue sem. (?!?!?!) RequireAndVerifyClientCert: exige E valida o certificado. Ufa!  Então, se o objetivo é limitar o acesso a usuários com certificados válidos assinados pela CA, apenas o último parâmetro é efetivamente útil.\nRestrição via subject (dn) do certificado A minha solução baseada em Nginx tornava possível a autorização do acesso aos endpoints das métricas apenas para o certificado com um determinado subject (no caso, o certificado configurado no servidor Prometheus responsável pelo scraping).\nPara aqueles curiosos de como fazer isso no Nginx, segue a dica:\n# trechos de um arquivo de configuração de nginx: http { map $ssl_client_s_dn $is_allowed { default no; \u0026quot;CN=prometheus-scraper\u0026quot; yes; } ... location / { ... if ($is_allowed = no) { add_header X-SSL-Client-S-DN $ssl_client_s_dn always; return 403; } ... Infelizmente isso não é possível de ser feito no Prometheus Node Exporter.\nSe alguém acha que a validação por certificado não é forte o suficiente, existe a alternativa de configurar um par usuário/senha para complementar a autenticação como descrito na própria documentação:\nbasic_auth_users: [ \u0026lt;string\u0026gt;: \u0026lt;secret\u0026gt; ... ] Brincadeira divertida!\n",
    "ref": "/marcelo/blog/prometheus-node-exporter-tls/"
  },{
    "title": "De repente, CKA",
    "date": "",
    "description": "4 anos de 'estudo' para uma certificação",
    "body": "Com alguns anos de atraso\u0026hellip;\nA história com Kubernetes e seus colegas Cloud Native vem de 2016 quando a área em que estava acabou e a equipe inteira seria \u0026ldquo;despejada\u0026rdquo; em uma área qualquer. Um grupo dissidente da empresa estava procurando almas perdidas e desenganadas para participar de um projeto esquisito, que, segundo eles, seria altamente diferenciado.\nNenhuma grande empresa de tecnologia jamais dedicaria esforço montando uma infraestrutura alternativa com um software de nome estranho que pouca gente do corpo gerencial sequer ouviu falar, ainda mais mantido por uma \u0026ldquo;skeleton crew\u0026rdquo; de poucos nomes desconihecidos. Que fornecedor vai dar o suporte?\nKubernetes estava na versão 1.4. Não existiam Ingress Controllers, a maioria dos tutoriais faziam menção a Replication Controllers e até mesmo o controle de acesso básico (RBAC) ainda era alpha.\nCom 16 anos de experiência com Linux, 13 desses em ambiente corporativos com muitas pessoas que não exatamente simpatizam com o SO ou com a filosofia de software livre, esse tipo de missão não me é nem um pouco desconfortável. É minha especialidade, na verdade. Mas uma especialidade do ponto de vista filosófico, e não da tecnologia.\nIsso porque eu estava preguiçosamente acomodado, completamente estagnado. Não acompanhava tendências gerais do mercado ou grandes atualizações. Não havia porque estudar Clouds públicas se minha empresa jamais poderia usá-las. Não havia porque tirar certificações - não estava procurando emprego, nem havia ganho interno em fazê-lo.\nFoi-me jogado no colo o desafio de implantar um conjunto de tecnologias altamente desafiadoras cujos nomes eu sequer havia ouvido falar no passado em quatro meses. Nem mesmo meu sistema operacional ficou intocado, sendo requisito o uso de uma distribuição estranha sem dpkg ou rpm completamente orientada a contêineres.\nEu poderia ter escolhido a estabilidade e seguido meu caminho por uma vida seguramente menos conturbada. Fui chamado para dar o meu melhor em outra posição na qual meu conhecimento já consolidado seria útil. Eu não precisaria perder várias madrugadas estudando um apinhado de coisas novas.\nFoi-me dada esta opção, e eu recusei.\nFaz precisamente 4 anos que o ambiente implantado entrou em produção. Hoje, este cluster executa de maneira impecável centenas de sistemas em diversas etapas, com maior ou menor grau de importância, severidade e volume de acesso.\nMas mais que o cluster, faz quatro anos que eu entrei \u0026ldquo;em produção\u0026rdquo;.\nEu sempre entreguei resultados acima da média. Mas a diferença entrre a qualidade do atual tipo de trabalho não tem qualquer tipo de comparação com o passado.\nPor fim, tornei-me um profissional imensamente superior. Sou referência no que faço, um dos poucos com visão sistêmica de todas as tecnologias envolvidas e que, agora sim, entende exatamente suas relações e o lugar certo de olhar quando há problemas.\nMais que isso, agora não estou mais \u0026ldquo;para trás\u0026rdquo;: acompanho as tendências, evoluo meu trabalho de acordo e me preparo para as possíveis mudanças que estão por vir.\nDe um funcionário público encostado com futuro profissional discutível, \u0026ldquo;ascendi\u0026rdquo; a uma posição de desejável pelo próprio mercado de trabalho brasileiro e internacional.\nNada mal para alguém cujo trabalho valia tão pouco aos olhos da empresa que teve seu setor extinto.\nAinda não tive a oportunidade de compensar os mais de 10 anos de atraso no auto desenvolvimento, mas chegamos lá.\nObrigado por tudo, Kubernetes!\n",
    "ref": "/marcelo/blog/suddenly-cka/"
  },{
    "title": "Linux, contêineres e descritores de arquivos",
    "date": "",
    "description": "Ou: por que você PRECISA de um Linux guru na sua equipe de 'software engineers'",
    "body": "O mundo mudou, e tudo hoje é \u0026ldquo;software engineer\u0026rdquo; e containers em todas as direções. X as a Code, \u0026ldquo;abordagem dev para tudo\u0026rdquo;, \u0026ldquo;o que um dev faria?\u0026rdquo;, e assim vai. E como fica o profissional de infraestrutura? Aquele cara que hoje é conhecido como Ops puro sangue, e que foi basicamente substituído por desenvolvedores que entendem um pouco mais de operação que os outros?\nEu sou um desses caras. Um Guru Linux das antigas, dos tempos do Slackware, dependency chain download e das configurações de X que queimavam monitores.\nTalvez este post de hoje mostre que ainda vale a pena ter \u0026ldquo;um de nós\u0026rdquo; no seu time!\n Uma nova imagem docker para uma solução interna foi gerada com atualizações e melhorias. A imagem está 100% operacional; todos os testes funcionais estão ok.\nMas ela está com um pequeno detalhe: não temos logs.\nE basicamente o único propósito desta imagem é gerar esses logs, que servem para auditoria. Sem logs, ela não serve para absolutamente nada.\nO que mudou no ambiente entre as duas versões? Basicamente tudo:\n Versão do SO; Versão do Docker; Versão do Kubernetes; Versão da imagem base; Versão do próprio software; Scripts de inicialização do container;  E aí, por onde começar?\n Aqui, um pouco de contexto:\nA aplicação é legada e não sabe jogar logs na saída padrão.\nPara remediar este problema, a solução foi:\n... ln -sf /dev/stdout /var/log/app/aplication.log \u0026amp;\u0026amp; \\ ...  Nota: nem sempre isso resolve o problema. O Tinyproxy e o [pureftpd],(https://www.pureftpd.org/) em algum momento na história, não funcionavam, embora atualmente salvo engano estão ok).\n  Esse problema foi trazido para a equipe.\nComo tudo foi mudado (o terror de quem adora culpar \u0026ldquo;a mudança\u0026rdquo; pela causa do problema), é particularmente doloroso descascar essa cebola sem chorar; em especial, se te falta conhecimento em sistemas operacionais.\nE é por isso que é importante que as empresas mantenham por perto seus Gurus em Ops!\nA solução imediata proposta por um dos outros Ops foi:\n introduzir um syslog no container via sidecar; reconfigurar a aplicação para enviar seus logs para o syslog; fazer o syslog exibir as mensagens na saída padrão.  Possivelmente esta é a solução adequada, mas eu me recuso a aceitar a derrota. Introduzir mais um container (ou processo) sem entender o que aconteceu, para mim, é inaceitável. Pode até ser implementado dessa forma, depois que o problema for devidamente entendido.\n Analisando o Dockerfile Como listado acima, foi criado um link simbólico associando o arquivo de logs a /dev/stdout:\n... ln -sf /dev/stdout /var/log/app/aplication.log \u0026amp;\u0026amp; \\ ... E o que isso representa para o container em execução?\nlrwxrwxrwx 1 root root 11 Mar 27 2018 /var/log/app/application.log -\u0026gt; /dev/stdout lrwxrwxrwx 1 user user 15 Oct 24 00:31 /dev/stdout -\u0026gt; /proc/self/fd/1 lrwx------ 1 user user 64 Oct 24 02:34 /proc/self/fd/1 -\u0026gt; /dev/pts/0 Basicamente, que estamos linkando o arquivo para /proc/self/fd/1.\nSempre lembrando que, no Linux, cada processo normalmente tem os três descritores de arquivo padrão POSIX:\n stdin: fd/0 stdout: fd/1 stderr: fd/2  Então estamos associado o arquivo de logs da aplicação ao stdout (fd/1) do processo (self).\nAté aqui, tudo bem.\n Para quem nunca parou para pensar, essa é a explicação de uma outra famosa \u0026lsquo;frase shell\u0026rsquo;:\n# Por que isso... $ command \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # É diferente disso? $ command 2\u0026gt;\u0026amp;1 \u0026gt; /dev/null Neste caso, você está redirecionando o descritor de arquivos stdout (ou 1) que será aberto para seu comando para /dev/null (\u0026gt;, também podendo ser escrito 1\u0026gt;). E redirecionando o descritor de arquivos stderr (ou 2) para o mesmo descritor de arquivos de 1, (i.e., /dev/null também). Isso faz com que toda a saída do comando \u0026lsquo;desapareça\u0026rsquo; - muito comum em scripts nos quais os devs querem sumir com todo tipo de mensagem de warning, e que torna depurar a causa do problema um terror mais tarde!\nSe você fizer na ordem inversa, você redireciona stderr para stdout, e depois, stdout para /dev/null. Essa operação não impacta a anterior; logo, a saída regular do comando é suprimida, mas a saída de erro ainda será exibida (no lugar \u0026ldquo;errado\u0026rdquo;, mas será). É útil se o programa é excessivamente \u0026lsquo;verbose\u0026rsquo; e você só tem interesse nas possíveis mensagens de erro.\nNão entendeu? Alguém desenhou aqui.\nO que o docker faz para exibir logs? ODocker possui um parâmetro log-driver que habilita a conhecida função docker logs - apenas os logdrivers \u0026lsquo;json-file\u0026rsquo; e \u0026lsquo;journald\u0026rsquo; viabilizam seu uso. O log-driver padrão é \u0026lsquo;json-file\u0026rsquo;.\nA descrição deste log-driver está na documentação oficial:\nBy default, Docker captures the standard output (and standard error) of all your containers, and writes them in files using the JSON format. The JSON format annotates each line with its origin (stdout or stderr) and its timestamp. Each log file contains information about only one container. Portanto, o Docker joga o conteúdo gerado pelo fd/1 e fd/2 do container em um arquivo.\n Nota: Um Pod Kubernetes composto por 2 containers irá gerar dois arquivos diferentes.\n Não tem exemplos na documentação, então aqui segue um:\n{\u0026quot;log\u0026quot;:\u0026quot;I1024 03:08:36.962547 12 instance.go:317] updating 0 host(s): []\\n\u0026quot;,\u0026quot;stream\u0026quot;:\u0026quot;stderr\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2020-10-24T03:08:36.962684216Z\u0026quot;} Ele inclusive tem a bondade de ilustrar para você de que tipo é (\u0026ldquo;stream\u0026rdquo;: \u0026ldquo;stderr\u0026quot;).\nOs arquivo normalmente são jogados em /var/lib/docker/containers, mas isso é irrelevante.\nO que realmente é relevante é como o Docker faz a ponte da saída dos comandos para este arquivo.\nComo o linux associa os descritores de arquivos de processos? Antes de chegar no Docker, como ver os descritores de arquivos de um processo?\nPrimeiro, descobrimos o pid do nosso processo, por exemplo, o shell da minha sessão atual:\n$ ps PID TTY TIME CMD 9896 pts/0 00:00:00 bash 195216 pts/0 00:00:00 ps PID 9896, ok. Vamos conferir:\nls -l /proc/9896/fd total 0 lrwx------. 1 core core 64 Oct 24 00:43 0 -\u0026gt; /dev/pts/0 lrwx------. 1 core core 64 Oct 24 00:43 1 -\u0026gt; /dev/pts/0 lrwx------. 1 core core 64 Oct 24 00:43 2 -\u0026gt; /dev/pts/0 lrwx------. 1 core core 64 Oct 24 01:20 255 -\u0026gt; /dev/pts/0 (255? Sim, porque bash. Deixo essa história para outro dia.)\nE o que é /dev/pts/0? São os \u0026ldquo;pseudo terminais\u0026rdquo; criados para podermos interagir com o SO. Se você abre múltiplas janelas de terminais, vai ver isso aqui:\n$ ls /dev/pts 0 1 10 11 12 2 3 4 5 6 7 8 9 ptmx Na máquina acima, só tenho um:\n$ ls /dev/pts/ 0 ptmx (E o ptmx? shhhh, ele é o \u0026lsquo;master\u0026rsquo;! Mas deixa esse assunto pra lá.)\nTá, o bash abriu um terminal. E outros programas que normalmente rodam em backgroun, por exemplo, o kubelet?\nsudo ls -l /proc/$( pgrep kubelet )/fd/ total 0 lr-x------. 1 root root 64 Sep 19 19:17 0 -\u0026gt; /dev/null lrwx------. 1 root root 64 Sep 19 19:17 1 -\u0026gt; 'socket:[14836]' lrwx------. 1 root root 64 Sep 19 19:17 10 -\u0026gt; 'socket:[49723]' lrwx------. 1 root root 64 Sep 19 19:17 11 -\u0026gt; 'socket:[25802]' lrwx------. 1 root root 64 Sep 19 19:17 12 -\u0026gt; 'socket:[33709]' lr-x------. 1 root root 64 Sep 19 19:17 13 -\u0026gt; anon_inode:inotify lrwx------. 1 root root 64 Sep 19 19:17 15 -\u0026gt; 'socket:[34950]' lr-x------. 1 root root 64 Sep 19 19:17 16 -\u0026gt; 'pipe:[14848]' l-wx------. 1 root root 64 Sep 19 19:17 17 -\u0026gt; 'pipe:[14848]' lrwx------. 1 root root 64 Sep 19 19:17 18 -\u0026gt; 'socket:[39097]' lrwx------. 1 root root 64 Sep 19 19:17 19 -\u0026gt; 'socket:[34949]' lrwx------. 1 root root 64 Sep 19 19:17 2 -\u0026gt; 'socket:[14836]' lrwx------. 1 root root 64 Sep 19 19:17 20 -\u0026gt; 'socket:[3073082]' lr-x------. 1 root root 64 Sep 19 19:17 21 -\u0026gt; /dev/kmsg lrwx------. 1 root root 64 Sep 19 19:17 22 -\u0026gt; 'socket:[274349500]' lrwx------. 1 root root 64 Sep 19 19:17 23 -\u0026gt; 'socket:[802786832]' lrwx------. 1 root root 64 Sep 19 19:17 26 -\u0026gt; 'socket:[44326]' lrwx------. 1 root root 64 Sep 19 19:17 27 -\u0026gt; 'socket:[44328]' lr-x------. 1 root root 64 Sep 19 19:17 28 -\u0026gt; anon_inode:inotify lrwx------. 1 root root 64 Oct 13 22:33 29 -\u0026gt; 'socket:[2566051059]' lrwx------. 1 root root 64 Sep 19 19:17 3 -\u0026gt; 'socket:[14840]' lr-x------. 1 root root 64 Sep 19 19:17 30 -\u0026gt; anon_inode:inotify lrwx------. 1 root root 64 Sep 19 19:17 32 -\u0026gt; 'socket:[36435]' lr-x------. 1 root root 64 Sep 19 19:17 33 -\u0026gt; anon_inode:inotify lrwx------. 1 root root 64 Sep 19 19:17 35 -\u0026gt; 'socket:[14849]' lr-x------. 1 root root 64 Sep 19 19:17 36 -\u0026gt; /dev/kmsg lrwx------. 1 root root 64 Sep 19 19:17 37 -\u0026gt; 'anon_inode:[eventpoll]' lr-x------. 1 root root 64 Sep 19 19:17 38 -\u0026gt; 'pipe:[25803]' l-wx------. 1 root root 64 Sep 19 19:17 39 -\u0026gt; 'pipe:[25803]' lrwx------. 1 root root 64 Sep 19 19:17 4 -\u0026gt; 'anon_inode:[eventpoll]' lrwx------. 1 root root 64 Sep 19 19:17 5 -\u0026gt; 'socket:[2621822183]' lrwx------. 1 root root 64 Sep 19 21:17 52 -\u0026gt; 'socket:[1275760594]' lrwx------. 1 root root 64 Sep 19 19:17 6 -\u0026gt; 'socket:[2623166919]' lrwx------. 1 root root 64 Sep 19 19:17 7 -\u0026gt; 'anon_inode:[eventpoll]' lrwx------. 1 root root 64 Sep 19 19:17 8 -\u0026gt; 'socket:[30969]' lrwx------. 1 root root 64 Sep 19 19:17 9 -\u0026gt; 'socket:[30971]' Putz, que horror, hein?\nBem, o que importa é que descritores de arquivos são associados a coisas. Pode ser um terminal, pode ser qualquer uma das tralhas acima.\nComo o docker faz a ponte entre container e log file Para ver facilmente informações sobre o seu container no systemd, você pode executar o comando machinectl para listar os containeres existentes:\n# machinectl No machines. Hum. Esqueci que o Docker odeia o systemd e eles não se conversam direito.\nVamos do jeito mais difícil:\n# systemd-cgls Control group /: -.slice ├─kubepods │ ├─burstable ... │ │ ├─pod6540eb7c-34d1-42c4-b6c7-cf83bd05e472 │ │ │ ├─df5869b859ea271c8836708c055b32387d26584c2e2824b0898e8bcbf7f6d6e5 │ │ │ │ └─164408 /pause │ │ │ └─8169619db23377b7df0ba05babcb8e3c17cfa662ff024b8cd136a4c6e2594070 │ │ │ ├─165234 /sbin/tini /usr/local/bin/start.sh │ │ │ ├─165311 /bin/bash /usr/local/bin/start.sh │ │ │ ├─/ bash /usr/local/bin/notify.sh │ │ │ ├─166143 inotifywait -qrm --exclude=.*\\.conf -e MOVED_TO --format %e:... │ │ │ ├─166155 /usr/bin/app Aqui vemos um típico Pod Kubernetes, que é sempre composto por pelo menos dois containers:\n O famoso container \u0026lsquo;pause\u0026rsquo; (df5869b859ea\u0026hellip;) O container da aplicação (8169619db233\u0026hellip;).  O container conta com um init simplório (tini) que executa um shell script, que executa um shell script(!), que executa a aplicação.\nA partir do nome do container, é possível investigar o PID do processo encarregado pelo containerd de cuidar do nosso container:\n# ps auxw | fgrep b68d2844a70ea6 | head -n1 root 144768 0.0 0.0 8564 4604 ? Sl 02:34 0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/b68d2844a70ea62317392d31f16590ef05f1afb2431519c47df7bfaa3ecd81f9 -address /run/docker/libcontainerd/docker-containerd.sock -containerd-binary /run/torcx/unpack/docker/bin/containerd -runtime-root /var/run/docker/runtime-runc -debug E, com isso, podemos ver seus file descriptors:\n# # Suprimi alguns da saída., daí os números ausentes. # ls -l /proc/144768/fd total 0 lr-x------. 1 root root 64 Oct 24 02:36 0 -\u0026gt; /dev/null lrwx------. 1 root root 64 Oct 24 02:36 1 -\u0026gt; 'socket:[25008]' lrwx------. 1 root root 64 Oct 24 02:36 2 -\u0026gt; 'socket:[25008]' ... lrwx------. 1 root root 64 Oct 24 02:36 4 -\u0026gt; 'anon_inode:[eventpoll]' lrwx------. 1 root root 64 Oct 24 02:36 5 -\u0026gt; 'anon_inode:[eventpoll]' lrwx------. 1 root root 64 Oct 24 02:36 6 -\u0026gt; 'socket:[3134831467]' lrwx------. 1 root root 64 Oct 24 02:36 7 -\u0026gt; 'socket:[3134853345]' lr-x------. 1 root root 64 Oct 24 02:36 8 -\u0026gt; 'pipe:[3134884039]' ... lr-x------. 1 root root 64 Oct 24 02:36 10 -\u0026gt; 'pipe:[3134884040]' lr-x------. 1 root root 64 Oct 24 02:36 12 -\u0026gt; 'pipe:[3134884041]' lrwx------. 1 root root 64 Oct 24 02:34 21 -\u0026gt; /dev/pts/ptmx Interessante.\nVamos olhar os descritores de arquivos desses processos:\n# ls -l /proc/{165234,165311,166142,166143,166155}/fd/ /proc/165234/fd/: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lr-x------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; ''pipe:[3134884039]'' l-wx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; 'pipe:[3134884040]' l-wx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; 'pipe:[3134884041]' /proc/165311/fd: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lr-x------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; ''pipe:[3134884039]'' l-wx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; 'pipe:[3134884040]' l-wx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; 'pipe:[3134884041]' lr-x------. 1 31 31 64 Oct 24 01:42 255 -\u0026gt; /usr/local/bin/start.sh /proc/166142/fd: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lr-x------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; ''pipe:[3134884039]'' l-wx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; 'pipe:[3134884040]' l-wx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; 'pipe:[3134884041]' lr-x------. 1 31 31 64 Oct 24 01:42 255 -\u0026gt; /usr/local/bin/notify.sh /proc/166143/fd: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lr-x------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; ''pipe:[3134884039]'' l-wx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; 'pipe:[2185069392]' l-wx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; /dev/null lr-x------. 1 31 31 64 Oct 24 01:42 3 -\u0026gt; anon_inode:inotify /proc/166155/fd/: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lrwx------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; /dev/null lrwx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; /dev/null lrwx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; /dev/null Ahá!\nBasicamente, todos os stdout dos PIDs dos shell scripts foram direcionados para\u0026rsquo;pipe:[3134884040]'! Este pipe está associado a um outro fd do containerd-shim que fará as mensagens chegarem no arquivo de logs apropriado em /var/lib/docker/containers.\nMas e o processo da aplicação no container? Tanto stdin, quando stdout quanto stderr apontam para /dev/null!\nPor que isso aconteceu?\nBem, porque a maior parte dos daemons no Linux, ao subir, fazem a atribuição de pelo menos os fds 0 e 1 a /dev/null. Alguns ainda deixam stderr livre para imprimir mensagens de erro, mas não foi o caso dessa aplicação.\nVeja este container aqui com nginx:\n# ps auxw PID USER TIME COMMAND 1 nobody 0:00 /usr/bin/dumb-init -- /usr/local/bin/start-inotify.sh 8 nobody 0:00 {start-inotify.s} /bin/bash /usr/local/bin/start-inotify.sh 10 nobody 0:00 nginx: master process nginx -c /etc/nginx/nginx.conf 11 nobody 13:17 nginx: worker process # ls -l /proc/10/fd total 0 lrwx------ 1 nobody nobody 64 Oct 24 05:19 0 -\u0026gt; /dev/null lrwx------ 1 nobody nobody 64 Oct 24 05:19 1 -\u0026gt; /dev/null l-wx------ 1 nobody nobody 64 Oct 24 05:19 2 -\u0026gt; pipe:[3001744697] lrwx------ 1 nobody nobody 64 Oct 24 05:19 3 -\u0026gt; socket:[3001748727] l-wx------ 1 nobody nobody 64 Oct 24 05:19 4 -\u0026gt; pipe:[3001744697] l-wx------ 1 nobody nobody 64 Oct 24 05:19 5 -\u0026gt; pipe:[3001744696] lrwx------ 1 nobody nobody 64 Oct 24 05:19 6 -\u0026gt; socket:[3001742291] lrwx------ 1 nobody nobody 64 Oct 24 05:19 7 -\u0026gt; socket:[3001748728] Então, relembrando o nosso Dockerfile:\n... ln -sf /dev/stdout /var/log/app/aplication.log \u0026amp;\u0026amp; \\ ... Não é nenhuma surpresa que não esteja funcionando. Você mandou apontar o log da aplicação para /dev/stdout, que é /dev/null.\nComo resolver? Este é o famoso caso em que se demora mais para explicar a solução do problema que para resolver o problema.\nA explicação já foi acima.\nA troca abaixo \u0026ldquo;resolve\u0026rdquo; o problema:\n ln -sf /dev/stdout /var/log/app/aplication.log \u0026amp;\u0026amp; \\ por\n ln -sf /proc/1/fd/1 /var/log/app/aplication.log \u0026amp;\u0026amp; \\ Um truque sujo, simples, porém altamente eficaz e com o mínimo de trauma no deploy da aplicação. O stdout do pid 1 não é /dev/null, e agora o logs da aplicação aparecem normalmente.\n Nota: solução pode não funcionar em ambientes com \u0026ndash;pid=host, SELinux ou Apparmor ativo sem modificações das policies padrão!\n ",
    "ref": "/marcelo/blog/linux-containers-and-file-descriptors/"
  },{
    "title": "Calicoctl, TLS e gestão de certificados",
    "date": "",
    "description": "O que fazer quando nem o modo DEBUG das ferramentas te dá uma dica?",
    "body": "Estava precisando configurar algumas GlobalNetworkPolicies em um cluster Kubernetes; para isso, é necessário intervir diretamente no Calico, pois o Kubernetes ainda não conta com objetos de Networkpolicies que não sejam \u0026lsquo;namespaced\u0026rsquo;.\nObviamente, usamos \u0026lsquo;backend\u0026rsquo; ETCD: por que simplificaríamos tudo usando CRDs no próprio Kubernetes, não é? Deixamos isso para os amadores!\n(PS: fazemos assim porque somos tão pioneiros em usar tecnologias \u0026lsquo;bleeding edge\u0026rsquo; que, à época da implantação, usar Kubernetes como backend sequer era uma opção - sim, nós somos \u0026lsquo;that old school\u0026rsquo;! Ah, e naturalmente, somos preguiçosos demais para migrar agora.)\nMas voltando ao assunto\u0026hellip;\n\u0026ldquo;Calico, dê-me informações!\u0026rdquo;\n$ calicoctl -l debug get globalnetworkpolicies _ NADA!\nPutz, caíram minhas regras de firewall de novo?\nChamei um teste de conectividade nas portas, tudo ok:\n$ for i in 192.168.0.11 192.168.0.12 192.168.0.13 ; do { timeout 1 curl -svz1 telnet://$i:2379 2\u0026gt;\u0026amp;1 ; } | grep Connected; done * Connected to 192.168.0.11 (192.168.0.11) port 2379 (#0) * Connected to 192.168.0.12 (192.168.0.12) port 2379 (#0) * Connected to 192.168.0.13 (192.168.0.13) port 2379 (#0) Nah, tudo ok.\nDEBUG, ATIVAR:\n$ calicoctl -l debug get globalnetworkpolicies INFO[0000] Log level set to debug INFO[0000] Executing config command DEBU[0000] Resource: projectcalico.org/v3, Kind=Node DEBU[0000] Data: - apiVersion: projectcalico.org/v3 kind: Node metadata: creationTimestamp: null spec: {} status: {} DEBU[0000] Loading config from JSON or YAML data DEBU[0000] Datastore type: etcdv3 INFO[0000] Loaded client config: apiconfig.CalicoAPIConfigSpec{DatastoreType:\u0026quot;etcdv3\u0026quot;, EtcdConfig:apiconfig.EtcdConfig{EtcdEndpoints:\u0026quot;https://192.168.0.11:2379,https://192.168.0.12:2379,https://192.168.0.13:2379\u0026quot;, EtcdDiscoverySrv:\u0026quot;\u0026quot;, EtcdUsername:\u0026quot;\u0026quot;, EtcdPassword:\u0026quot;\u0026quot;, EtcdKeyFile:\u0026quot;/etc/calico/tls/acof/tls.key\u0026quot;, EtcdCertFile:\u0026quot;/etc/calico/tls/acof/tls.crt\u0026quot;, EtcdCACertFile:\u0026quot;/etc/calico/tls/acof/tls.ca\u0026quot;, EtcdKey:\u0026quot;\u0026quot;, EtcdCert:\u0026quot;\u0026quot;, EtcdCACert:\u0026quot;\u0026quot;}, KubeConfig:apiconfig.KubeConfig{Kubeconfig:\u0026quot;\u0026quot;, K8sAPIEndpoint:\u0026quot;\u0026quot;, K8sKeyFile:\u0026quot;\u0026quot;, K8sCertFile:\u0026quot;\u0026quot;, K8sCAFile:\u0026quot;\u0026quot;, K8sAPIToken:\u0026quot;\u0026quot;, K8sInsecureSkipTLSVerify:false, K8sDisableNodePoll:false, K8sUsePodCIDR:false, KubeconfigInline:\u0026quot;\u0026quot;, K8sClientQPS:0}} DEBU[0000] Using datastore type 'etcdv3' INFO[0000] Client: {{{CalicoAPIConfig projectcalico.org/v3} { 0 {{0 0 \u0026lt;nil\u0026gt;}} \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[] [] [] []} {etcdv3 {https://192.168.0.11:2379,https://192.168.0.12:2379,https://192.168.0.13:2379 /etc/calico/tls/acof/tls.key /etc/calico/tls/acof/tls.crt /etc/calico/tls/acof/tls.ca } { false false false 0}}} 0xc00000ec18 0xc0001fed70} DEBU[0000] Processing List request list-interface=Node rev= DEBU[0000] Get Global Resource key from /calico/resources/v3/projectcalico.org/nodes DEBU[0000] Didn't match regex DEBU[0000] List options is a parent prefix, ensure path ends in / list-interface=Node rev= DEBU[0000] Adding / to path list-interface=Node rev= DEBU[0000] Calling Get on etcdv3 client etcdv3-etcdKey=/calico/resources/v3/projectcalico.org/nodes/ list-interface=Node rev= Putz, imprime um monte de lixo inútil para a resolução do problema, e trava na requisição ao servidor ETCD do mesmo jeito.\nQual o próximo passo lógico de diagnóstico agora? Tarô? Leitura de mão?\n A chave para a solução do problema passa sutilmente despercebida na diretiva EtcdEndpoints:\n...https://... Esse tipo de erro obscuro, fantasma, de conexões que não começam (ou, no caso, que nunca terminal) são típicos de problemas de validação TLS. Mas que tipo de validação TLS?\nUm programa \u0026ldquo;bem feito\u0026rdquo; normalmente ajuda a depuração. Vamos analisar o comportamento do comando curl quando exposto a diversos problemas de validação TLS:\n Autoridade certificadora inválida:  # curl https://etcd1.local:2379 curl: (60) Peer's Certificate issuer is not recognized.  Nome do servidor não consta no certificado:  # curl --cacert tls/sp2/ca.pem --resolve hostname.invalido:2379:10.99.17.11 https://hostname.invalido:2379 curl: (51) Unable to communicate securely with peer: requested domain name does not match the server's certificate.  O servidor requer autenticação com certificado válido pelo cliente, e vocẽ não enviou nenhum:  # curl --cacert tls/sp2/ca.pem https://etcd1.local:2379 curl: (58) NSS: client certificate not found (nickname not specified) Bem, já é o suficiente.\nO calicoctl, entretanto, deixa bastante a desejar nesse cenário (A Tigera possivelmente acredita que você não terá problemas para reconhecer algo tão bobo?).\nPraticamente todos os erros que envolvem certificados e TLS terminam com o calicoctl travando por tempo indefinido.\n Autoridade certificadora inválida:  # ETCD_ENDPOINTS=https://etcd1.local:2379 calicoctl -l debug get nodes ... INFO[0000] Loaded client config: apiconfig.CalicoAPIConfigSpec{DatastoreType:\u0026quot;etcdv3\u0026quot;, EtcdConfig:apiconfig.EtcdConfig{EtcdEndpoints:\u0026quot;https://etcd1.local:2379\u0026quot;, EtcdDiscoverySrv:\u0026quot;\u0026quot;, EtcdUsername:\u0026quot;\u0026quot;, EtcdPassword:\u0026quot;\u0026quot;, EtcdKeyFile:\u0026quot;\u0026quot;, EtcdCertFile:\u0026quot;\u0026quot;, EtcdCACertFile:\u0026quot;\u0026quot;, EtcdKey:\u0026quot;\u0026quot;, EtcdCert:\u0026quot;\u0026quot;, EtcdCACert:\u0026quot;\u0026quot;}, KubeConfig:apiconfig.KubeConfig{Kubeconfig:\u0026quot;\u0026quot;, K8sAPIEndpoint:\u0026quot;\u0026quot;, K8sKeyFile:\u0026quot;\u0026quot;, K8sCertFile:\u0026quot;\u0026quot;, K8sCAFile:\u0026quot;\u0026quot;, K8sAPIToken:\u0026quot;\u0026quot;, K8sInsecureSkipTLSVerify:false, K8sDisableNodePoll:false, K8sUsePodCIDR:false, KubeconfigInline:\u0026quot;\u0026quot;, K8sClientQPS:0}} ... ^C  Nome do servidor não consta no certificado:  # # Criei uma entrada no /etc/hosts para etcd.devsres.com, o nome não resolve. # ETCD_ENDPOINTS=https://etcd.devsres.com:2379 ETCD_CA_CERT_FILE=tls/tls.ca calicoctl -l debug get nodes ... INFO[0000] Loaded client config: apiconfig.CalicoAPIConfigSpec{DatastoreType:\u0026quot;etcdv3\u0026quot;, EtcdConfig:apiconfig.EtcdConfig{EtcdEndpoints:\u0026quot;https://etcd.devsres.com:2379\u0026quot;, EtcdDiscoverySrv:\u0026quot;\u0026quot;, EtcdUsername:\u0026quot;\u0026quot;, EtcdPassword:\u0026quot;\u0026quot;, EtcdKeyFile:\u0026quot;\u0026quot;, EtcdCertFile:\u0026quot;\u0026quot;, EtcdCACertFile:\u0026quot;tls/tls.ca\u0026quot;, EtcdKey:\u0026quot;\u0026quot;, EtcdCert:\u0026quot;\u0026quot;, EtcdCACert:\u0026quot;tls/tls.ca\u0026quot;}, KubeConfig:apiconfig.KubeConfig{Kubeconfig:\u0026quot;\u0026quot;, K8sAPIEndpoint:\u0026quot;\u0026quot;, K8sKeyFile:\u0026quot;\u0026quot;, K8sCertFile:\u0026quot;\u0026quot;, K8sCAFile:\u0026quot;\u0026quot;, K8sAPIToken:\u0026quot;\u0026quot;, K8sInsecureSkipTLSVerify:false, K8sDisableNodePoll:false, K8sUsePodCIDR:false, KubeconfigInline:\u0026quot;\u0026quot;, K8sClientQPS:0}} ... ^C  O servidor requer autenticação com certificado válido pelo cliente, e vocẽ não enviou nenhum:  # ETCD_ENDPOINTS=https://sp2srvvpkv00001:2379 ETCD_CA_CERT_FILE=tls/tls.ca calicoctl -l debug get nodes ... INFO[0000] Loaded client config: apiconfig.CalicoAPIConfigSpec{DatastoreType:\u0026quot;etcdv3\u0026quot;, EtcdConfig:apiconfig.EtcdConfig{EtcdEndpoints:\u0026quot;https://sp2srvvpkv00001:2379\u0026quot;, EtcdDiscoverySrv:\u0026quot;\u0026quot;, EtcdUsername:\u0026quot;\u0026quot;, EtcdPassword:\u0026quot;\u0026quot;, EtcdKeyFile:\u0026quot;\u0026quot;, EtcdCertFile:\u0026quot;\u0026quot;, EtcdCACertFile:\u0026quot;tls/tls.ca\u0026quot;, EtcdKey:\u0026quot;\u0026quot;, EtcdCert:\u0026quot;\u0026quot;, EtcdCACert:\u0026quot;\u0026quot;}, KubeConfig:apiconfig.KubeConfig{Kubeconfig:\u0026quot;\u0026quot;, K8sAPIEndpoint:\u0026quot;\u0026quot;, K8sKeyFile:\u0026quot;\u0026quot;, K8sCertFile:\u0026quot;\u0026quot;, K8sCAFile:\u0026quot;\u0026quot;, K8sAPIToken:\u0026quot;\u0026quot;, K8sInsecureSkipTLSVerify:false, K8sDisableNodePoll:false, K8sUsePodCIDR:false, KubeconfigInline:\u0026quot;\u0026quot;, K8sClientQPS:0}} ... ^C Enfim, você não pode confiar que o calicoctl irá te contar se houver qualquer tipo de falha na validação TLS, seja ela do lado do servidor ou do cliente.\nA mensagem que me permitiu diagnosticar adequadamente o problema foi a consulta dos logs dos servidores ETCD diretamente:\nOct 15 12:18:29 etcd1.local docker[832]: 2020-10-15 15:18:29.066104 I | embed: rejected connection from \u0026quot;192.168.255.42:54608\u0026quot; (error \u0026quot;tls: failed to verify client's certificate: x509: certificate has expired or is not yet valid\u0026quot;, ServerName \u0026quot; etcd1.local\u0026quot;) Oct 15 12:18:59 etcd1.local docker[832]: 2020-10-15 15:18:59.136121 I | embed: rejected connection from \u0026quot;192.168.255.42:56290\u0026quot; (error \u0026quot;tls: failed to verify client's certificate: x509: certificate has expired or is not yet valid\u0026quot;, ServerName \u0026quot; etcd1.local\u0026quot;) Aqui, diagnóstico trivial: os pseudo administradores deste ambiente deixaram os certificados do cliente calicoctl expirarem.\n Conclusão:\n Conectividade TLS pode ser um horror para depurar; Se você gera certificados para sua Intranet, implemente um controle adequado para saber quando estes estiverem próximos de expirar;  ",
    "ref": "/marcelo/blog/calicoctl-stuck/"
  },{
    "title": "Cuidado com terraform import",
    "date": "",
    "description": "Importando recursos que usam count e for_each",
    "body": "(Nota: eu tenho tentado produzir conteúdo de qualidade seguindo uma linha de raciocínio com começo, meio e fim, com valor histórico e altamente apreciável. Com isso, meu último post foi mês passado e eu tenho mais de 30 drafts a concluir. Portanto, vou tornar isso aqui um braindump de conteúdos aleatórios que eu julgar relevante. Foi mal!)\n Quem não adora Terraform?\nBasta escrever meia dúzia de arquivos que algum programa magicamente interpreta tudo e cria coisas mágicas para você. Fantástico! O único problema é aprender a usar direito.\nA Hashicorp, até alguns anos atrás, fazia documentações tão crípticas e ilegíveis que eventualmente perceberam que era necessário um esforço maior para \u0026ldquo;mentes menores\u0026rdquo; compreenderem seus softwares. Com isso, investiram somas substanciais de dinheiro fazendo sites como o Learn Hashicorp ou mesmo workshops gratuitos interativos com Instruqt. Eles realmente têm feito um bom trabalho nessa seara.\nEm muitos aspectos, o Terraform ainda é deficiente.\nÀs vezes é pura frescura: vide o terraform-provider-kubernetes, que simplesmente se recusava a implementar APIs betas por anos, tornando-o praticamente inútil. E mesmo quando deployments chegaram à GA, demorou quase um ano para implementar este elemento que é considerado como pedra fundamental para qualquer aplicação Kubernetes.\nMas muitas vezes nem é culpa da Hashicorp: a integração com VMware, mesmo com as últimas funcionalidades implementadas pela última versão, é, literalmente, um horror. E o problema aqui são as severas limitações funcionais das APIs públicas disponibilizadas pela própria VMWare.\nMas chega de \u0026lsquo;rant\u0026rsquo;: vamos a conteúdo!\nTerraform import Todo mundo sabe que o Terraform precisa de total controle sobre o que cria; o que existe tem que ser criado por ele, e isso é inegociável.\nQuer dizer, nem tanto; eles dão uma colher de chá para você tentar adaptar uma infraestrutura já existente à sua automação: o comando terraform import.\nSuponha que, agora, você usa Terraform e conseguiu criar 2 máquinas usando seu novíssimo programa! Como você faz para \u0026lsquo;incorporar\u0026rsquo; as 2000 máqunas que já existem na sua infraestrutura? Executando 2000 vezes (no mínimo) o comando \u0026lsquo;import\u0026rsquo; especificando os \u0026lsquo;terraform resources\u0026rsquo; que as máquinas representam, bem como um identificador alienígena que varia completamente de maneira praticamente imprevisível de acordo com o tipo de \u0026lsquo;provider\u0026rsquo;! (Tudo bem, é melhor que toda infraestrutura tem que ser criado por ele e isso ser inegociável).\nNosso problema: cluster Kubernetes com 10 máquinas virtuais usando VMWare Vsphere. Precisamos adicionar mais 4. Obviamente não usamos Terraform no passado. Como fazer?\nComando da documentação oficial:\nterraform import vsphere_virtual_machine.vm /dc1/vm/srv1 O colega com pouca experiência que está trilhando os tortuosos caminhos dos programas Terraform criados por mim (que consigo ser ainda mais críptico que a própria Hashicorp) falou:\n \u0026ldquo;Ufa! Ainda bem que é só isso, certo?\u0026rdquo;\n Claro que não!\n Se o seu programa (tipo, literalmente, no diretório corrente) criar \u0026lsquo;resources\u0026rsquo; do tipo \u0026lsquo;vsphere_virtual_machine\u0026rsquo; que sejam escalares (i.e., não usem \u0026lsquo;for_each\u0026rsquo; ou \u0026lsquo;count'), o primeiro parãmetro do comando está correto. Caso contrário, está errado! /dc1/vm/srv1 é obviamente um placeholder; você precisa descobrir o \u0026lsquo;caminho VMware\u0026rsquo; das suas máquinas virtuais, e, se você não entende de VMWare, pode ter alguma dificuldade com a nomenclatura.  A segunda parte é fácil: basta compor o nome do datacenter com a string arbitrária vm com as \u0026lsquo;pastas\u0026rsquo; criadas no Vcenter e, por fim, o nome dos servidores:\n \u0026ldquo;/bsa/vm/infra/10069/kubernetes df1/hosts/master1\u0026rdquo; \u0026ldquo;/bsa/vm/infra/10069/kubernetes df1/hosts/master2\u0026rdquo; \u0026ldquo;/bsa/vm/infra/10069/kubernetes df1/hosts/worker1\u0026rdquo; \u0026ldquo;/bsa/vm/infra/10069/kubernetes df1/hosts/worker2\u0026rdquo;  Tranquilo. (Espero que tenha acesso à API do VMWare para descobrir isso!)\nA primeira parte, por outro lado, irá variar radicalmente de acordo com o Terraform que você está usando.\nEm nosso caso, por exemplo, eu criei um módulo terraform que encapsula a criação dos recursos VMWare VSphere necessários. Esse encapsulamento \u0026lsquo;vaza\u0026rsquo; por uma série de razões, mas me permite passar as máquinas como uma variável map para o módulo mais ou menos da seguinte forma:\nvms = { \u0026quot;master1\u0026quot; = { \u0026quot;profile\u0026quot; = \u0026quot;k8s_worker\u0026quot; \u0026quot;hostname\u0026quot; = \u0026quot;master1\u0026quot; \u0026quot;network\u0026quot; = { \u0026quot;rede1\u0026quot; = \u0026quot;192.168.0.1\u0026quot;, \u0026quot;rede2\u0026quot; = \u0026quot;10.0.0.1\u0026quot; } } \u0026quot;worker1\u0026quot; = { \u0026quot;profile\u0026quot; = \u0026quot;k8s_worker\u0026quot; \u0026quot;hostname\u0026quot; = \u0026quot;worker1\u0026quot; \u0026quot;network\u0026quot; = { \u0026quot;rede1\u0026quot; = \u0026quot;192.168.0.101\u0026quot;, \u0026quot;rede2\u0026quot; = \u0026quot;10.0.0.101\u0026quot;, \u0026quot;rede3\u0026quot; = \u0026quot;172.16.0.1\u0026quot;, } } \u0026quot;ingress1\u0026quot; = { \u0026quot;profile\u0026quot; = \u0026quot;k8s_ingress\u0026quot; \u0026quot;hostname\u0026quot; = \u0026quot;ingress1\u0026quot; \u0026quot;network\u0026quot; = { \u0026quot;rede1\u0026quot; = \u0026quot;192.168.0.201\u0026quot;, \u0026quot;rede2\u0026quot; = \u0026quot;10.0.0.201\u0026quot;, \u0026quot;rede4\u0026quot; = \u0026quot;200.160.2.1\u0026quot;, } }  (E aqui, um conselho: evite ao máximo criar maps de maps de maps ou maps de arrays de maps como eu gosto de fazer. É uma ideia idiota, que deixa seu programa Terraform praticamente incompreensível para seres humanos. Eu adoro fazer assim por razões, mas definitivamente não recomendo).\n Enfim, a criação dos meus resources estão encapsulados no módulo.\nEntão, o comando \u0026lsquo;terraform import\u0026rsquo; deve, necessariamente, fazer referência ao resource dentro do módulo:\n module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm  E aqui está a pegadinha: aqui está a definição do meu resource vm:\nresource \u0026quot;vsphere_virtual_machine\u0026quot; \u0026quot;vm\u0026quot; { for_each = local.vms name = each.value.hostname ... Portanto, o meu \u0026lsquo;resource\u0026rsquo; vm não é um tipo simples (ou escalar, como eu chamo), e sim um tipo complexo (por causa do uso do iterador for_each) do tipo map.\nO comando correto para importar máquinas já existentes, portanto, é assim:\n# terraform import \u0026#39;module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026#34;master2\u0026#34;]\u0026#39; \u0026#39;/bsa/vm/infra/10069/kubernetes df1/hosts/master2\u0026#39; # terraform import \u0026#39;module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026#34;worker2\u0026#34;]\u0026#39; \u0026#39;/bsa/vm/infra/10069/kubernetes df1/hosts/worker2\u0026#39; ... Tá, mas e daí? E daí que: o que pode acontecer se eu executar o comando errado?\n# terraform import module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm \u0026#39;/bsa/vm/infra/10069/kubernetes df1/hosts/worker2\u0026#39; Acima, erramos o comando, e importamos \u0026lsquo;worker2\u0026rsquo;, que deveria ser membro de um map, para vsphere_virtual_machine.vm do módulo diretamente.\nO que se espera normalmente? Um erro de execução, certo?\nVai dar erro, mas não no import. O import irá executar de maneira bem sucedida. Mas, depois disso, provavelmente tudo relacionado a este programa falhará.\nApós a execução do comando errado, fui agraciado com a seguinte mensagem:\n# terraform apply ... module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026quot;ingress3\u0026quot;]: Refreshing state... [id=421ab407-1bc3-dceb-6906-72fe3fad4c0e] Error: Unsupported attribute on .terraform/modules/vmware-cluster-k8s-raw/locals.tf line 104, in locals: 104: vms_result = { for key, value in vsphere_virtual_machine.vm : key =\u0026gt; { for network in value.network_interface : network.network_id =\u0026gt; network.mac_address } } O que esse erro quer dizer? Quer dizer que aquela sequência mágica de \u0026lsquo;terraform maps comprehension\u0026rsquo; falhou. Por quê? Putz, vai adivinhar.\nMeu colega aplicou um \u0026ldquo;Senhor, eu desisto, Senhor!\u0026quot;:\n  E isso, obviamente, faz qualquer sênior SRE muito feliz!\n Como fui eu que pari a besta, voltei para entender.\nObservei o seguinte descrevendo o arquivo de estados:\n# terraform state list module.vmware-cluster-k8s-raw.data.vsphere_compute_cluster.compute_cluster module.vmware-cluster-k8s-raw.data.vsphere_datacenter.cluster_datacenter module.vmware-cluster-k8s-raw.data.vsphere_datastore_cluster.cluster_datastore module.vmware-cluster-k8s-raw.data.vsphere_network.networks[\u0026quot;cluster\u0026quot;] module.vmware-cluster-k8s-raw.data.vsphere_network.networks[\u0026quot;ingress\u0026quot;] module.vmware-cluster-k8s-raw.data.vsphere_network.networks[\u0026quot;management\u0026quot;] module.vmware-cluster-k8s-raw.data.vsphere_network.networks[\u0026quot;storage\u0026quot;] module.vmware-cluster-k8s-raw.vsphere_folder.folder module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026quot;worker1\u0026quot;] module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026quot;worker2\u0026quot;] Algo estranho aqui: temos dois tipos de ocorrências para o objeto que representa o map vm:\n module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm: um \u0026lsquo;escalar\u0026rsquo;! module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[]: este sim com todas as ocorrências adequadas!  O comando errado de import criou uma entrada errada no \u0026lsquo;state file\u0026rsquo; que bagunçou o funcionamento da lógica do módulo.\nNossa situação foi um pouco pior, porque o comando gerou a máquina em um lugar no VMWare que ela não deveria ser criada por razões desconhecidas (novamente, em vez de dar erro!), e que não tínhamos permissão para remover também por razões desconhecidas.\nA solução para este caso foi a remoção da máquina problemática do \u0026lsquo;map\u0026rsquo; de objetos criados; após um apply bem sucedido com a remoção da máquina problemática, o Terraform foi capaz de perceber o desvio no arquivo de estado e corrigiu sozinho, removendo a entrada inválida e me poupando (desta vez!) a experiência aterrorizante de manipular o estado com \u0026lsquo;terraform state\u0026rsquo;, então, se era para isos que você veio aqui, desculpe frustrar sua expectativa!\n Para aprender com esta experiência:\n Criar módulos Terraform para tudo parece uma ideia genial, mas quanto mais você encapsula achando que está reusando código, mais estará duplicando entradas de variáveis e se distanciando das validações básicas do programa; Evite usar de maneira leviana maps, arrays, counts e for_eachs a menos que você esteja disposto a ir até o fim por suas escolhas - ou trabalhe com alguém insano que use em todo canto; aí, meu amigo, meus pêsames; Não execute \u0026lsquo;imports\u0026rsquo; até ter certeza absoluta do que está fazendo.  ",
    "ref": "/marcelo/blog/terraform-import-watch-out/"
  },{
    "title": "Por que Docker?",
    "date": "",
    "description": "Docker e contêiners - razões diretas e pragmáticas perdidas em um texto bem menos direto e pragmático",
    "body": "Em conversas - principalmente com pessoas dos dois extremos do espectro de idade - sobre nosso glorioso mercado de TI vez por outra surge este questionamento: por que usar Docker?\nEu tendo a reclamação, em especial de estudantes e do pessoal novo: em casa, sem um arsenal de ferramentas de CI/CD implantadas por terceiros, parece uma incrível bobagem e perda de tempo. Dockerfile, rede virtual, putz, pra que complicar, se eu posso rodar tudo na minha máquina sem problemas?\n(Já o pessoal antigo, é preguiça e má vontade: vá aprender a usar contêineres e saia da zona de conforto imediatamente!)\nPor que Docker? Essa resposta é trivial:\nDocker é simples; Docker é fácil de entender; Docker \u0026ldquo;faz tudo\u0026rdquo;.\nDocker abstrai a infinidade de detalhes necessários para a execução de contêineres no Linux com um cobertor quente, bonito e agradável que permite ao seu usuário ser produtivo com um mínimo de overhead no aprendizado de uma tecnologia \u0026ldquo;colateral\u0026rdquo;.\nE mais: o \u0026ldquo;engine Docker\u0026rdquo; vai muito além: o software veio cuidar de praticamente todas as etapas do ciclo de vida de um contêiner, desde sua criação até viabilizar que as imagens cheguem onde desejamos - seja no host, seja no Docker Registry, mantido pelo Docker! -, passando pela execução e manutenção dos containers e imagens no servidor.\nExiste uma pergunta muito mais difícil, que inclusive muitos administradores de sistemas que usam contêineres todos os dias nos últimos anos não sabe responder:\nSe você não usar Docker, vai usar o quê?\nAté mesmo pesquisar essa resposta é difícil1; muitos dos conteúdos disponíveis sobre o assunto estão ou extremamente defasados ou simplesmente errados em suas comparações. A resposta a essa pergunta não atende às necessidades da maioria dos profissionais de TI, logo é irrelevante (você tem interesse na resposta? Se sim, me deixe saber!).\nEnquanto a \u0026ldquo;concorrência\u0026rdquo; não trabalhar isso, Docker será o líder incontestável em uso.\nPor que containers? Falando a verdade, a pergunta correta não é \u0026ldquo;por que usar Docker\u0026rdquo;, e sim, por que usar contêineres. Esta sim, diferentemente da primeira, é uma pergunta sem resposta direta e imediata.\nEu poderia copiar definições, justificativas e citar mil livros ou páginas importantes explicando o porquê da relevância da tecnologia de conteinerização sob as diferentes óticas2. Mas, em verdade, o que falta para muitos é entender que problemas essa tecnologia veio resolver, e que pode ser resumido em uma sentença:\nContêineres resolvem o problema de \u0026ldquo;na minha máquina funciona!\u0026rdquo;.\nHistoricamente, sempre coube aos profissionais de infraestrutura o processo de implantação, manutenção e resolução de problemas em ambientes que executam programas elaborados por terceiros. E um desafio tradicional dessa época era a divergência entre os ambientes em que os softwares eram desenvolvidos, homologados e, por fim, postos em produção.\nNão era incomum ajustes nos ambientes serem feitos \u0026ldquo;em tempo de homologação\u0026rdquo; para corrigir problemas e se perderem na \u0026ldquo;não documentação\u0026rdquo; do sistema, que era passada para as áreas de operações implantarem (e, consequentemente, falharem). Também não era incomum o uso de plataformas radicalmente diferentes e incompatíveis entre si, gerando conflitos homéricos sobre o uso de versão de um sistema operacional não internalizado pela empresa, ou a necessidade de atualização de um serviço para uma tecnologia de pouco domínio por parte dos administradores de serviços. Mesmo para tecnologias que naturalmente são pensadas para \u0026ldquo;rodar em todo lugar\u0026rdquo;, como Java, encontra problemas com versões e parametrizações específicas que podem se perder na transição entre ambientes.\nContêineres, por definição, resolvem o problema de \u0026ldquo;reproducibilidade de resultados\u0026rdquo;, ou \u0026ldquo;portabilidade\u0026rdquo; das aplicações: uma imagem Docker é gerada para a sua aplicação, com todos os arquivos (como elementos do sistema operacional, bibliotecas e utilitários) e configurações necessários para sua execução. Uma consequência comum da adoção deste modelo é a obrigatoriedade de adoção de um mínimo de boas práticas, como por exemplo a parametrização para que a aplicação receba configurações que variam de ambiente para ambiente.\nEste problema já foi amplamente abordado no passado como virtualização de servidores e tecnologias de orquestração e gestão de configuração; nenhuma delas, entretanto, resolveu com a elegância e simplicidade de simplesmente \u0026ldquo;socar\u0026rdquo; tudo que é necessário em uma unidade parametrizável e orquestrável.\nA partir desta entidade Contêiner3 que trivializa a implantação (\u0026quot;deploy\u0026quot;) de um software, está aberto agora o caminho para a construção de novas soluções que resolvam outros problemas do processo de desenvolvimento de software e otimizem ainda mais o processo de entrega de resultados por parte das equipes.\n 1 Os melhores links sequer apareciam na página principal do Google:\n A Comprehensive Container Runtime Comparison Aquasec: Docker Alternatives  2 \u0026ldquo;Ópticas\u0026rdquo; é muito feio.\n3 Observe que, no Linux, \u0026ldquo;Contêiner\u0026rdquo; não é uma primitiva do sistema operacional, e sim um conjunto destas. Do ponto de vista prático, acaba sendo uma.\n",
    "ref": "/marcelo/blog/why-docker/"
  },{
    "title": "Sobre",
    "date": "",
    "description": "",
    "body": "O fundador da DevSREs Network Initiative, Marcelo Andrade, é entusiasta do universo Linux e software livre desde a primeira instalação do saudoso Conectiva Parolin em 1997.\nAo longo dos anos, transitou profissionalmente entre os mais diversos nomes comumente atribuídos aos profissionais de TI que não trabalham diretamente com desenvolvimento de software: sysop, sysadmin, administrador de redes, analista de redes, analista de infraestrutura, analista de suporte, engenheiro de soluções\u0026hellip; Até se auto instituir o título de Site Reliability Engineer, ou engenheiro de confiabilidade (de sites?) em português.\nA realidade é que, em todas elas, o foco é sempre o mesmo: manter ambientes on-line, garantindo performance, escalabilidade e confiabilidade aos sistemas da empresa. O que muda, com os anos, é abordagem e o conjunto de ferramentas a ser usado.\nSeu principal nicho de atuação, no momento, é a manutenção de clusters Kubernetes em conjuto com outras tecnologias Cloud Native (ou nem tanto).\n",
    "ref": "/marcelo/about/"
  },{
    "title": "Kubernetes: por onde começar",
    "date": "",
    "description": "Como dar os primeiros passos nesta tecnologia",
    "body": "No último ano, aqui em Recife, tive a oportunidade de participar (e palestrar!) em alguns eventos. Em comum, havia o fato de muito de seus conteúdos - se não integralmente - estarem relacionados ao Kubernetes.\nNas duas oportunidade em que fiz apresentações, entrei em detalhes sobre:\n o desafio de usar o objeto Ingress - que, até a recém-lançada versão 1.19, ainda era beta, embora exista desde a versão 1.1; uma nova abordagem de como fazer a gerência de um cluster e a entrega contínua de aplicações: GitOps;  Embora eu sempre procure abordar o assunto de uma forma que não seja absolutamente incompreensível para quem não tenha qualquer tipo de contato com Kubernetes, em todos os eventos, me deparo com uma realidade: a de que existe muita gente interessada sobre Kubernetes, mas com bastante dificuldade de se introduzir a esta tecnologia.\nSempre me é feita uma pergunta: por onde começar? Como dou os primeiros passos? E eu nunca tenho uma resposta de qualidade a oferecer: eu já \u0026ldquo;aprendi tudo\u0026rdquo; lá atrás, em um passado remoto no qual inexistia bons tutorials ou cursos, e a documentação descrevia o Kube Controller como \u0026ldquo;the program that runs the control loops that controls the state of objects\u0026rdquo; (ou algo assim).\nFelizmente, os tempos mudaram, e é ampla a documentação de cada mínimo aspecto relacionado ao Kubernetes, com bastante oferta de cursos (pagos e gratuitos) sobre o assunto.\nSem mais delongas, por onde começar quando quer se aprender sobre Kubernetes?\nKubeAcademy  https://kube.academy/  A VMware disponibilizou um excelente material (e, vale ressaltar completamente gratuito) sobre containers e Kubernetes. Os conteúdos são apresentados de maneira fácil de digerir, e alguns dos cursos inclusive contam com laboratórios práticos interativos com Katacoda, o que, na minha opinião, torna este conteúdo possivelmente o melhor disponível para uma apresentação inicial.\nA desvantagem para quem não domina o inglês é o uso de player próprio e a inexistência de legendas, embora seja possível regular a velocidade do player para melhorar a compreensão para aqueles não tão fluentes.\nKatacoda / instruqt  https://www.katacoda.com https://play.instruqt.com/public  O Katacoda, adquirido recentemente pela tradicional editora de livros técnicos O\u0026rsquo;Reilly, por muito tempo, serviu como laboratório prático para quem não tem recursos para subir um cluster Kubernetes para prática pessoal.\nOs cursos dno Katacoda são interativos e executados diretamente no navegador, o que torna o processo simples e dinâmico. Antes de produção restrita, agora o Katacoda permite que qualquer um disposto a aprender a usar a plataforma possa criar seus próprios cursos com ela - o que\nO ponto negativo é que a maneira de abordar o conteúdo teórico fica em segundo plano. E, acredite, muito do trabalhar bem com Kubernetes adequadamente vem de entender a maneira e o porquê optou-se por resolver os problemas desta forma.\nInstruqt é bastante semelhante em abordagem ao Katacoda; mas, no geral, a interface apresenta mais problemas e os cursos de Kubernetes em qualidade inferior.\nKubernetes.io A documentação do projeto vem crescendo em volume e qualidade a olhos vistos a cada versão que passa. A versão 1.18 introduziu a oferta em outros idiomas que não o inglês, o que nem imaginava que fosse acontecer algum dia.\nO destaque da documentação fica por parte dos conceitos. Muitos dos textos crípticos do começo foram reescritos de maneira compreensível, e agora as ideias são, sim, plenamente acessíveis ao \u0026ldquo;grande público\u0026rdquo;.\nVale a pena mencionar que a documentação conta com duas seções extremamente valiosas para quem está aprendendo e pensa em tirar a certificação CKA/CKAD: a seção de Tasks e a de Tutoriais.\nEmbora o português esteja lá, a maior parte das páginas que acessei ainda está disponível apenas em inglês.\nEDX - Introduction to Kubernetes https://www.edx.org/course/introduction-to-kubernetes\nLembro que, à medida que novos membros chegavam à equipe, a recomendação padrão era a fazer este curso do EDX elaborado pela Linux Foundation - basicamente porque era a única opção.\nÉ possível ler grande parte dos conteúdos disponíveis sem custo adicional, mas vários recursos são oferecidos apenas aos pagantes.\nDeixo aqui esta entrada pelo valor histórico, mas acredito que a relevância deste curso hoje é limitada se comparado ao que já é oferecido pela documentação oficial do Kubernetes.\nMas e em português? Aqui, eu peço desculpas, mas vou ficar devendo.\nA qualidade dos materiais que tive a oportunidade de ler (inclusive em sites pagos) está entre o ruim e o sofrível. Então, não tenho uma boa recomendação a oferecer.\n Se você conhecer algum bom site com conteúdo de Kubernetes em português e quiser recomendar, entre em contato por qualquer uma das minhas redes sociais que eu terei um prazer enorme em incluí-los nesta lista!\n",
    "ref": "/marcelo/blog/kubernetes-where-to-start/"
  },{
    "title": "Contact",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/marcelo/contact/"
  }]
