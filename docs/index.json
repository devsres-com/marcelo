[{
    "title": "Calico, Linux e tabelas de rota",
    "date": "",
    "description": "Features não documentadas vão te surpreender quando você menos espera",
    "body": "Mudança no ambiente! Atualização de cada um dos 32 componentes do cluster Kubernetes. Tranquilo! O que pode dar errado?\nApós a estabilização do ambiente, começamos a atualização dos servidores que atuam como balanceadores de carga de entrada para o cluster - aqueles que executam o Haproxy Ingress Controller do dev hero João Morais.\nA métrica de erros começa a estourar do nada; e o melhor, em tese, não estamos recebendo nenhum erro - algo quebrou, e algo completamente desconhecido.\nE agora?\nPreâmbulo - rotas assimétricas Uma coisa que comumente faz falta em equipes de desenvolvimento de software ou mesmo de infra-estrutura é entendimento adequado de como redes funcionam (e de seus detalhes de implementação nos sistemas operacionais).\nNão é raro eu ter que explicar que o problema de conectividade em determinada situação é causado por rotas assimétricas e não bloqueio por firewall.\nNeste caso em particular, temos Pods de sistemas em execução no cluster Kubernetes acessando aplicações por meio de suas URLs públicas. Portanto:\n a comunicação chega aos balanceadores por suas interfaces públicas; os balanceadores também fazem parte do cluster Kubernetes, portanto seu caminho de retorno ocorrerá pelas interfaces privadas;  Até aí tudo bem, fácil de entender.\nO que só anos de experiência como sysadmin Linux traz é o conhecimento de que, por padrão, se a situação de cima ocorrer, uma máquina Linux irá necessariamente descartar o pacote sem qualquer tipo de informação. A Red Hat descreve isso aqui, mas vale a pena copiar porque isso é feito:\n Current recommended practice in RFC3704 is to enable strict mode to prevent IP spoofing from DDos attacks. If using asymmetric routing or other complicated routing, then loose mode is recommended.\n A solução \u0026lsquo;ruim\u0026rsquo; é modificar este comportamento do kernel.\nA solução \u0026lsquo;correta\u0026rsquo; é interferir no roteamento, forçando a resposta a sair por onde veio.\nPreâmbulo do preâmbulo - roteamento no Linux Em uma máquina regular sem \u0026lsquo;efeitos especiais\u0026rsquo;, se você executar o comando abaixo, provavelmente vai receber o seguinte resultado:\n# ip rule list 0: from all lookup local 32766: from all lookup main 32767: from all lookup default Aqui vale uma observação interessante: por \u0026lsquo;default\u0026rsquo;, temos três tabelas definidas nas regras: \u0026lsquo;local\u0026rsquo;, \u0026lsquo;main\u0026rsquo; e \u0026lsquo;default\u0026rsquo;.\nSe alguém te perguntar em inglês \u0026lsquo;which is the default routing table used\u0026rsquo;, qual tabela você chutaria que é a \u0026lsquo;default\u0026rsquo;?\nA resposta, é claro, a tabela main! Por que você escolheria \u0026lsquo;default\u0026rsquo;, ser ignóbil?\n# ip route show table main default via 172.31.48.1 dev eth0 172.31.48.0/20 dev eth0 proto kernel scope link src 172.31.56.202 Em algumas distribuições Linux, como o CoreOS/Flatcar, embora conste na tabela de \u0026lsquo;rules\u0026rsquo;, ela sequer é criada:\n# sudo ip route show table default Error: ipv4: FIB table does not exist. Dump terminated Uma nota relevante: esses nomes só são usados porque são especificados em algum lugar; tabelas de roteamento são representadas, no Kernel, por números. Este lugar é o arquivo /etc/iproute2/rt_tables:\n# cat /etc/iproute2/rt_tables # # reserved values # 255 local 254 main 253 default 0 unspec Aqui você pode prestigiar um pouco da história viva do Linux: em algum ponto no passado distante, a última tabela suportada pelo kernel era necessariamente a 255, usada para \u0026lsquo;local\u0026rsquo;. Mas vivemos tempos modernos, e você pode escolher qualquer número agora que não seja maior que 2147483647 (2^31-1).\nDe volta às rotas assimétricas Para permitir o reencaminhamento dos pacotes para sua interface de origem, eu devo criar uma tabela e colocar uma prioridade superior à tabela padrão do sistema (que é \u0026lsquo;main') para evitar o descarte.\nSe o número 0, 253, 254 e 255 são reservados, e eu tenho até o número 2147483647 para minha tabela, que número eu escolheria?\nTabela 1, é claro. Quem precisa de tantos números?\n# cat /etc/systemd/system/routingpolicy.service [Unit] Description=Configure routes After=network-online.target Requires=network-online.target [Service] Type=oneshot RemainAfterExit=true ExecStart=-/usr/bin/ip route add default via 10.0.0.1 dev ens256 tab 1 ExecStart=-/usr/bin/ip route add 10.0.0.1/8 dev ens256 tab 1 ExecStart=-/usr/bin/ip rule add from 10.0.0.69/32 tab 1 priority 100 A configuração acima resolve meu problema de rota assimétrica, alterando as regras da seguinte maneira:\n# ip rule list 0: from all lookup local 100: from 10.0.0.69 lookup 1 32766: from all lookup main 32767: from all lookup default O que chegar pela Interface com IP 10.0.0.69, ele segue a tabela de rotas 1.\nEsta é a tabela de rota 1, criada pela configuração da unit de systemd acima:\n# ip route show table 1 default via 10.0.0.1 dev ens256 10.0.0.0/8 dev ens256 scope link Diagnóstico Não foi um \u0026lsquo;senhor\u0026rsquo; processo de diagnóstico; sabia-se que algum dos componentes estava interferindo. A lógica aponta para o Calico, já que ele é responsável por amplas modificações nas configurações de rede da máquina.\nA parte triste é que isso não está listado em nenhum release notes. Então não apenas é bem difícil de diagnosticar o que aconteceu, mas também em se preparar para o que iria acontecer.\nA descrição do problema está na página de configuração do felix, a partir da versão 3.14:\n RouteTableRange (FELIX_ROUTETABLERANGE): Calico programs additional Linux route tables for various purposes. RouteTableRange specifies the indices of the route tables that Calico should use. [Default: 1-250]\n Portanto, o Calico irá limpar qualquer conteúdo associado a tabelas existentes da 1 até 250. Como usamos a tabela 1, tivemos problema.\nE ele não limpa apenas uma vez as instruções das tabelas; ele constantemente ajusta, ainda que não vá fazer nada com elas.\n Tá vendo? Deveríamos ter escolhido a tabela 2147483647.\n",
    "ref": "/marcelo/blog/calico-and-route-tables/"
  },{
    "title": "Kubernetes on-premisses - parte 1",
    "date": "",
    "description": "Relato histórico das decisões tomadas na implantação de um cluster Kubernetes on-premises.",
    "body": "Recentemente, participei do Kubicast, em que pude desabafar um pouco com o João Brito da Getup sobre como é criar e um cluster Kubernetes em um ambiente tecnologicamente inóspito e oposto ao universo super estável, escalável e redundante dos cloud providers.\nRecebi algumas mensagens de alguns poucos valentes que precisam trilhar esse caminho. Mas, antes, vou ressaltar uma coisa importante: neste primeiro momento, eu vou abordar a história do que foi feito e compartilhar a experiência. Provavelmente existem alternativas melhores para um começo menos pedregoso; eu vou tentar listá-las à medida que for abordando os assuntos, mas não é um requisito no momento.\nPonto de partida: infraestrutura Não queria perder tempo discutindo infraestrutura, mas, querendo ou não, entender o tamanho das suas limitações é importante para planejar o que se quer fazer.\nO que estava disponível para conduzir os meus trabalhos eram:\n Máquinas físicas (\u0026ldquo;bare metal\u0026rdquo;) para atuar como nós para as cargas (\u0026ldquo;workers\u0026rdquo;); VMWare VSphere com um datacenter dedicado e cluster datastores pré configurados para a criação de máquinas virtuais para todo o resto.  Não posso compartilhar algo que não fiz; portanto, não vou indicar soluções de virtualização ou orquestração de máquinas virtuais que você pode implantar em máquinas físicas. Mas isso é apenas por pura falta de experiência.\nAlém do que você \u0026ldquo;tem\u0026rdquo;, você precisa avaliar o que você não tem:\nEu, a princípio, não tinha disponível:\n Balanceadores de carga; Acesso ao firewall; Acesso ao roteador; Acesso aos switches para configuração dinâmica das portas (para coisas tipo VLAN);  Dependendo do tamanho da empresa e da hostilidade das equipes diante das tecnologias que você está implantando, você pode ter diversos problemas de integração do seu cluster Kubernetes ao resto da empresa.\nO que eu julgo mais importante nesta etapa é usar o máximo da automação pré-existente. Infelizmente, para mim, não havia nenhuma, e quantidades extenuantes de procedimentos manuais foram executados para garantir atendimento de prazos, o que cobra a conta mais tarde.\nHoje usamos uma combinação de Terraform com instalação via PXE usando Matchbox de maneira muito semelhante ao disposto pelo projeto Typhoon\nDefinições do cluster Existem algumas perguntas que você deve se fazer antes de construir o cluster. Podemos englobar todas elas em uma discussão sobre \u0026ldquo;topologia\u0026rdquo;, embora seja muito mais que isso.\nVou elencar uma lista (não exaustiva) de algumas das decisões mais importantes que pretendo trabalhar nesta série de posts:\n Topologia do Controlplane; Como fazer o deploy do cluster; Como fazer o deploy dos \u0026lsquo;penduricalhos\u0026rsquo; (outros utilitários a serem usados no cluster). Sistema operacional (sim, por incrível que pareça!); Modelo de redes do cluster (i.e., qual o CNI e que recursos avançados deste usar); Modelo de tráfego de entrada - um dos maiores desafios para os clusters on-premises;  Topologia do cluster - Controlplane: Esta definição prévia é importante porque o \u0026lsquo;salto\u0026rsquo; de um modelo mono-master para outro multi-master acaba sendo mais trabalhoso que simplesmente implantar um multi-master de primeira.\nPara fazer testes breves e criar familiaridade com a tecnologia, vale a pena testar o modelo mais simples. Mas muito cuidado ao implantar \u0026lsquo;pilotos\u0026rsquo; e \u0026lsquo;provas de conceito\u0026rsquo; simplórias demais, especialmente se sua organização tem a mania de transformar estes ambientes em produção por decreto.\nO que se encontra de mais comum em topologia de Controlplane é uma organização \u0026lsquo;stacked\u0026rsquo; em que são criados 3 máquinas que combinam Master com ETCDs e uma solução de loadbalancer externa.\nOptamos por:\n 2 VMs para loadbalancers dedicados para os Kubernetes Masters; 2 VMs para Kubernetes masters; 5 VMs de ETCDs dedicados para atender ao cluster Kubernetes; 3 VMs de ETCDs adicionais dedicados para o Calico;  Imagino que não seja necessário mencionar, mas um \u0026lsquo;Kubernetes Master\u0026rsquo; é a máquina que irá executar (no mínimo) os três principais componentes do cluster Kubernetes: o Apiserver (em que múltiplos podem estar ativos no mesmo momento) e os par Scheduler/Controller, os quais apenas um por cluster atua como ativo, funcionando a base de leases para identificar os líderes e assim garantir o failover.\nMaster loadbalancers Em um ambiente multi-master, você precisa de uma maneira de distribuir a carga entre eles que não seja DNS round robin. Algumas empresas contam com hardware dedicado para esta finalidade, que faz healthcheck e coisas elegantes do tipo.\nPor estarem fora do meu \u0026lsquo;alcance\u0026rsquo;, optamos por subir duas VMs com Nginx atuando como proxy para os Apiservers, usando Keepalived para implementar VRRP para o IP virtual associado ao nome do host.\nNota: Interessante notar que projetos como o Kubespray fazem o deploy automático de um Daemonset com Nginx que atua como um proxy interno da comunicação entre os nós e o Apiserver. Se o acesso aos Apiservers por produtos externos ao cluster não for um requisito, de repente nem seria necessário subir essas máquinas. Não é o nosso caso, mas pode ser o seu! Ainda que não use Kubespray, pode usar a ideia como inspiração.\nMasters não \u0026lsquo;stacked\u0026rsquo; nos ETCDs A documentação do Kubernetes descreve, em sua sessão de High Availability, duas topologias básicas:\n Stacked control plane + ETCD nodes; External ETCD nodes  A esmagadora maioria das pessoas opta por uma topologia \u0026lsquo;stacked\u0026rsquo; em que tanto o Kubernetes Master quanto o ETCD são instalados na mesma máquina. É uma topologia óbvia e conveniente. Optamos por não o fazer, pois preferimos usar um cluster ETCD de 5 membros em vez de 3.\nPor que não subir 5 masters, então?\nPorque as máquinas ETCD praticamente não demandam recursos. Um nó ETCD pode rodar, por exemplo, com 4GB de RAM e 2 processadores. Já um Kubernetes Master demanda uma quantidade substancialmente maior de recursos. Como os recursos virtuais eram relativamente exíguos à época, optamos por apenas duas máquinas como Master Kubernetes enquanto criávamos mais máquinas ETCD com \u0026ldquo;o que sobrou\u0026rdquo;.\nObviamente, você deve dimensionar as máquinas \u0026lsquo;de acordo com a necessidade\u0026rsquo;, mas qual um bom ponto de partida? Existia uma tabela interessante na documentação do Kubernetes que fazia uma correlação entre os tipos de instância dos Cloud Providers que você deveria usar de acordo com o tamanho do seu cluster. Ela foi removida na versão 1.19, mas ainda está disponível para as versões anteriores (ou se você selecionar o idioma japonês!), e eu acho legal para ter uma ideia:\n On GCE/Google Kubernetes Engine, and AWS, kube-up automatically configures the proper VM size for your master depending on the number of nodes in your cluster. On other providers, you will need to configure it manually. For reference, the sizes we use on GCE are\n  1-5 nodes: n1-standard-1 6-10 nodes: n1-standard-2 11-100 nodes: n1-standard-4 101-250 nodes: n1-standard-8 251-500 nodes: n1-standard-16 more than 500 nodes: n1-standard-32   And the sizes we use on AWS are\n  1-5 nodes: m3.medium 6-10 nodes: m3.large 11-100 nodes: m3.xlarge 101-250 nodes: m3.2xlarge 251-500 nodes: c4.4xlarge more than 500 nodes: c4.8xlarge  É importante levar em consideração o que ele chama de um nó, que seria uma máquina com não mais que 100 pods - sua realidade pode ser drasticamente diferente do esperado.\nETCDs do Kubernetes O tamanho mínimo aceitável para executar ETCD que ofereça alguma tolerância a falhas corresponde a três nós. Porque cinco então?\nUm cluster de cinco membros permitirá a falha concorrente de dois membros. Em Cloud Providers como a AWS, você provavelmente pode sobreviver com um cluster de três máquinas espalhadas em três Availability Zones diferentes; dificilmente uma Region irá experimentar uma falha em duas AZs ao mesmo tempo.\nEm nosso ambiente, devido a problemas de performance ao usar discos virtuais com o cluster datastore oferecido via SAN, optamos por usar discos NVME locais do host VMWare em que a máquina ETCD está hospedada. Esta dependência do host faz com que valha a pena gastar recursos subindo duas máquinas a mais para essa finalidade, pois não há a flexibilidade de live migration ou outros recursos do tipo.\nEu pretendo abordar ETCD em mais detalhes em outro post, mas aqui vale a ressalva: o cluster ETCD é *mais importante que qualquer outra coisa no ambiente (talvez, exceto, do que você usa para armazenar os dados das aplicações).\nExecutar um cluster on-premise significa arcar com plena administração desta solução, que não é particularmente trivial: é necessário cuidado extremo com os backups (e seus testes), automação afiadíssima para reconstrução do cluster com velocidade em caso de tragédias e monitoração bastante exagerada para tentar identificar a proximidade dos possíveis gargalos de performance antes que sintomas se abatam sobre o cluster.\nETCDs do Calico O uso de ETCDs para o Calico hoje é opcional, já que o uso do próprio Kubernetes como datastore já é considerado maduro e estável o suficiente. Optar por Kubernetes Datastore normalmente demanda implantar um componente adicional do Calico (Typha) para desonerar os apiserver. Embora a recomendação seja para \u0026lsquo;clusters grandes\u0026rsquo;, melhor já ir se acostumando com ele do que ter que implementar emergencialmente porque a performance do cluster está degradada.\nUma vantagem de ter um cluster dedicado para o Calico é que, se você eventualmente quiser fazer um \u0026lsquo;offload\u0026rsquo; da carga do ETCD do Kubernetes removendo, por exemplo, os Event objects como recomendado aqui, as máquinas já estão prontas!\nUma desvantagem de usar ETCD como backend para o Calico é o fato de que todos os nós precisarão de comunicação estável com o cluster ETCD. Em algumas literaturas, uma recomendação comum de segurança é a de sugerir que as máquinas de ETCD estejam em uma rede isolada das demais máquinas, oferecendo acesso apenas ao Kubernetes Apiserver. Dependendo do tipo de rede que você precisa lidar e a quantidade de intermediários atuando entre os barramentos, isso dificilmente é uma opção neste caso.\n",
    "ref": "/marcelo/blog/kubernetes-on-prem-1/"
  },{
    "title": "Prometheus node exporter com TLS",
    "date": "",
    "description": "Restringindo acesso ao endpoint de métricas da sua máquina",
    "body": "Ah, Prometheus. O mundo \u0026ldquo;cloud-native\u0026rdquo; simplesmente adora.\nDesde sua recepção pela CNCF em 2016 como primeiro projeto - após o Kubernetes - a ser incubado, a adesão e admiração tem sido crescente, a ponto de você encontrar issues como esta em projetos aleatórios na Internet:\n jnovack commented on Apr 1, 2018 I\u0026rsquo;m pretty sure Prometheus-compatible metrics exposure is almost a requirement in 2018 for project adoption.\n (Ps: o projeto em questão ainda não exporta métricas)\nE isso antes mesmo de ser considerado um projeto \u0026ldquo;graduado\u0026rdquo;\nLogo, como somos modistas, vamos substituir todas as monitorações internas por Prometheus porque a gente pode.\n(Algum dia faço uma análise se isso é estúpido ou genial!)\nO primeiro passo é implantar o Prometheus Node Exporter para exportar métricas gerais do nó (i.e., substituir o tradicional cpu/memória/disco).\n O mínimo que você precisa saber sobre Prometheus para não passar vergonha  Você executa o programa na máquina; Ele abre um endpoint HTTP em uma porta (para o Node Exporter, o default é 9100); Os softwares que você instala e geram métricas são chamados de \u0026ldquo;exporters\u0026rdquo; (dã). Você precisa configurar o servidor para \u0026lsquo;ir buscar\u0026rsquo; as métricas; o nome jurídico disso em prometês é \u0026ldquo;scrape\u0026rdquo;.  O que torna o Prometheus uma ideia genial como solução de monitoração é que o foco não são os exporters, mas sim a instrumentação do código das aplicações usando as bibliotecas do projeto. Assim, sua aplicação pode oferecer nativamente um endpoint para métricas sem precisar de um exporter.\nPrometheus é uma solução de monitoração que vem com sotaque \u0026ldquo;Dev\u0026rdquo;, não \u0026ldquo;Ops\u0026rdquo;.\nPrometheus e TLS O acesso aos endpoints de métricas dos exporters corriqueiramente é feito usando protocolo HTTP simples nas portas expostas, o que não incomoda a maioria das pessoas.\nObviamente isso não é o mais recomendado; você não quer suas métricas expostas por aí, ou mesmo uma saraivada de portas abertas no seu host exposto à Internet, por exemplo.\nAinda há um agravante: determinados exporters (o próprio Node Exporter é um desses), dependendo do tipo de carga a que a máquina está submetida, podem, por si só, onerar excessivamente a máquina em caso de muitas consultas repetidas.\nNós já conseguimos simular um DoS em um host Kubernetes acessando continuamente o endpoint de métricas daquela máquina exposto via HTTP. Logo, deixar estes endpoints expostos não é uma alternativa.\nO problema aqui é: a maioria dos exporters não implementa a opção de controle de acesso ou mesmo HTTPS/TLS. Você precisa se virar.\nExistem várias alternativas para resolver este problema:\n A documentação oficial sugere você instalar e configurar um Nginx; Uma issue no projeto Prometheus Node Exporter sugere o projeto Ghostunnel; Para aplicações que executam em clusters Kubernetes, existe a alternativa do Kube RBAC Proxy.  O que usamos hoje é uma solução interna baseada em Nginx que lê um configmap e expõe diversos exporters em uma única porta, usando o truque de \u0026ldquo;path redirect\u0026rdquo; dos \u0026ldquo;scrapers\u0026quot;; os diversos exporters são configurados para bind em localhost apenas.\nPrometheus Node Exporter com suporte nativo TLS Porém, se você prestou atenção na issue em que é sugerido o Ghostunell, vai observar que ela faz menção à implementação nativa de um endpoint TLS HTTPS para o Prometheus Node Exporter!\nCalma, esta funcionalidade ainda está descrita como experimental, e a recomendação ainda é você manter seu proxy nos ambientes de produção, ok?\nDe jeito nenhum! Aqui é bleeding edge, p@r#!\u0026amp;!\nVamos implantar isso já!\nConfigurando o Prometheus Node Exporter TLS A documentação é tão extensa que eu vou copiar integralmente aqui:\nTLS endpoint ** EXPERIMENTAL ** The exporter supports TLS via a new web configuration file. ./node_exporter --web.config=web-config.yml See the https package for more details. Ok, eles criaram um README.md no diretório com o código.\nAs orientações aqui acompanham um modelo de arquivo de configuração (o tal web-config.yml) que é indicado no exemplo acima, bem como um arquivo com a configuração mínima necessária:\n# web-config.yml # Minimal TLS configuration example. Additionally, a certificate and a key file # are needed. tls_server_config: cert_file: server.crt key_file: server.key A configuração acima configura o endpoint TLS. Mas, obviamente, não faz qualquer tipo de restrição ao acesso - apenas protege as informações usando criptografia.\nO que, no nosso caso, potencializa o ataque de DoS com o consumo extra de CPU do TLS! Só isso não nos serve! Precisamos limitar o acesso a um conjunto de certificados digitais.\nObservando o arquivo de configuração de exemplo, temos o seguinte trecho que interessa:\n # Server policy for client authentication. Maps to ClientAuth Policies. # For more detail on clientAuth options: [ClientAuthType](https://golang.org/pkg/crypto/tls/#ClientAuthType) [ client_auth_type: \u0026lt;string\u0026gt; | default = \u0026quot;NoClientCert\u0026quot; ] Ok, é isso que estamos procurando: ClientAuth. A opção padrão, naturalmente, é NoClientCert, ou seja, não fazer qualquer tipo de exigência, como observamos no exemplo acima.\nQuais são os valores aceitos por este parâmetro?\nBem, era querer demais que estivesse nessa documentação, não é mesmo? Siga o link e olhe direto na biblioteca, seu preguiçoso!\nAo clicar no link descrito no modelo de arquivo de configuração, somos recebidos por uma página com a seguinte informação:\n// ClientAuthType declares the policy the server will follow for TLS Client Authentication. type ClientAuthType int const ( NoClientCert ClientAuthType = iota RequestClientCert RequireAnyClientCert VerifyClientCertIfGiven RequireAndVerifyClientCert ) Opa, estes devem ser os valores válidos para a configuração, certo?\nErrado!\n$ cat /etc/prometheus/web-config.yml tls_server_config: cert_file: /etc/ssl/private/tls.crt key_file: /etc/ssl/private/tls.key client_auth_type: \u0026quot;RequireAnyClientCert\u0026quot; client_ca_file: /etc/ssl/private/tls.ca $ systemctl restart prometheus-node-exporter $ journalctl -fu prometheus-node-exporter ... Oct 29 23:37:21host.intranet docker[59791]: level=error ts=2020-10-30T02:37:21.312Z caller=node_exporter.go:194 err=\u0026quot;Invalid ClientAuth: RequireAnyClientCert\u0026quot; Oct 29 23:37:21 host.intranet systemd[1]: prometheus-node-exporter.service: Main process exited, code=exited, status=1/FAILURE Aproveitando que estamos aqui, nem tente remover o o parâmetro client_auth_type e deixar o client_ca_file no arquivo de configuração, você vai receber este erro aqui:\nOct 29 23:40:08 host.intranet docker[61689]: level=error ts=2020-10-30T02:40:08.981Z caller=node_exporter.go:194 err=\u0026quot;Client CA's have been configured without a Client Auth Policy\u0026quot; O que nem é condenável. Por que você especificaria uma CA sem especificar um Client Auth? Seu idiota!\nEnfim, quais são os parâmetros?\nA melhor maneira de descobrir isso, obviamente, é olhando no código:\n https://github.com/prometheus/node_exporter/blob/master/https/tls_config.go#L145  \tswitch c.ClientAuth { case \u0026quot;RequestClientCert\u0026quot;: cfg.ClientAuth = tls.RequestClientCert case \u0026quot;RequireClientCert\u0026quot;: cfg.ClientAuth = tls.RequireAnyClientCert case \u0026quot;VerifyClientCertIfGiven\u0026quot;: cfg.ClientAuth = tls.VerifyClientCertIfGiven case \u0026quot;RequireAndVerifyClientCert\u0026quot;: cfg.ClientAuth = tls.RequireAndVerifyClientCert case \u0026quot;\u0026quot;, \u0026quot;NoClientCert\u0026quot;: cfg.ClientAuth = tls.NoClientCert default: return nil, errors.New(\u0026quot;Invalid ClientAuth: \u0026quot; + c.ClientAuth) } if c.ClientCAs != \u0026quot;\u0026quot; \u0026amp;\u0026amp; cfg.ClientAuth == tls.NoClientCert { return nil, errors.New(\u0026quot;Client CA's have been configured without a Client Auth Policy\u0026quot;) } Agora sim, sabemos exatamente quais são os parâmetros aceitos! E descobrimos que os caras do Prometheus Node Exporter queriam apenas zoar com a nossa cara:\n Para usar tls.RequestClientCert, use \u0026ldquo;RequestClientCert\u0026rdquo;; Para usar tls.VerifyClientCertIfGiven, configure \u0026ldquo;VerifyClientCertIfGiven\u0026rdquo;; Para usar tls.RequireAndVerifyClientCert, configure \u0026ldquo;RequireAndVerifyClientCert\u0026rdquo;;  Agora:\n Para usar tls.RequireAnyClientCert, use \u0026ldquo;RequireClientCert\u0026rdquo; - sacaram a fuleragem aqui?  Pffff!\nPassada esta etapa, a segunda pergunta: qual a diferença entre eles?\nÉ possível cavocar isso em lugares obscuros, mas eu vou sugerir lemos a incrível documentação escrita pelo projeto Traefik a respeito:\nThe clientAuth.clientAuthType option governs the behaviour as follows: * NoClientCert: disregards any client certificate. * RequestClientCert: asks for a certificate but proceeds anyway if none is provided. * RequireAnyClientCert: requires a certificate but does not verify if it is signed by a CA listed in clientAuth.caFiles. * VerifyClientCertIfGiven: if a certificate is provided, verifies if it is signed by a CA listed in clientAuth.caFiles. Otherwise proceeds without any certificate. * RequireAndVerifyClientCert: requires a certificate, which must be signed by a CA listed in clientAuth.caFiles. Basicamente:\n RequestClientCert: \u0026ldquo;exige\u0026rdquo; o certificado do cliente, mas se o cliente não enviar, tudo bem (?!); RequireAnyClientCert: \u0026ldquo;exige\u0026rdquo; o certificado, dá erro se nenhum for enviado, mas não verifica se ele é assinado pela CA especificada em client_ca_file! VerifyClientCertIfGiven: se o usuário enviar um cerfificado, ele é verificado contra a CA especificada. Se não, prossegue sem. (?!?!?!) RequireAndVerifyClientCert: exige E valida o certificado. Ufa!  Então, se o objetivo é limitar o acesso a usuários com certificados válidos assinados pela CA, apenas o último parâmetro é efetivamente útil.\nRestrição via subject (dn) do certificado A minha solução baseada em Nginx tornava possível a autorização do acesso aos endpoints das métricas apenas para o certificado com um determinado subject (no caso, o certificado configurado no servidor Prometheus responsável pelo scraping).\nPara aqueles curiosos de como fazer isso no Nginx, segue a dica:\n# trechos de um arquivo de configuração de nginx: http { map $ssl_client_s_dn $is_allowed { default no; \u0026quot;CN=prometheus-scraper\u0026quot; yes; } ... location / { ... if ($is_allowed = no) { add_header X-SSL-Client-S-DN $ssl_client_s_dn always; return 403; } ... Infelizmente isso não é possível de ser feito no Prometheus Node Exporter.\nSe alguém acha que a validação por certificado não é forte o suficiente, existe a alternativa de configurar um par usuário/senha para complementar a autenticação como descrito na própria documentação:\nbasic_auth_users: [ \u0026lt;string\u0026gt;: \u0026lt;secret\u0026gt; ... ] Brincadeira divertida!\n",
    "ref": "/marcelo/blog/prometheus-node-exporter-tls/"
  },{
    "title": "De repente, CKA",
    "date": "",
    "description": "4 anos de 'estudo' para uma certificação",
    "body": "Com alguns anos de atraso\u0026hellip;\nA história com Kubernetes e seus colegas Cloud Native vem de 2016 quando a área em que estava acabou e a equipe inteira seria \u0026ldquo;despejada\u0026rdquo; em uma área qualquer. Um grupo dissidente da empresa estava procurando almas perdidas e desenganadas para participar de um projeto esquisito, que, segundo eles, seria altamente diferenciado.\nNenhuma grande empresa de tecnologia jamais dedicaria esforço montando uma infraestrutura alternativa com um software de nome estranho que pouca gente do corpo gerencial sequer ouviu falar, ainda mais mantido por uma \u0026ldquo;skeleton crew\u0026rdquo; de poucos nomes desconihecidos. Que fornecedor vai dar o suporte?\nKubernetes estava na versão 1.4. Não existiam Ingress Controllers, a maioria dos tutoriais faziam menção a Replication Controllers e até mesmo o controle de acesso básico (RBAC) ainda era alpha.\nCom 16 anos de experiência com Linux, 13 desses em ambiente corporativos com muitas pessoas que não exatamente simpatizam com o SO ou com a filosofia de software livre, esse tipo de missão não me é nem um pouco desconfortável. É minha especialidade, na verdade. Mas uma especialidade do ponto de vista filosófico, e não da tecnologia.\nIsso porque eu estava preguiçosamente acomodado, completamente estagnado. Não acompanhava tendências gerais do mercado ou grandes atualizações. Não havia porque estudar Clouds públicas se minha empresa jamais poderia usá-las. Não havia porque tirar certificações - não estava procurando emprego, nem havia ganho interno em fazê-lo.\nFoi-me jogado no colo o desafio de implantar um conjunto de tecnologias altamente desafiadoras cujos nomes eu sequer havia ouvido falar no passado em quatro meses. Nem mesmo meu sistema operacional ficou intocado, sendo requisito o uso de uma distribuição estranha sem dpkg ou rpm completamente orientada a contêineres.\nEu poderia ter escolhido a estabilidade e seguido meu caminho por uma vida seguramente menos conturbada. Fui chamado para dar o meu melhor em outra posição na qual meu conhecimento já consolidado seria útil. Eu não precisaria perder várias madrugadas estudando um apinhado de coisas novas.\nFoi-me dada esta opção, e eu recusei.\nFaz precisamente 4 anos que o ambiente implantado entrou em produção. Hoje, este cluster executa de maneira impecável centenas de sistemas em diversas etapas, com maior ou menor grau de importância, severidade e volume de acesso.\nMas mais que o cluster, faz quatro anos que eu entrei \u0026ldquo;em produção\u0026rdquo;.\nEu sempre entreguei resultados acima da média. Mas a diferença entrre a qualidade do atual tipo de trabalho não tem qualquer tipo de comparação com o passado.\nPor fim, tornei-me um profissional imensamente superior. Sou referência no que faço, um dos poucos com visão sistêmica de todas as tecnologias envolvidas e que, agora sim, entende exatamente suas relações e o lugar certo de olhar quando há problemas.\nMais que isso, agora não estou mais \u0026ldquo;para trás\u0026rdquo;: acompanho as tendências, evoluo meu trabalho de acordo e me preparo para as possíveis mudanças que estão por vir.\nDe um funcionário público encostado com futuro profissional discutível, \u0026ldquo;ascendi\u0026rdquo; a uma posição de desejável pelo próprio mercado de trabalho brasileiro e internacional.\nNada mal para alguém cujo trabalho valia tão pouco aos olhos da empresa que teve seu setor extinto.\nAinda não tive a oportunidade de compensar os mais de 10 anos de atraso no auto desenvolvimento, mas chegamos lá.\nObrigado por tudo, Kubernetes!\n",
    "ref": "/marcelo/blog/suddenly-cka/"
  },{
    "title": "Linux, contêineres e descritores de arquivos",
    "date": "",
    "description": "Ou: por que você PRECISA de um Linux guru na sua equipe de 'software engineers'",
    "body": "O mundo mudou, e tudo hoje é \u0026ldquo;software engineer\u0026rdquo; e containers em todas as direções. X as a Code, \u0026ldquo;abordagem dev para tudo\u0026rdquo;, \u0026ldquo;o que um dev faria?\u0026rdquo;, e assim vai. E como fica o profissional de infraestrutura? Aquele cara que hoje é conhecido como Ops puro sangue, e que foi basicamente substituído por desenvolvedores que entendem um pouco mais de operação que os outros?\nEu sou um desses caras. Um Guru Linux das antigas, dos tempos do Slackware, dependency chain download e das configurações de X que queimavam monitores.\nTalvez este post de hoje mostre que ainda vale a pena ter \u0026ldquo;um de nós\u0026rdquo; no seu time!\n Uma nova imagem docker para uma solução interna foi gerada com atualizações e melhorias. A imagem está 100% operacional; todos os testes funcionais estão ok.\nMas ela está com um pequeno detalhe: não temos logs.\nE basicamente o único propósito desta imagem é gerar esses logs, que servem para auditoria. Sem logs, ela não serve para absolutamente nada.\nO que mudou no ambiente entre as duas versões? Basicamente tudo:\n Versão do SO; Versão do Docker; Versão do Kubernetes; Versão da imagem base; Versão do próprio software; Scripts de inicialização do container;  E aí, por onde começar?\n Aqui, um pouco de contexto:\nA aplicação é legada e não sabe jogar logs na saída padrão.\nPara remediar este problema, a solução foi:\n... ln -sf /dev/stdout /var/log/app/aplication.log \u0026amp;\u0026amp; \\ ...  Nota: nem sempre isso resolve o problema. O Tinyproxy e o [pureftpd],(https://www.pureftpd.org/) em algum momento na história, não funcionavam, embora atualmente salvo engano estão ok).\n  Esse problema foi trazido para a equipe.\nComo tudo foi mudado (o terror de quem adora culpar \u0026ldquo;a mudança\u0026rdquo; pela causa do problema), é particularmente doloroso descascar essa cebola sem chorar; em especial, se te falta conhecimento em sistemas operacionais.\nE é por isso que é importante que as empresas mantenham por perto seus Gurus em Ops!\nA solução imediata proposta por um dos outros Ops foi:\n introduzir um syslog no container via sidecar; reconfigurar a aplicação para enviar seus logs para o syslog; fazer o syslog exibir as mensagens na saída padrão.  Possivelmente esta é a solução adequada, mas eu me recuso a aceitar a derrota. Introduzir mais um container (ou processo) sem entender o que aconteceu, para mim, é inaceitável. Pode até ser implementado dessa forma, depois que o problema for devidamente entendido.\n Analisando o Dockerfile Como listado acima, foi criado um link simbólico associando o arquivo de logs a /dev/stdout:\n... ln -sf /dev/stdout /var/log/app/aplication.log \u0026amp;\u0026amp; \\ ... E o que isso representa para o container em execução?\nlrwxrwxrwx 1 root root 11 Mar 27 2018 /var/log/app/application.log -\u0026gt; /dev/stdout lrwxrwxrwx 1 user user 15 Oct 24 00:31 /dev/stdout -\u0026gt; /proc/self/fd/1 lrwx------ 1 user user 64 Oct 24 02:34 /proc/self/fd/1 -\u0026gt; /dev/pts/0 Basicamente, que estamos linkando o arquivo para /proc/self/fd/1.\nSempre lembrando que, no Linux, cada processo normalmente tem os três descritores de arquivo padrão POSIX:\n stdin: fd/0 stdout: fd/1 stderr: fd/2  Então estamos associado o arquivo de logs da aplicação ao stdout (fd/1) do processo (self).\nAté aqui, tudo bem.\n Para quem nunca parou para pensar, essa é a explicação de uma outra famosa \u0026lsquo;frase shell\u0026rsquo;:\n# Por que isso... $ command \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # É diferente disso? $ command 2\u0026gt;\u0026amp;1 \u0026gt; /dev/null Neste caso, você está redirecionando o descritor de arquivos stdout (ou 1) que será aberto para seu comando para /dev/null (\u0026gt;, também podendo ser escrito 1\u0026gt;). E redirecionando o descritor de arquivos stderr (ou 2) para o mesmo descritor de arquivos de 1, (i.e., /dev/null também). Isso faz com que toda a saída do comando \u0026lsquo;desapareça\u0026rsquo; - muito comum em scripts nos quais os devs querem sumir com todo tipo de mensagem de warning, e que torna depurar a causa do problema um terror mais tarde!\nSe você fizer na ordem inversa, você redireciona stderr para stdout, e depois, stdout para /dev/null. Essa operação não impacta a anterior; logo, a saída regular do comando é suprimida, mas a saída de erro ainda será exibida (no lugar \u0026ldquo;errado\u0026rdquo;, mas será). É útil se o programa é excessivamente \u0026lsquo;verbose\u0026rsquo; e você só tem interesse nas possíveis mensagens de erro.\nNão entendeu? Alguém desenhou aqui.\nO que o docker faz para exibir logs? ODocker possui um parâmetro log-driver que habilita a conhecida função docker logs - apenas os logdrivers \u0026lsquo;json-file\u0026rsquo; e \u0026lsquo;journald\u0026rsquo; viabilizam seu uso. O log-driver padrão é \u0026lsquo;json-file\u0026rsquo;.\nA descrição deste log-driver está na documentação oficial:\nBy default, Docker captures the standard output (and standard error) of all your containers, and writes them in files using the JSON format. The JSON format annotates each line with its origin (stdout or stderr) and its timestamp. Each log file contains information about only one container. Portanto, o Docker joga o conteúdo gerado pelo fd/1 e fd/2 do container em um arquivo.\n Nota: Um Pod Kubernetes composto por 2 containers irá gerar dois arquivos diferentes.\n Não tem exemplos na documentação, então aqui segue um:\n{\u0026quot;log\u0026quot;:\u0026quot;I1024 03:08:36.962547 12 instance.go:317] updating 0 host(s): []\\n\u0026quot;,\u0026quot;stream\u0026quot;:\u0026quot;stderr\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2020-10-24T03:08:36.962684216Z\u0026quot;} Ele inclusive tem a bondade de ilustrar para você de que tipo é (\u0026ldquo;stream\u0026rdquo;: \u0026ldquo;stderr\u0026quot;).\nOs arquivo normalmente são jogados em /var/lib/docker/containers, mas isso é irrelevante.\nO que realmente é relevante é como o Docker faz a ponte da saída dos comandos para este arquivo.\nComo o linux associa os descritores de arquivos de processos? Antes de chegar no Docker, como ver os descritores de arquivos de um processo?\nPrimeiro, descobrimos o pid do nosso processo, por exemplo, o shell da minha sessão atual:\n$ ps PID TTY TIME CMD 9896 pts/0 00:00:00 bash 195216 pts/0 00:00:00 ps PID 9896, ok. Vamos conferir:\nls -l /proc/9896/fd total 0 lrwx------. 1 core core 64 Oct 24 00:43 0 -\u0026gt; /dev/pts/0 lrwx------. 1 core core 64 Oct 24 00:43 1 -\u0026gt; /dev/pts/0 lrwx------. 1 core core 64 Oct 24 00:43 2 -\u0026gt; /dev/pts/0 lrwx------. 1 core core 64 Oct 24 01:20 255 -\u0026gt; /dev/pts/0 (255? Sim, porque bash. Deixo essa história para outro dia.)\nE o que é /dev/pts/0? São os \u0026ldquo;pseudo terminais\u0026rdquo; criados para podermos interagir com o SO. Se você abre múltiplas janelas de terminais, vai ver isso aqui:\n$ ls /dev/pts 0 1 10 11 12 2 3 4 5 6 7 8 9 ptmx Na máquina acima, só tenho um:\n$ ls /dev/pts/ 0 ptmx (E o ptmx? shhhh, ele é o \u0026lsquo;master\u0026rsquo;! Mas deixa esse assunto pra lá.)\nTá, o bash abriu um terminal. E outros programas que normalmente rodam em backgroun, por exemplo, o kubelet?\nsudo ls -l /proc/$( pgrep kubelet )/fd/ total 0 lr-x------. 1 root root 64 Sep 19 19:17 0 -\u0026gt; /dev/null lrwx------. 1 root root 64 Sep 19 19:17 1 -\u0026gt; 'socket:[14836]' lrwx------. 1 root root 64 Sep 19 19:17 10 -\u0026gt; 'socket:[49723]' lrwx------. 1 root root 64 Sep 19 19:17 11 -\u0026gt; 'socket:[25802]' lrwx------. 1 root root 64 Sep 19 19:17 12 -\u0026gt; 'socket:[33709]' lr-x------. 1 root root 64 Sep 19 19:17 13 -\u0026gt; anon_inode:inotify lrwx------. 1 root root 64 Sep 19 19:17 15 -\u0026gt; 'socket:[34950]' lr-x------. 1 root root 64 Sep 19 19:17 16 -\u0026gt; 'pipe:[14848]' l-wx------. 1 root root 64 Sep 19 19:17 17 -\u0026gt; 'pipe:[14848]' lrwx------. 1 root root 64 Sep 19 19:17 18 -\u0026gt; 'socket:[39097]' lrwx------. 1 root root 64 Sep 19 19:17 19 -\u0026gt; 'socket:[34949]' lrwx------. 1 root root 64 Sep 19 19:17 2 -\u0026gt; 'socket:[14836]' lrwx------. 1 root root 64 Sep 19 19:17 20 -\u0026gt; 'socket:[3073082]' lr-x------. 1 root root 64 Sep 19 19:17 21 -\u0026gt; /dev/kmsg lrwx------. 1 root root 64 Sep 19 19:17 22 -\u0026gt; 'socket:[274349500]' lrwx------. 1 root root 64 Sep 19 19:17 23 -\u0026gt; 'socket:[802786832]' lrwx------. 1 root root 64 Sep 19 19:17 26 -\u0026gt; 'socket:[44326]' lrwx------. 1 root root 64 Sep 19 19:17 27 -\u0026gt; 'socket:[44328]' lr-x------. 1 root root 64 Sep 19 19:17 28 -\u0026gt; anon_inode:inotify lrwx------. 1 root root 64 Oct 13 22:33 29 -\u0026gt; 'socket:[2566051059]' lrwx------. 1 root root 64 Sep 19 19:17 3 -\u0026gt; 'socket:[14840]' lr-x------. 1 root root 64 Sep 19 19:17 30 -\u0026gt; anon_inode:inotify lrwx------. 1 root root 64 Sep 19 19:17 32 -\u0026gt; 'socket:[36435]' lr-x------. 1 root root 64 Sep 19 19:17 33 -\u0026gt; anon_inode:inotify lrwx------. 1 root root 64 Sep 19 19:17 35 -\u0026gt; 'socket:[14849]' lr-x------. 1 root root 64 Sep 19 19:17 36 -\u0026gt; /dev/kmsg lrwx------. 1 root root 64 Sep 19 19:17 37 -\u0026gt; 'anon_inode:[eventpoll]' lr-x------. 1 root root 64 Sep 19 19:17 38 -\u0026gt; 'pipe:[25803]' l-wx------. 1 root root 64 Sep 19 19:17 39 -\u0026gt; 'pipe:[25803]' lrwx------. 1 root root 64 Sep 19 19:17 4 -\u0026gt; 'anon_inode:[eventpoll]' lrwx------. 1 root root 64 Sep 19 19:17 5 -\u0026gt; 'socket:[2621822183]' lrwx------. 1 root root 64 Sep 19 21:17 52 -\u0026gt; 'socket:[1275760594]' lrwx------. 1 root root 64 Sep 19 19:17 6 -\u0026gt; 'socket:[2623166919]' lrwx------. 1 root root 64 Sep 19 19:17 7 -\u0026gt; 'anon_inode:[eventpoll]' lrwx------. 1 root root 64 Sep 19 19:17 8 -\u0026gt; 'socket:[30969]' lrwx------. 1 root root 64 Sep 19 19:17 9 -\u0026gt; 'socket:[30971]' Putz, que horror, hein?\nBem, o que importa é que descritores de arquivos são associados a coisas. Pode ser um terminal, pode ser qualquer uma das tralhas acima.\nComo o docker faz a ponte entre container e log file Para ver facilmente informações sobre o seu container no systemd, você pode executar o comando machinectl para listar os containeres existentes:\n# machinectl No machines. Hum. Esqueci que o Docker odeia o systemd e eles não se conversam direito.\nVamos do jeito mais difícil:\n# systemd-cgls Control group /: -.slice ├─kubepods │ ├─burstable ... │ │ ├─pod6540eb7c-34d1-42c4-b6c7-cf83bd05e472 │ │ │ ├─df5869b859ea271c8836708c055b32387d26584c2e2824b0898e8bcbf7f6d6e5 │ │ │ │ └─164408 /pause │ │ │ └─8169619db23377b7df0ba05babcb8e3c17cfa662ff024b8cd136a4c6e2594070 │ │ │ ├─165234 /sbin/tini /usr/local/bin/start.sh │ │ │ ├─165311 /bin/bash /usr/local/bin/start.sh │ │ │ ├─/ bash /usr/local/bin/notify.sh │ │ │ ├─166143 inotifywait -qrm --exclude=.*\\.conf -e MOVED_TO --format %e:... │ │ │ ├─166155 /usr/bin/app Aqui vemos um típico Pod Kubernetes, que é sempre composto por pelo menos dois containers:\n O famoso container \u0026lsquo;pause\u0026rsquo; (df5869b859ea\u0026hellip;) O container da aplicação (8169619db233\u0026hellip;).  O container conta com um init simplório (tini) que executa um shell script, que executa um shell script(!), que executa a aplicação.\nA partir do nome do container, é possível investigar o PID do processo encarregado pelo containerd de cuidar do nosso container:\n# ps auxw | fgrep b68d2844a70ea6 | head -n1 root 144768 0.0 0.0 8564 4604 ? Sl 02:34 0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/b68d2844a70ea62317392d31f16590ef05f1afb2431519c47df7bfaa3ecd81f9 -address /run/docker/libcontainerd/docker-containerd.sock -containerd-binary /run/torcx/unpack/docker/bin/containerd -runtime-root /var/run/docker/runtime-runc -debug E, com isso, podemos ver seus file descriptors:\n# # Suprimi alguns da saída., daí os números ausentes. # ls -l /proc/144768/fd total 0 lr-x------. 1 root root 64 Oct 24 02:36 0 -\u0026gt; /dev/null lrwx------. 1 root root 64 Oct 24 02:36 1 -\u0026gt; 'socket:[25008]' lrwx------. 1 root root 64 Oct 24 02:36 2 -\u0026gt; 'socket:[25008]' ... lrwx------. 1 root root 64 Oct 24 02:36 4 -\u0026gt; 'anon_inode:[eventpoll]' lrwx------. 1 root root 64 Oct 24 02:36 5 -\u0026gt; 'anon_inode:[eventpoll]' lrwx------. 1 root root 64 Oct 24 02:36 6 -\u0026gt; 'socket:[3134831467]' lrwx------. 1 root root 64 Oct 24 02:36 7 -\u0026gt; 'socket:[3134853345]' lr-x------. 1 root root 64 Oct 24 02:36 8 -\u0026gt; 'pipe:[3134884039]' ... lr-x------. 1 root root 64 Oct 24 02:36 10 -\u0026gt; 'pipe:[3134884040]' lr-x------. 1 root root 64 Oct 24 02:36 12 -\u0026gt; 'pipe:[3134884041]' lrwx------. 1 root root 64 Oct 24 02:34 21 -\u0026gt; /dev/pts/ptmx Interessante.\nVamos olhar os descritores de arquivos desses processos:\n# ls -l /proc/{165234,165311,166142,166143,166155}/fd/ /proc/165234/fd/: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lr-x------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; ''pipe:[3134884039]'' l-wx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; 'pipe:[3134884040]' l-wx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; 'pipe:[3134884041]' /proc/165311/fd: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lr-x------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; ''pipe:[3134884039]'' l-wx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; 'pipe:[3134884040]' l-wx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; 'pipe:[3134884041]' lr-x------. 1 31 31 64 Oct 24 01:42 255 -\u0026gt; /usr/local/bin/start.sh /proc/166142/fd: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lr-x------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; ''pipe:[3134884039]'' l-wx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; 'pipe:[3134884040]' l-wx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; 'pipe:[3134884041]' lr-x------. 1 31 31 64 Oct 24 01:42 255 -\u0026gt; /usr/local/bin/notify.sh /proc/166143/fd: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lr-x------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; ''pipe:[3134884039]'' l-wx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; 'pipe:[2185069392]' l-wx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; /dev/null lr-x------. 1 31 31 64 Oct 24 01:42 3 -\u0026gt; anon_inode:inotify /proc/166155/fd/: total 0 dr-x------. 2 31 31 0 Oct 24 01:42 . dr-xr-xr-x. 9 31 31 0 Oct 24 01:42 .. lrwx------. 1 31 31 64 Oct 24 01:42 0 -\u0026gt; /dev/null lrwx------. 1 31 31 64 Oct 24 01:42 1 -\u0026gt; /dev/null lrwx------. 1 31 31 64 Oct 24 01:42 2 -\u0026gt; /dev/null Ahá!\nBasicamente, todos os stdout dos PIDs dos shell scripts foram direcionados para\u0026rsquo;pipe:[3134884040]'! Este pipe está associado a um outro fd do containerd-shim que fará as mensagens chegarem no arquivo de logs apropriado em /var/lib/docker/containers.\nMas e o processo da aplicação no container? Tanto stdin, quando stdout quanto stderr apontam para /dev/null!\nPor que isso aconteceu?\nBem, porque a maior parte dos daemons no Linux, ao subir, fazem a atribuição de pelo menos os fds 0 e 1 a /dev/null. Alguns ainda deixam stderr livre para imprimir mensagens de erro, mas não foi o caso dessa aplicação.\nVeja este container aqui com nginx:\n# ps auxw PID USER TIME COMMAND 1 nobody 0:00 /usr/bin/dumb-init -- /usr/local/bin/start-inotify.sh 8 nobody 0:00 {start-inotify.s} /bin/bash /usr/local/bin/start-inotify.sh 10 nobody 0:00 nginx: master process nginx -c /etc/nginx/nginx.conf 11 nobody 13:17 nginx: worker process # ls -l /proc/10/fd total 0 lrwx------ 1 nobody nobody 64 Oct 24 05:19 0 -\u0026gt; /dev/null lrwx------ 1 nobody nobody 64 Oct 24 05:19 1 -\u0026gt; /dev/null l-wx------ 1 nobody nobody 64 Oct 24 05:19 2 -\u0026gt; pipe:[3001744697] lrwx------ 1 nobody nobody 64 Oct 24 05:19 3 -\u0026gt; socket:[3001748727] l-wx------ 1 nobody nobody 64 Oct 24 05:19 4 -\u0026gt; pipe:[3001744697] l-wx------ 1 nobody nobody 64 Oct 24 05:19 5 -\u0026gt; pipe:[3001744696] lrwx------ 1 nobody nobody 64 Oct 24 05:19 6 -\u0026gt; socket:[3001742291] lrwx------ 1 nobody nobody 64 Oct 24 05:19 7 -\u0026gt; socket:[3001748728] Então, relembrando o nosso Dockerfile:\n... ln -sf /dev/stdout /var/log/app/aplication.log \u0026amp;\u0026amp; \\ ... Não é nenhuma surpresa que não esteja funcionando. Você mandou apontar o log da aplicação para /dev/stdout, que é /dev/null.\nComo resolver? Este é o famoso caso em que se demora mais para explicar a solução do problema que para resolver o problema.\nA explicação já foi acima.\nA troca abaixo \u0026ldquo;resolve\u0026rdquo; o problema:\n ln -sf /dev/stdout /var/log/app/aplication.log \u0026amp;\u0026amp; \\ por\n ln -sf /proc/1/fd/1 /var/log/app/aplication.log \u0026amp;\u0026amp; \\ Um truque sujo, simples, porém altamente eficaz e com o mínimo de trauma no deploy da aplicação. O stdout do pid 1 não é /dev/null, e agora o logs da aplicação aparecem normalmente.\n Nota: solução pode não funcionar em ambientes com \u0026ndash;pid=host, SELinux ou Apparmor ativo sem modificações das policies padrão!\n ",
    "ref": "/marcelo/blog/linux-containers-and-file-descriptors/"
  },{
    "title": "Calicoctl, TLS e gestão de certificados",
    "date": "",
    "description": "O que fazer quando nem o modo DEBUG das ferramentas te dá uma dica?",
    "body": "Estava precisando configurar algumas GlobalNetworkPolicies em um cluster Kubernetes; para isso, é necessário intervir diretamente no Calico, pois o Kubernetes ainda não conta com objetos de Networkpolicies que não sejam \u0026lsquo;namespaced\u0026rsquo;.\nObviamente, usamos \u0026lsquo;backend\u0026rsquo; ETCD: por que simplificaríamos tudo usando CRDs no próprio Kubernetes, não é? Deixamos isso para os amadores!\n(PS: fazemos assim porque somos tão pioneiros em usar tecnologias \u0026lsquo;bleeding edge\u0026rsquo; que, à época da implantação, usar Kubernetes como backend sequer era uma opção - sim, nós somos \u0026lsquo;that old school\u0026rsquo;! Ah, e naturalmente, somos preguiçosos demais para migrar agora.)\nMas voltando ao assunto\u0026hellip;\n\u0026ldquo;Calico, dê-me informações!\u0026rdquo;\n$ calicoctl -l debug get globalnetworkpolicies _ NADA!\nPutz, caíram minhas regras de firewall de novo?\nChamei um teste de conectividade nas portas, tudo ok:\n$ for i in 192.168.0.11 192.168.0.12 192.168.0.13 ; do { timeout 1 curl -svz1 telnet://$i:2379 2\u0026gt;\u0026amp;1 ; } | grep Connected; done * Connected to 192.168.0.11 (192.168.0.11) port 2379 (#0) * Connected to 192.168.0.12 (192.168.0.12) port 2379 (#0) * Connected to 192.168.0.13 (192.168.0.13) port 2379 (#0) Nah, tudo ok.\nDEBUG, ATIVAR:\n$ calicoctl -l debug get globalnetworkpolicies INFO[0000] Log level set to debug INFO[0000] Executing config command DEBU[0000] Resource: projectcalico.org/v3, Kind=Node DEBU[0000] Data: - apiVersion: projectcalico.org/v3 kind: Node metadata: creationTimestamp: null spec: {} status: {} DEBU[0000] Loading config from JSON or YAML data DEBU[0000] Datastore type: etcdv3 INFO[0000] Loaded client config: apiconfig.CalicoAPIConfigSpec{DatastoreType:\u0026quot;etcdv3\u0026quot;, EtcdConfig:apiconfig.EtcdConfig{EtcdEndpoints:\u0026quot;https://192.168.0.11:2379,https://192.168.0.12:2379,https://192.168.0.13:2379\u0026quot;, EtcdDiscoverySrv:\u0026quot;\u0026quot;, EtcdUsername:\u0026quot;\u0026quot;, EtcdPassword:\u0026quot;\u0026quot;, EtcdKeyFile:\u0026quot;/etc/calico/tls/acof/tls.key\u0026quot;, EtcdCertFile:\u0026quot;/etc/calico/tls/acof/tls.crt\u0026quot;, EtcdCACertFile:\u0026quot;/etc/calico/tls/acof/tls.ca\u0026quot;, EtcdKey:\u0026quot;\u0026quot;, EtcdCert:\u0026quot;\u0026quot;, EtcdCACert:\u0026quot;\u0026quot;}, KubeConfig:apiconfig.KubeConfig{Kubeconfig:\u0026quot;\u0026quot;, K8sAPIEndpoint:\u0026quot;\u0026quot;, K8sKeyFile:\u0026quot;\u0026quot;, K8sCertFile:\u0026quot;\u0026quot;, K8sCAFile:\u0026quot;\u0026quot;, K8sAPIToken:\u0026quot;\u0026quot;, K8sInsecureSkipTLSVerify:false, K8sDisableNodePoll:false, K8sUsePodCIDR:false, KubeconfigInline:\u0026quot;\u0026quot;, K8sClientQPS:0}} DEBU[0000] Using datastore type 'etcdv3' INFO[0000] Client: {{{CalicoAPIConfig projectcalico.org/v3} { 0 {{0 0 \u0026lt;nil\u0026gt;}} \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[] [] [] []} {etcdv3 {https://192.168.0.11:2379,https://192.168.0.12:2379,https://192.168.0.13:2379 /etc/calico/tls/acof/tls.key /etc/calico/tls/acof/tls.crt /etc/calico/tls/acof/tls.ca } { false false false 0}}} 0xc00000ec18 0xc0001fed70} DEBU[0000] Processing List request list-interface=Node rev= DEBU[0000] Get Global Resource key from /calico/resources/v3/projectcalico.org/nodes DEBU[0000] Didn't match regex DEBU[0000] List options is a parent prefix, ensure path ends in / list-interface=Node rev= DEBU[0000] Adding / to path list-interface=Node rev= DEBU[0000] Calling Get on etcdv3 client etcdv3-etcdKey=/calico/resources/v3/projectcalico.org/nodes/ list-interface=Node rev= Putz, imprime um monte de lixo inútil para a resolução do problema, e trava na requisição ao servidor ETCD do mesmo jeito.\nQual o próximo passo lógico de diagnóstico agora? Tarô? Leitura de mão?\n A chave para a solução do problema passa sutilmente despercebida na diretiva EtcdEndpoints:\n...https://... Esse tipo de erro obscuro, fantasma, de conexões que não começam (ou, no caso, que nunca terminal) são típicos de problemas de validação TLS. Mas que tipo de validação TLS?\nUm programa \u0026ldquo;bem feito\u0026rdquo; normalmente ajuda a depuração. Vamos analisar o comportamento do comando curl quando exposto a diversos problemas de validação TLS:\n Autoridade certificadora inválida:  # curl https://etcd1.local:2379 curl: (60) Peer's Certificate issuer is not recognized.  Nome do servidor não consta no certificado:  # curl --cacert tls/sp2/ca.pem --resolve hostname.invalido:2379:10.99.17.11 https://hostname.invalido:2379 curl: (51) Unable to communicate securely with peer: requested domain name does not match the server's certificate.  O servidor requer autenticação com certificado válido pelo cliente, e vocẽ não enviou nenhum:  # curl --cacert tls/sp2/ca.pem https://etcd1.local:2379 curl: (58) NSS: client certificate not found (nickname not specified) Bem, já é o suficiente.\nO calicoctl, entretanto, deixa bastante a desejar nesse cenário (A Tigera possivelmente acredita que você não terá problemas para reconhecer algo tão bobo?).\nPraticamente todos os erros que envolvem certificados e TLS terminam com o calicoctl travando por tempo indefinido.\n Autoridade certificadora inválida:  # ETCD_ENDPOINTS=https://etcd1.local:2379 calicoctl -l debug get nodes ... INFO[0000] Loaded client config: apiconfig.CalicoAPIConfigSpec{DatastoreType:\u0026quot;etcdv3\u0026quot;, EtcdConfig:apiconfig.EtcdConfig{EtcdEndpoints:\u0026quot;https://etcd1.local:2379\u0026quot;, EtcdDiscoverySrv:\u0026quot;\u0026quot;, EtcdUsername:\u0026quot;\u0026quot;, EtcdPassword:\u0026quot;\u0026quot;, EtcdKeyFile:\u0026quot;\u0026quot;, EtcdCertFile:\u0026quot;\u0026quot;, EtcdCACertFile:\u0026quot;\u0026quot;, EtcdKey:\u0026quot;\u0026quot;, EtcdCert:\u0026quot;\u0026quot;, EtcdCACert:\u0026quot;\u0026quot;}, KubeConfig:apiconfig.KubeConfig{Kubeconfig:\u0026quot;\u0026quot;, K8sAPIEndpoint:\u0026quot;\u0026quot;, K8sKeyFile:\u0026quot;\u0026quot;, K8sCertFile:\u0026quot;\u0026quot;, K8sCAFile:\u0026quot;\u0026quot;, K8sAPIToken:\u0026quot;\u0026quot;, K8sInsecureSkipTLSVerify:false, K8sDisableNodePoll:false, K8sUsePodCIDR:false, KubeconfigInline:\u0026quot;\u0026quot;, K8sClientQPS:0}} ... ^C  Nome do servidor não consta no certificado:  # # Criei uma entrada no /etc/hosts para etcd.devsres.com, o nome não resolve. # ETCD_ENDPOINTS=https://etcd.devsres.com:2379 ETCD_CA_CERT_FILE=tls/tls.ca calicoctl -l debug get nodes ... INFO[0000] Loaded client config: apiconfig.CalicoAPIConfigSpec{DatastoreType:\u0026quot;etcdv3\u0026quot;, EtcdConfig:apiconfig.EtcdConfig{EtcdEndpoints:\u0026quot;https://etcd.devsres.com:2379\u0026quot;, EtcdDiscoverySrv:\u0026quot;\u0026quot;, EtcdUsername:\u0026quot;\u0026quot;, EtcdPassword:\u0026quot;\u0026quot;, EtcdKeyFile:\u0026quot;\u0026quot;, EtcdCertFile:\u0026quot;\u0026quot;, EtcdCACertFile:\u0026quot;tls/tls.ca\u0026quot;, EtcdKey:\u0026quot;\u0026quot;, EtcdCert:\u0026quot;\u0026quot;, EtcdCACert:\u0026quot;tls/tls.ca\u0026quot;}, KubeConfig:apiconfig.KubeConfig{Kubeconfig:\u0026quot;\u0026quot;, K8sAPIEndpoint:\u0026quot;\u0026quot;, K8sKeyFile:\u0026quot;\u0026quot;, K8sCertFile:\u0026quot;\u0026quot;, K8sCAFile:\u0026quot;\u0026quot;, K8sAPIToken:\u0026quot;\u0026quot;, K8sInsecureSkipTLSVerify:false, K8sDisableNodePoll:false, K8sUsePodCIDR:false, KubeconfigInline:\u0026quot;\u0026quot;, K8sClientQPS:0}} ... ^C  O servidor requer autenticação com certificado válido pelo cliente, e vocẽ não enviou nenhum:  # ETCD_ENDPOINTS=https://sp2srvvpkv00001:2379 ETCD_CA_CERT_FILE=tls/tls.ca calicoctl -l debug get nodes ... INFO[0000] Loaded client config: apiconfig.CalicoAPIConfigSpec{DatastoreType:\u0026quot;etcdv3\u0026quot;, EtcdConfig:apiconfig.EtcdConfig{EtcdEndpoints:\u0026quot;https://sp2srvvpkv00001:2379\u0026quot;, EtcdDiscoverySrv:\u0026quot;\u0026quot;, EtcdUsername:\u0026quot;\u0026quot;, EtcdPassword:\u0026quot;\u0026quot;, EtcdKeyFile:\u0026quot;\u0026quot;, EtcdCertFile:\u0026quot;\u0026quot;, EtcdCACertFile:\u0026quot;tls/tls.ca\u0026quot;, EtcdKey:\u0026quot;\u0026quot;, EtcdCert:\u0026quot;\u0026quot;, EtcdCACert:\u0026quot;\u0026quot;}, KubeConfig:apiconfig.KubeConfig{Kubeconfig:\u0026quot;\u0026quot;, K8sAPIEndpoint:\u0026quot;\u0026quot;, K8sKeyFile:\u0026quot;\u0026quot;, K8sCertFile:\u0026quot;\u0026quot;, K8sCAFile:\u0026quot;\u0026quot;, K8sAPIToken:\u0026quot;\u0026quot;, K8sInsecureSkipTLSVerify:false, K8sDisableNodePoll:false, K8sUsePodCIDR:false, KubeconfigInline:\u0026quot;\u0026quot;, K8sClientQPS:0}} ... ^C Enfim, você não pode confiar que o calicoctl irá te contar se houver qualquer tipo de falha na validação TLS, seja ela do lado do servidor ou do cliente.\nA mensagem que me permitiu diagnosticar adequadamente o problema foi a consulta dos logs dos servidores ETCD diretamente:\nOct 15 12:18:29 etcd1.local docker[832]: 2020-10-15 15:18:29.066104 I | embed: rejected connection from \u0026quot;192.168.255.42:54608\u0026quot; (error \u0026quot;tls: failed to verify client's certificate: x509: certificate has expired or is not yet valid\u0026quot;, ServerName \u0026quot; etcd1.local\u0026quot;) Oct 15 12:18:59 etcd1.local docker[832]: 2020-10-15 15:18:59.136121 I | embed: rejected connection from \u0026quot;192.168.255.42:56290\u0026quot; (error \u0026quot;tls: failed to verify client's certificate: x509: certificate has expired or is not yet valid\u0026quot;, ServerName \u0026quot; etcd1.local\u0026quot;) Aqui, diagnóstico trivial: os pseudo administradores deste ambiente deixaram os certificados do cliente calicoctl expirarem.\n Conclusão:\n Conectividade TLS pode ser um horror para depurar; Se você gera certificados para sua Intranet, implemente um controle adequado para saber quando estes estiverem próximos de expirar;  ",
    "ref": "/marcelo/blog/calicoctl-stuck/"
  },{
    "title": "Cuidado com terraform import",
    "date": "",
    "description": "Importando recursos que usam count e for_each",
    "body": "(Nota: eu tenho tentado produzir conteúdo de qualidade seguindo uma linha de raciocínio com começo, meio e fim, com valor histórico e altamente apreciável. Com isso, meu último post foi mês passado e eu tenho mais de 30 drafts a concluir. Portanto, vou tornar isso aqui um braindump de conteúdos aleatórios que eu julgar relevante. Foi mal!)\n Quem não adora Terraform?\nBasta escrever meia dúzia de arquivos que algum programa magicamente interpreta tudo e cria coisas mágicas para você. Fantástico! O único problema é aprender a usar direito.\nA Hashicorp, até alguns anos atrás, fazia documentações tão crípticas e ilegíveis que eventualmente perceberam que era necessário um esforço maior para \u0026ldquo;mentes menores\u0026rdquo; compreenderem seus softwares. Com isso, investiram somas substanciais de dinheiro fazendo sites como o Learn Hashicorp ou mesmo workshops gratuitos interativos com Instruqt. Eles realmente têm feito um bom trabalho nessa seara.\nEm muitos aspectos, o Terraform ainda é deficiente.\nÀs vezes é pura frescura: vide o terraform-provider-kubernetes, que simplesmente se recusava a implementar APIs betas por anos, tornando-o praticamente inútil. E mesmo quando deployments chegaram à GA, demorou quase um ano para implementar este elemento que é considerado como pedra fundamental para qualquer aplicação Kubernetes.\nMas muitas vezes nem é culpa da Hashicorp: a integração com VMware, mesmo com as últimas funcionalidades implementadas pela última versão, é, literalmente, um horror. E o problema aqui são as severas limitações funcionais das APIs públicas disponibilizadas pela própria VMWare.\nMas chega de \u0026lsquo;rant\u0026rsquo;: vamos a conteúdo!\nTerraform import Todo mundo sabe que o Terraform precisa de total controle sobre o que cria; o que existe tem que ser criado por ele, e isso é inegociável.\nQuer dizer, nem tanto; eles dão uma colher de chá para você tentar adaptar uma infraestrutura já existente à sua automação: o comando terraform import.\nSuponha que, agora, você usa Terraform e conseguiu criar 2 máquinas usando seu novíssimo programa! Como você faz para \u0026lsquo;incorporar\u0026rsquo; as 2000 máqunas que já existem na sua infraestrutura? Executando 2000 vezes (no mínimo) o comando \u0026lsquo;import\u0026rsquo; especificando os \u0026lsquo;terraform resources\u0026rsquo; que as máquinas representam, bem como um identificador alienígena que varia completamente de maneira praticamente imprevisível de acordo com o tipo de \u0026lsquo;provider\u0026rsquo;! (Tudo bem, é melhor que toda infraestrutura tem que ser criado por ele e isso ser inegociável).\nNosso problema: cluster Kubernetes com 10 máquinas virtuais usando VMWare Vsphere. Precisamos adicionar mais 4. Obviamente não usamos Terraform no passado. Como fazer?\nComando da documentação oficial:\nterraform import vsphere_virtual_machine.vm /dc1/vm/srv1 O colega com pouca experiência que está trilhando os tortuosos caminhos dos programas Terraform criados por mim (que consigo ser ainda mais críptico que a própria Hashicorp) falou:\n \u0026ldquo;Ufa! Ainda bem que é só isso, certo?\u0026rdquo;\n Claro que não!\n Se o seu programa (tipo, literalmente, no diretório corrente) criar \u0026lsquo;resources\u0026rsquo; do tipo \u0026lsquo;vsphere_virtual_machine\u0026rsquo; que sejam escalares (i.e., não usem \u0026lsquo;for_each\u0026rsquo; ou \u0026lsquo;count'), o primeiro parãmetro do comando está correto. Caso contrário, está errado! /dc1/vm/srv1 é obviamente um placeholder; você precisa descobrir o \u0026lsquo;caminho VMware\u0026rsquo; das suas máquinas virtuais, e, se você não entende de VMWare, pode ter alguma dificuldade com a nomenclatura.  A segunda parte é fácil: basta compor o nome do datacenter com a string arbitrária vm com as \u0026lsquo;pastas\u0026rsquo; criadas no Vcenter e, por fim, o nome dos servidores:\n \u0026ldquo;/bsa/vm/infra/10069/kubernetes df1/hosts/master1\u0026rdquo; \u0026ldquo;/bsa/vm/infra/10069/kubernetes df1/hosts/master2\u0026rdquo; \u0026ldquo;/bsa/vm/infra/10069/kubernetes df1/hosts/worker1\u0026rdquo; \u0026ldquo;/bsa/vm/infra/10069/kubernetes df1/hosts/worker2\u0026rdquo;  Tranquilo. (Espero que tenha acesso à API do VMWare para descobrir isso!)\nA primeira parte, por outro lado, irá variar radicalmente de acordo com o Terraform que você está usando.\nEm nosso caso, por exemplo, eu criei um módulo terraform que encapsula a criação dos recursos VMWare VSphere necessários. Esse encapsulamento \u0026lsquo;vaza\u0026rsquo; por uma série de razões, mas me permite passar as máquinas como uma variável map para o módulo mais ou menos da seguinte forma:\nvms = { \u0026quot;master1\u0026quot; = { \u0026quot;profile\u0026quot; = \u0026quot;k8s_worker\u0026quot; \u0026quot;hostname\u0026quot; = \u0026quot;master1\u0026quot; \u0026quot;network\u0026quot; = { \u0026quot;rede1\u0026quot; = \u0026quot;192.168.0.1\u0026quot;, \u0026quot;rede2\u0026quot; = \u0026quot;10.0.0.1\u0026quot; } } \u0026quot;worker1\u0026quot; = { \u0026quot;profile\u0026quot; = \u0026quot;k8s_worker\u0026quot; \u0026quot;hostname\u0026quot; = \u0026quot;worker1\u0026quot; \u0026quot;network\u0026quot; = { \u0026quot;rede1\u0026quot; = \u0026quot;192.168.0.101\u0026quot;, \u0026quot;rede2\u0026quot; = \u0026quot;10.0.0.101\u0026quot;, \u0026quot;rede3\u0026quot; = \u0026quot;172.16.0.1\u0026quot;, } } \u0026quot;ingress1\u0026quot; = { \u0026quot;profile\u0026quot; = \u0026quot;k8s_ingress\u0026quot; \u0026quot;hostname\u0026quot; = \u0026quot;ingress1\u0026quot; \u0026quot;network\u0026quot; = { \u0026quot;rede1\u0026quot; = \u0026quot;192.168.0.201\u0026quot;, \u0026quot;rede2\u0026quot; = \u0026quot;10.0.0.201\u0026quot;, \u0026quot;rede4\u0026quot; = \u0026quot;200.160.2.1\u0026quot;, } }  (E aqui, um conselho: evite ao máximo criar maps de maps de maps ou maps de arrays de maps como eu gosto de fazer. É uma ideia idiota, que deixa seu programa Terraform praticamente incompreensível para seres humanos. Eu adoro fazer assim por razões, mas definitivamente não recomendo).\n Enfim, a criação dos meus resources estão encapsulados no módulo.\nEntão, o comando \u0026lsquo;terraform import\u0026rsquo; deve, necessariamente, fazer referência ao resource dentro do módulo:\n module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm  E aqui está a pegadinha: aqui está a definição do meu resource vm:\nresource \u0026quot;vsphere_virtual_machine\u0026quot; \u0026quot;vm\u0026quot; { for_each = local.vms name = each.value.hostname ... Portanto, o meu \u0026lsquo;resource\u0026rsquo; vm não é um tipo simples (ou escalar, como eu chamo), e sim um tipo complexo (por causa do uso do iterador for_each) do tipo map.\nO comando correto para importar máquinas já existentes, portanto, é assim:\n# terraform import \u0026#39;module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026#34;master2\u0026#34;]\u0026#39; \u0026#39;/bsa/vm/infra/10069/kubernetes df1/hosts/master2\u0026#39; # terraform import \u0026#39;module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026#34;worker2\u0026#34;]\u0026#39; \u0026#39;/bsa/vm/infra/10069/kubernetes df1/hosts/worker2\u0026#39; ... Tá, mas e daí? E daí que: o que pode acontecer se eu executar o comando errado?\n# terraform import module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm \u0026#39;/bsa/vm/infra/10069/kubernetes df1/hosts/worker2\u0026#39; Acima, erramos o comando, e importamos \u0026lsquo;worker2\u0026rsquo;, que deveria ser membro de um map, para vsphere_virtual_machine.vm do módulo diretamente.\nO que se espera normalmente? Um erro de execução, certo?\nVai dar erro, mas não no import. O import irá executar de maneira bem sucedida. Mas, depois disso, provavelmente tudo relacionado a este programa falhará.\nApós a execução do comando errado, fui agraciado com a seguinte mensagem:\n# terraform apply ... module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026quot;ingress3\u0026quot;]: Refreshing state... [id=421ab407-1bc3-dceb-6906-72fe3fad4c0e] Error: Unsupported attribute on .terraform/modules/vmware-cluster-k8s-raw/locals.tf line 104, in locals: 104: vms_result = { for key, value in vsphere_virtual_machine.vm : key =\u0026gt; { for network in value.network_interface : network.network_id =\u0026gt; network.mac_address } } O que esse erro quer dizer? Quer dizer que aquela sequência mágica de \u0026lsquo;terraform maps comprehension\u0026rsquo; falhou. Por quê? Putz, vai adivinhar.\nMeu colega aplicou um \u0026ldquo;Senhor, eu desisto, Senhor!\u0026quot;:\n  E isso, obviamente, faz qualquer sênior SRE muito feliz!\n Como fui eu que pari a besta, voltei para entender.\nObservei o seguinte descrevendo o arquivo de estados:\n# terraform state list module.vmware-cluster-k8s-raw.data.vsphere_compute_cluster.compute_cluster module.vmware-cluster-k8s-raw.data.vsphere_datacenter.cluster_datacenter module.vmware-cluster-k8s-raw.data.vsphere_datastore_cluster.cluster_datastore module.vmware-cluster-k8s-raw.data.vsphere_network.networks[\u0026quot;cluster\u0026quot;] module.vmware-cluster-k8s-raw.data.vsphere_network.networks[\u0026quot;ingress\u0026quot;] module.vmware-cluster-k8s-raw.data.vsphere_network.networks[\u0026quot;management\u0026quot;] module.vmware-cluster-k8s-raw.data.vsphere_network.networks[\u0026quot;storage\u0026quot;] module.vmware-cluster-k8s-raw.vsphere_folder.folder module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026quot;worker1\u0026quot;] module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[\u0026quot;worker2\u0026quot;] Algo estranho aqui: temos dois tipos de ocorrências para o objeto que representa o map vm:\n module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm: um \u0026lsquo;escalar\u0026rsquo;! module.vmware-cluster-k8s-raw.vsphere_virtual_machine.vm[]: este sim com todas as ocorrências adequadas!  O comando errado de import criou uma entrada errada no \u0026lsquo;state file\u0026rsquo; que bagunçou o funcionamento da lógica do módulo.\nNossa situação foi um pouco pior, porque o comando gerou a máquina em um lugar no VMWare que ela não deveria ser criada por razões desconhecidas (novamente, em vez de dar erro!), e que não tínhamos permissão para remover também por razões desconhecidas.\nA solução para este caso foi a remoção da máquina problemática do \u0026lsquo;map\u0026rsquo; de objetos criados; após um apply bem sucedido com a remoção da máquina problemática, o Terraform foi capaz de perceber o desvio no arquivo de estado e corrigiu sozinho, removendo a entrada inválida e me poupando (desta vez!) a experiência aterrorizante de manipular o estado com \u0026lsquo;terraform state\u0026rsquo;, então, se era para isos que você veio aqui, desculpe frustrar sua expectativa!\n Para aprender com esta experiência:\n Criar módulos Terraform para tudo parece uma ideia genial, mas quanto mais você encapsula achando que está reusando código, mais estará duplicando entradas de variáveis e se distanciando das validações básicas do programa; Evite usar de maneira leviana maps, arrays, counts e for_eachs a menos que você esteja disposto a ir até o fim por suas escolhas - ou trabalhe com alguém insano que use em todo canto; aí, meu amigo, meus pêsames; Não execute \u0026lsquo;imports\u0026rsquo; até ter certeza absoluta do que está fazendo.  ",
    "ref": "/marcelo/blog/terraform-import-watch-out/"
  },{
    "title": "Por que Docker?",
    "date": "",
    "description": "Docker e contêiners - razões diretas e pragmáticas perdidas em um texto bem menos direto e pragmático",
    "body": "Em conversas - principalmente com pessoas dos dois extremos do espectro de idade - sobre nosso glorioso mercado de TI vez por outra surge este questionamento: por que usar Docker?\nEu tendo a reclamação, em especial de estudantes e do pessoal novo: em casa, sem um arsenal de ferramentas de CI/CD implantadas por terceiros, parece uma incrível bobagem e perda de tempo. Dockerfile, rede virtual, putz, pra que complicar, se eu posso rodar tudo na minha máquina sem problemas?\n(Já o pessoal antigo, é preguiça e má vontade: vá aprender a usar contêineres e saia da zona de conforto imediatamente!)\nPor que Docker? Essa resposta é trivial:\nDocker é simples; Docker é fácil de entender; Docker \u0026ldquo;faz tudo\u0026rdquo;.\nDocker abstrai a infinidade de detalhes necessários para a execução de contêineres no Linux com um cobertor quente, bonito e agradável que permite ao seu usuário ser produtivo com um mínimo de overhead no aprendizado de uma tecnologia \u0026ldquo;colateral\u0026rdquo;.\nE mais: o \u0026ldquo;engine Docker\u0026rdquo; vai muito além: o software veio cuidar de praticamente todas as etapas do ciclo de vida de um contêiner, desde sua criação até viabilizar que as imagens cheguem onde desejamos - seja no host, seja no Docker Registry, mantido pelo Docker! -, passando pela execução e manutenção dos containers e imagens no servidor.\nExiste uma pergunta muito mais difícil, que inclusive muitos administradores de sistemas que usam contêineres todos os dias nos últimos anos não sabe responder:\nSe você não usar Docker, vai usar o quê?\nAté mesmo pesquisar essa resposta é difícil1; muitos dos conteúdos disponíveis sobre o assunto estão ou extremamente defasados ou simplesmente errados em suas comparações. A resposta a essa pergunta não atende às necessidades da maioria dos profissionais de TI, logo é irrelevante (você tem interesse na resposta? Se sim, me deixe saber!).\nEnquanto a \u0026ldquo;concorrência\u0026rdquo; não trabalhar isso, Docker será o líder incontestável em uso.\nPor que containers? Falando a verdade, a pergunta correta não é \u0026ldquo;por que usar Docker\u0026rdquo;, e sim, por que usar contêineres. Esta sim, diferentemente da primeira, é uma pergunta sem resposta direta e imediata.\nEu poderia copiar definições, justificativas e citar mil livros ou páginas importantes explicando o porquê da relevância da tecnologia de conteinerização sob as diferentes óticas2. Mas, em verdade, o que falta para muitos é entender que problemas essa tecnologia veio resolver, e que pode ser resumido em uma sentença:\nContêineres resolvem o problema de \u0026ldquo;na minha máquina funciona!\u0026rdquo;.\nHistoricamente, sempre coube aos profissionais de infraestrutura o processo de implantação, manutenção e resolução de problemas em ambientes que executam programas elaborados por terceiros. E um desafio tradicional dessa época era a divergência entre os ambientes em que os softwares eram desenvolvidos, homologados e, por fim, postos em produção.\nNão era incomum ajustes nos ambientes serem feitos \u0026ldquo;em tempo de homologação\u0026rdquo; para corrigir problemas e se perderem na \u0026ldquo;não documentação\u0026rdquo; do sistema, que era passada para as áreas de operações implantarem (e, consequentemente, falharem). Também não era incomum o uso de plataformas radicalmente diferentes e incompatíveis entre si, gerando conflitos homéricos sobre o uso de versão de um sistema operacional não internalizado pela empresa, ou a necessidade de atualização de um serviço para uma tecnologia de pouco domínio por parte dos administradores de serviços. Mesmo para tecnologias que naturalmente são pensadas para \u0026ldquo;rodar em todo lugar\u0026rdquo;, como Java, encontra problemas com versões e parametrizações específicas que podem se perder na transição entre ambientes.\nContêineres, por definição, resolvem o problema de \u0026ldquo;reproducibilidade de resultados\u0026rdquo;, ou \u0026ldquo;portabilidade\u0026rdquo; das aplicações: uma imagem Docker é gerada para a sua aplicação, com todos os arquivos (como elementos do sistema operacional, bibliotecas e utilitários) e configurações necessários para sua execução. Uma consequência comum da adoção deste modelo é a obrigatoriedade de adoção de um mínimo de boas práticas, como por exemplo a parametrização para que a aplicação receba configurações que variam de ambiente para ambiente.\nEste problema já foi amplamente abordado no passado como virtualização de servidores e tecnologias de orquestração e gestão de configuração; nenhuma delas, entretanto, resolveu com a elegância e simplicidade de simplesmente \u0026ldquo;socar\u0026rdquo; tudo que é necessário em uma unidade parametrizável e orquestrável.\nA partir desta entidade Contêiner3 que trivializa a implantação (\u0026quot;deploy\u0026quot;) de um software, está aberto agora o caminho para a construção de novas soluções que resolvam outros problemas do processo de desenvolvimento de software e otimizem ainda mais o processo de entrega de resultados por parte das equipes.\n 1 Os melhores links sequer apareciam na página principal do Google:\n A Comprehensive Container Runtime Comparison Aquasec: Docker Alternatives  2 \u0026ldquo;Ópticas\u0026rdquo; é muito feio.\n3 Observe que, no Linux, \u0026ldquo;Contêiner\u0026rdquo; não é uma primitiva do sistema operacional, e sim um conjunto destas. Do ponto de vista prático, acaba sendo uma.\n",
    "ref": "/marcelo/blog/why-docker/"
  },{
    "title": "Sobre",
    "date": "",
    "description": "",
    "body": "O fundador da DevSREs Network Initiative, Marcelo Andrade, é entusiasta do universo Linux e software livre desde a primeira instalação do saudoso Conectiva Parolin em 1997.\nAo longo dos anos, transitou profissionalmente entre os mais diversos nomes comumente atribuídos aos profissionais de TI que não trabalham diretamente com desenvolvimento de software: sysop, sysadmin, administrador de redes, analista de redes, analista de infraestrutura, analista de suporte, engenheiro de soluções\u0026hellip; Até se auto instituir o título de Site Reliability Engineer, ou engenheiro de confiabilidade (de sites?) em português.\nA realidade é que, em todas elas, o foco é sempre o mesmo: manter ambientes on-line, garantindo performance, escalabilidade e confiabilidade aos sistemas da empresa. O que muda, com os anos, é abordagem e o conjunto de ferramentas a ser usado.\nSeu principal nicho de atuação, no momento, é a manutenção de clusters Kubernetes em conjuto com outras tecnologias Cloud Native (ou nem tanto).\n",
    "ref": "/marcelo/about/"
  },{
    "title": "Kubernetes: por onde começar",
    "date": "",
    "description": "Como dar os primeiros passos nesta tecnologia",
    "body": "No último ano, aqui em Recife, tive a oportunidade de participar (e palestrar!) em alguns eventos. Em comum, havia o fato de muito de seus conteúdos - se não integralmente - estarem relacionados ao Kubernetes.\nNas duas oportunidade em que fiz apresentações, entrei em detalhes sobre:\n o desafio de usar o objeto Ingress - que, até a recém-lançada versão 1.19, ainda era beta, embora exista desde a versão 1.1; uma nova abordagem de como fazer a gerência de um cluster e a entrega contínua de aplicações: GitOps;  Embora eu sempre procure abordar o assunto de uma forma que não seja absolutamente incompreensível para quem não tenha qualquer tipo de contato com Kubernetes, em todos os eventos, me deparo com uma realidade: a de que existe muita gente interessada sobre Kubernetes, mas com bastante dificuldade de se introduzir a esta tecnologia.\nSempre me é feita uma pergunta: por onde começar? Como dou os primeiros passos? E eu nunca tenho uma resposta de qualidade a oferecer: eu já \u0026ldquo;aprendi tudo\u0026rdquo; lá atrás, em um passado remoto no qual inexistia bons tutorials ou cursos, e a documentação descrevia o Kube Controller como \u0026ldquo;the program that runs the control loops that controls the state of objects\u0026rdquo; (ou algo assim).\nFelizmente, os tempos mudaram, e é ampla a documentação de cada mínimo aspecto relacionado ao Kubernetes, com bastante oferta de cursos (pagos e gratuitos) sobre o assunto.\nSem mais delongas, por onde começar quando quer se aprender sobre Kubernetes?\nKubeAcademy  https://kube.academy/  A VMware disponibilizou um excelente material (e, vale ressaltar completamente gratuito) sobre containers e Kubernetes. Os conteúdos são apresentados de maneira fácil de digerir, e alguns dos cursos inclusive contam com laboratórios práticos interativos com Katacoda, o que, na minha opinião, torna este conteúdo possivelmente o melhor disponível para uma apresentação inicial.\nA desvantagem para quem não domina o inglês é o uso de player próprio e a inexistência de legendas, embora seja possível regular a velocidade do player para melhorar a compreensão para aqueles não tão fluentes.\nKatacoda / instruqt  https://www.katacoda.com https://play.instruqt.com/public  O Katacoda, adquirido recentemente pela tradicional editora de livros técnicos O\u0026rsquo;Reilly, por muito tempo, serviu como laboratório prático para quem não tem recursos para subir um cluster Kubernetes para prática pessoal.\nOs cursos dno Katacoda são interativos e executados diretamente no navegador, o que torna o processo simples e dinâmico. Antes de produção restrita, agora o Katacoda permite que qualquer um disposto a aprender a usar a plataforma possa criar seus próprios cursos com ela - o que\nO ponto negativo é que a maneira de abordar o conteúdo teórico fica em segundo plano. E, acredite, muito do trabalhar bem com Kubernetes adequadamente vem de entender a maneira e o porquê optou-se por resolver os problemas desta forma.\nInstruqt é bastante semelhante em abordagem ao Katacoda; mas, no geral, a interface apresenta mais problemas e os cursos de Kubernetes em qualidade inferior.\nKubernetes.io A documentação do projeto vem crescendo em volume e qualidade a olhos vistos a cada versão que passa. A versão 1.18 introduziu a oferta em outros idiomas que não o inglês, o que nem imaginava que fosse acontecer algum dia.\nO destaque da documentação fica por parte dos conceitos. Muitos dos textos crípticos do começo foram reescritos de maneira compreensível, e agora as ideias são, sim, plenamente acessíveis ao \u0026ldquo;grande público\u0026rdquo;.\nVale a pena mencionar que a documentação conta com duas seções extremamente valiosas para quem está aprendendo e pensa em tirar a certificação CKA/CKAD: a seção de Tasks e a de Tutoriais.\nEmbora o português esteja lá, a maior parte das páginas que acessei ainda está disponível apenas em inglês.\nEDX - Introduction to Kubernetes https://www.edx.org/course/introduction-to-kubernetes\nLembro que, à medida que novos membros chegavam à equipe, a recomendação padrão era a fazer este curso do EDX elaborado pela Linux Foundation - basicamente porque era a única opção.\nÉ possível ler grande parte dos conteúdos disponíveis sem custo adicional, mas vários recursos são oferecidos apenas aos pagantes.\nDeixo aqui esta entrada pelo valor histórico, mas acredito que a relevância deste curso hoje é limitada se comparado ao que já é oferecido pela documentação oficial do Kubernetes.\nMas e em português? Aqui, eu peço desculpas, mas vou ficar devendo.\nA qualidade dos materiais que tive a oportunidade de ler (inclusive em sites pagos) está entre o ruim e o sofrível. Então, não tenho uma boa recomendação a oferecer.\n Se você conhecer algum bom site com conteúdo de Kubernetes em português e quiser recomendar, entre em contato por qualquer uma das minhas redes sociais que eu terei um prazer enorme em incluí-los nesta lista!\n",
    "ref": "/marcelo/blog/kubernetes-where-to-start/"
  },{
    "title": "Contact",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/marcelo/contact/"
  }]
